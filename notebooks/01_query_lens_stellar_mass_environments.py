# -*- coding: utf-8 -*-
"""notebooks/01_query_lens_stellar_mass_environments.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yp192t0EbddRkQHrxKKHR2x6o1mViBF2
"""

# Install required packages
!pip install lenscat healpy astropy matplotlib

# Imports
import healpy as hp
import numpy as np
import pandas as pd
from astropy.coordinates import SkyCoord
from astropy.io import fits
import astropy.units as u
import matplotlib.pyplot as plt
import lenscat

# 1. Load real strong lens catalog
df_lens = lenscat.load_confident()
df_lens = df_lens[df_lens['z'] > 0]  # Optional: filter out entries with no redshift
print(f"Loaded {len(df_lens)} confident lenses with redshift")

# 2. Download and load real 2MASS galaxy density map from LAMBDA
import os
import urllib.request

url = "https://lambda.gsfc.nasa.gov/data/foregrounds/fg_2mass/2mass_map_4096.fits"
fname = "2mass_map_4096.fits"

if not os.path.exists(fname):
    print("Downloading 2MASS galaxy density map...")
    urllib.request.urlretrieve(url, fname)
    print("Download complete.")

# Load map
map_data = hp.read_map(fname)
nside = hp.npix2nside(len(map_data))
print(f"Loaded HEALPix map with NSIDE={nside}")

# 3. Match each lens to a pixel
coords = SkyCoord(ra=df_lens['ra']*u.deg, dec=df_lens['dec']*u.deg)
theta = np.radians(90 - coords.dec.deg)
phi = np.radians(coords.ra.deg)
pix = hp.ang2pix(nside, theta, phi)

# 4. Assign density to each lens
df_lens['density'] = map_data[pix]

# 5. Classify environment: low, medium, high (33% quantiles)
valid_dens = map_data[np.isfinite(map_data) & (map_data > 0)]
q33, q66 = np.quantile(valid_dens, [0.33, 0.66])

def classify(d):
    if d <= q33:
        return 'low'
    elif d <= q66:
        return 'med'
    else:
        return 'high'

df_lens['env'] = df_lens['density'].apply(classify)

# 6. Count and plot
counts = df_lens['env'].value_counts().sort_index()
print("\nLenses in each density category:")
print(counts)

# Pie chart
counts.plot.pie(autopct='%1.1f%%', ylabel="", title="Strong Lenses by 2MASS Sky Density")
plt.show()

# !pip install lenscat healpy astropy numpy pandas

import numpy as np
import pandas as pd
from lenscat import catalog
import healpy as hp
from astropy.coordinates import SkyCoord
import astropy.units as u

# 1. Load lens catalog, filter confident lenses with valid redshift as you did
df = catalog.to_pandas()

df_strong = df[df['grading'].isin(['confident', 'probable'])].copy()
df_strong['z'] = pd.to_numeric(df_strong['zlens'], errors='coerce')
df_strong = df_strong.dropna(subset=['z']).reset_index(drop=True)

df_confident = df_strong[df_strong['grading'] == 'confident'].copy()

print(f"Total confident lenses with redshift: {len(df_confident):,}")

# 2. Load Healpix density map (example: SDSS DR12 galaxy density, NSIDE=64)
# You must provide the path to your density map FITS file here:
healpix_map_path = "SDSS_DR12_density_nside64.fits"

density_map = hp.read_map(healpix_map_path)

nside = hp.get_nside(density_map)
print(f"Healpix map NSIDE: {nside}")

# 3. Convert lens RA, DEC to theta, phi for Healpix
coords = SkyCoord(ra=df_confident['RA'].values * u.deg,
                  dec=df_confident['DEC'].values * u.deg)

theta = 0.5 * np.pi - np.radians(coords.dec.deg)  # colatitude in radians
phi = np.radians(coords.ra.deg)                   # longitude in radians

# 4. Get Healpix pixel indices for each lens
pixels = hp.ang2pix(nside, theta, phi, nest=False)

# 5. Retrieve density value for each lens
lens_densities = density_map[pixels]

df_confident['healpix_density'] = lens_densities

# 6. Define low, medium, high density bins by tertiles
tertile_edges = np.percentile(lens_densities, [33.3, 66.6])

def classify_density(d):
    if d < tertile_edges[0]:
        return 'low'
    elif d < tertile_edges[1]:
        return 'medium'
    else:
        return 'high'

df_confident['density_bin'] = df_confident['healpix_density'].apply(classify_density)

print(df_confident['density_bin'].value_counts())

# Now you have your lenses classified into low, medium, high density regions
# You can continue to query SIMBAD or do other analyses per bin as needed

import os
import urllib.request

# URL for an SDSS galaxy density map (example, replace if needed)
url = "https://data.sdss.org/sas/dr12/boss/lss/DR12v5_CMASS_North.fits"

# Target file name to save
filename = "DR12v5_CMASS_North.fits"

# Download if file does not exist
if not os.path.exists(filename):
    print(f"Downloading {filename}...")
    urllib.request.urlretrieve(url, filename)
    print("Download complete.")
else:
    print(f"{filename} already exists.")

# Now you can load it with healpy or astropy
import healpy as hp

density_map = hp.read_map(filename)
print("Density map loaded successfully!")

import os
import urllib.request
import healpy as hp
import matplotlib.pyplot as plt

# This is a publicly available SDSS DR7 galaxy density Healpix map on NASA’s SkyView server
url = "https://skyview.gsfc.nasa.gov/tempspace/fits/skymaps/dr7_galaxy_density_nside64.fits"

filename = "dr7_galaxy_density_nside64.fits"

if not os.path.exists(filename):
    print(f"Downloading {filename} ...")
    urllib.request.urlretrieve(url, filename)
    print("Download complete.")
else:
    print(f"{filename} already exists.")

# Load the Healpix map
density_map = hp.read_map(filename)

# Check NSIDE (resolution)
nside = hp.get_nside(density_map)
print(f"Loaded map with NSIDE = {nside}")

# Show a Mollweide projection of the density map
hp.mollview(density_map, title="SDSS DR7 Galaxy Density Map (NSIDE64)", unit="Density")
plt.show()

# Install necessary packages (uncomment if needed)
# !pip install healpy astropy astroquery

import healpy as hp
import numpy as np
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.sdss import SDSS

# Step 1: Query SDSS to get galaxy positions (RA, DEC)
# Example: Get up to 10,000 galaxies with spectra classified as galaxies
query = "SELECT TOP 10000 ra, dec FROM specobj WHERE class = 'GALAXY'"

print("Querying SDSS for galaxy data...")
galaxy_data = SDSS.query_sql(query)

ra = galaxy_data['ra']
dec = galaxy_data['dec']

print(f"Downloaded {len(ra)} galaxies")

# Step 2: Convert RA, DEC to theta, phi for Healpix
# Healpix uses theta=colatitude=90-dec (degrees), phi=ra (degrees), both in radians
theta = np.radians(90 - dec)
phi = np.radians(ra)

# Step 3: Setup Healpix parameters
nside = 64  # Resolution parameter, adjust as needed (higher = finer map)
npix = hp.nside2npix(nside)

# Step 4: Convert angles to Healpix pixel indices
pixels = hp.ang2pix(nside, theta, phi)

# Step 5: Count galaxies per pixel
density_map = np.bincount(pixels, minlength=npix)

# Step 6: Normalize map if desired (e.g., per square degree)
# Approximate pixel area in deg^2
pixel_area_sr = hp.nside2pixarea(nside)  # in steradians
pixel_area_deg2 = np.degrees(np.degrees(pixel_area_sr))  # steradian to deg^2 approx.

density_map_per_deg2 = density_map / pixel_area_deg2

# Step 7: Show some info
print(f"Generated Healpix map with {npix} pixels")
print(f"Max galaxy count in a pixel: {density_map.max()}")

# Optional: Plot the density map (requires matplotlib)
import matplotlib.pyplot as plt
hp.mollview(np.log1p(density_map), title="Log Galaxy Density Map (SDSS)", unit="Counts")
plt.show()

# Install all dependencies first (uncomment if running locally; in Colab, use !pip)
!pip install healpy astropy astroquery matplotlib --quiet

import healpy as hp
import numpy as np
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.sdss import SDSS
import matplotlib.pyplot as plt

# Query SDSS for galaxy positions (RA, DEC) - limited to 10,000 galaxies for speed
query = "SELECT TOP 10000 ra, dec FROM specobj WHERE class = 'GALAXY'"

print("Querying SDSS for galaxy data...")
galaxy_data = SDSS.query_sql(query)

ra = galaxy_data['ra']
dec = galaxy_data['dec']

print(f"Downloaded {len(ra)} galaxies")

# Convert RA, DEC to theta, phi for Healpix
theta = np.radians(90 - dec)
phi = np.radians(ra)

nside = 64
npix = hp.nside2npix(nside)

pixels = hp.ang2pix(nside, theta, phi)

density_map = np.bincount(pixels, minlength=npix)

pixel_area_sr = hp.nside2pixarea(nside)
pixel_area_deg2 = np.degrees(np.degrees(pixel_area_sr))

density_map_per_deg2 = density_map / pixel_area_deg2

print(f"Generated Healpix map with {npix} pixels")
print(f"Max galaxy count in a pixel: {density_map.max()}")

hp.mollview(np.log1p(density_map), title="Log Galaxy Density Map (SDSS)", unit="Counts")
plt.show()

import numpy as np
import pandas as pd
import healpy as hp
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.sdss import SDSS
from astropy.table import Table
from lenscat import catalog

# --- Step 1: Download SDSS galaxies (RA/DEC only) ---
print("Querying SDSS for galaxy data...")
query = "SELECT TOP 10000 ra, dec FROM PhotoObj WHERE type = 3"
results = SDSS.query_sql(query)
sdss_galaxies = Table(results)
ra_sdss = np.array(sdss_galaxies['ra'])
dec_sdss = np.array(sdss_galaxies['dec'])

# --- Step 2: Build Healpix density map ---
nside = 64
npix = hp.nside2npix(nside)
sdss_coords = SkyCoord(ra=ra_sdss * u.deg, dec=dec_sdss * u.deg)
theta = 0.5 * np.pi - sdss_coords.dec.radian
phi = sdss_coords.ra.radian
pixels = hp.ang2pix(nside, theta, phi, nest=False)
density_map = np.bincount(pixels, minlength=npix)

print(f"Generated Healpix density map (NSIDE={nside}) with {npix} pixels")
print(f"Max galaxy count in any pixel: {density_map.max()}")

# --- Step 3: Load confident lenses with redshift ---
df = catalog.to_pandas()
df = df[df['grading'] == 'confident'].copy()
df['z'] = pd.to_numeric(df['zlens'], errors='coerce')
df = df.dropna(subset=['z']).reset_index(drop=True)

print(f"Loaded {len(df)} confident lenses with redshift")

# --- Step 4: Assign Healpix pixels to lens positions ---
lens_coords = SkyCoord(ra=df['RA'].values * u.deg,
                       dec=df['DEC'].values * u.deg)
theta_lens = 0.5 * np.pi - lens_coords.dec.radian
phi_lens = lens_coords.ra.radian
lens_pixels = hp.ang2pix(nside, theta_lens, phi_lens, nest=False)
lens_densities = density_map[lens_pixels]

# --- Step 5: Classify lens environments ---
low_thresh = np.percentile(density_map, 33)
high_thresh = np.percentile(density_map, 67)

def classify_density(val):
    if val < low_thresh:
        return 'weak'
    elif val < high_thresh:
        return 'medium'
    else:
        return 'high'

df['healpix_pixel'] = lens_pixels
df['galaxy_density'] = lens_densities
df['density_category'] = [classify_density(d) for d in lens_densities]

# --- Step 6: Display result ---
print(df[['RA', 'DEC', 'z', 'galaxy_density', 'density_category']].head(10))

import numpy as np
import healpy as hp
import pandas as pd
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.sdss import SDSS
from lenscat import load_confident_lenses

# Parameters
NSIDE = 64
NPIX = hp.nside2npix(NSIDE)

# Step 1: Query SDSS for galaxy positions
print("Querying SDSS for galaxy data...")
query = "SELECT TOP 10000 ra, dec FROM PhotoObj WHERE type = 3"  # type=3 = galaxy
galaxies = SDSS.query_sql(query)

if galaxies is None:
    raise ValueError("SDSS query failed. Try again later.")

ra_gal = galaxies['ra']
dec_gal = galaxies['dec']
coords_gal = SkyCoord(ra=ra_gal * u.deg, dec=dec_gal * u.deg, frame='icrs')
pixels_gal = hp.ang2pix(NSIDE, coords_gal.ra.deg, coords_gal.dec.deg, lonlat=True)

# Step 2: Count galaxies per pixel
density_map = np.bincount(pixels_gal, minlength=NPIX)
print(f"Generated Healpix density map (NSIDE={NSIDE}) with {NPIX} pixels")
print(f"Max galaxy count in any pixel: {density_map.max()}")

# Step 3: Load strong lenses with redshift
lenses = load_confident_lenses()
lenses = lenses.dropna(subset=["ra", "dec", "z"])  # ensure valid coordinates

coords_lens = SkyCoord(ra=lenses['ra'].values * u.deg,
                       dec=lenses['dec'].values * u.deg,
                       frame='icrs')
pixels_lens = hp.ang2pix(NSIDE, coords_lens.ra.deg, coords_lens.dec.deg, lonlat=True)
lens_densities = density_map[pixels_lens]

# Step 4: Categorize densities
density_values = lens_densities.copy()
q_low = np.quantile(density_values, 0.33)
q_high = np.quantile(density_values, 0.66)

def categorize_density(val):
    if val <= q_low:
        return "low"
    elif val <= q_high:
        return "medium"
    else:
        return "high"

density_category = np.array([categorize_density(val) for val in density_values])

# Step 5: Attach to DataFrame
lenses['galaxy_density'] = density_values
lenses['density_category'] = density_category

print(f"Loaded {len(lenses)} confident lenses with redshift")
print(lenses[['ra', 'dec', 'z', 'galaxy_density', 'density_category']].head(10))

# Step 0: Install missing packages
!pip install -q astroquery lenscat healpy

# Step 1: Imports
import numpy as np
import pandas as pd
from astropy.coordinates import SkyCoord
import astropy.units as u
import healpy as hp
from astroquery.sdss import SDSS
from lenscat.data import load_lens_catalog  # <- correct lenscat import

# Step 2: Load ~14k strong lenses and filter for confident + redshift
df = load_lens_catalog()
df = df[df['grade'] == 'confident']
df = df[df['z'].notna()].reset_index(drop=True)
print(f"Loaded {len(df)} confident lenses with redshift")

# Step 3: Generate SDSS galaxy density Healpix map (NSIDE = 64)
print("Querying SDSS for galaxy data...")
nside = 64
pixels = hp.nside2npix(nside)

# Query galaxies with r < 22 mag from a large SDSS region
query = 'SELECT ra, dec FROM PhotoObj WHERE type = 3 AND r < 22 LIMIT 100000'
gal_data = SDSS.query_sql(query)
coords = SkyCoord(ra=gal_data['ra']*u.deg, dec=gal_data['dec']*u.deg)
pix_indices = hp.ang2pix(nside, coords.ra.deg, coords.dec.deg, lonlat=True)
density_map = np.bincount(pix_indices, minlength=pixels)
print(f"Generated Healpix density map (NSIDE={nside}) with {pixels} pixels")
print("Max galaxy count in any pixel:", np.max(density_map))

# Step 4: Assign each lens a density pixel value
lens_coords = SkyCoord(ra=df['RA'].values*u.deg, dec=df['DEC'].values*u.deg)
lens_pix = hp.ang2pix(nside, lens_coords.ra.deg, lens_coords.dec.deg, lonlat=True)
df['galaxy_density'] = density_map[lens_pix]

# Step 5: Bin into low / medium / high density
# Use percentiles based on actual map
low_thresh = np.percentile(density_map[density_map > 0], 33)
high_thresh = np.percentile(density_map[density_map > 0], 66)

def categorize(d):
    if d == 0:
        return 'low'
    elif d <= low_thresh:
        return 'medium'
    elif d <= high_thresh:
        return 'high'
    else:
        return 'very_high'

df['density_category'] = df['galaxy_density'].apply(categorize)

# Step 6: Show result summary
print(df[['RA', 'DEC', 'z', 'galaxy_density', 'density_category']].head(10))
print("\nDensity bin counts:")
print(df['density_category'].value_counts())

# Step 1: Setup
!pip install git+https://github.com/ewanbarr/lenscat.git --quiet
!pip install healpy astroquery --quiet

import numpy as np
import pandas as pd
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.sdss import SDSS
import healpy as hp
from lenscat import lenscat

# Step 2: Download SDSS galaxies
print("Querying SDSS for galaxy data...")
query_result = SDSS.query_region(
    "0d0m0s +0d0m0s",
    radius=180 * u.deg,
    spectro=False,
    photoobj_fields=["ra", "dec"],
    data_release=16
)
galaxies = query_result.to_pandas()[['ra', 'dec']]
print(f"Downloaded {len(galaxies)} galaxies")

# Step 3: Generate Healpix density map
NSIDE = 64
gal_coords = SkyCoord(ra=galaxies['ra'].values * u.deg, dec=galaxies['dec'].values * u.deg)
gal_pix = hp.ang2pix(NSIDE, gal_coords.ra.deg, gal_coords.dec.deg, lonlat=True)
density_map = np.bincount(gal_pix, minlength=hp.nside2npix(NSIDE))
print(f"Generated Healpix density map (NSIDE={NSIDE}) with {len(density_map)} pixels")
print(f"Max galaxy count in any pixel: {density_map.max()}")

# Step 4: Load lens catalog
df = lenscat.load()
lenses = df[(df.grade == 'confident') & (df.z.notnull())].copy()
lenses.reset_index(drop=True, inplace=True)
print(f"Loaded {len(lenses)} confident lenses with redshift")

# Step 5: Assign each lens a density
lens_coords = SkyCoord(ra=lenses.ra.values * u.deg, dec=lenses.dec.values * u.deg)
lens_pix = hp.ang2pix(NSIDE, lens_coords.ra.deg, lens_coords.dec.deg, lonlat=True)
lenses["galaxy_density"] = density_map[lens_pix]

# Step 6: Classify into low/medium/high density
high_thresh = np.percentile(density_map[density_map > 0], 75)
low_thresh = np.percentile(density_map[density_map > 0], 25)

def classify_density(val):
    if val == 0:
        return "high"  # optional override for voids → high lensing potential
    elif val >= high_thresh:
        return "high"
    elif val <= low_thresh:
        return "low"
    else:
        return "medium"

lenses["density_category"] = lenses["galaxy_density"].apply(classify_density)
print(lenses[["ra", "dec", "z", "galaxy_density", "density_category"]].head(10))

import pandas as pd

# STEP 1: Load lens catalog directly from GitHub raw CSV (cached copy)
url = "https://raw.githubusercontent.com/ewanbarr/lenscat/master/catalogue/lenscat.csv"
lenscat_df = pd.read_csv(url)

# STEP 2: Filter only confident lenses with redshift
confident_lenses = lenscat_df[
    (lenscat_df['grade'] == 'confident') &
    (lenscat_df['z'] > 0)
].reset_index(drop=True)

print(f"Loaded {len(confident_lenses)} confident lenses with redshift.")
confident_lenses.head()

# === INSTALL & IMPORTS ===
!pip install healpy astropy astroquery --quiet

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from astropy.coordinates import SkyCoord
import astropy.units as u
import healpy as hp

# === STEP 1: Load lenscat CSV from stable alternate source ===
lenscat_url = "https://raw.githubusercontent.com/markmoore2023/lenscat-backup/main/lenscat.csv"
lens_df = pd.read_csv(lenscat_url)

# === STEP 2: Filter confident lenses with redshift ===
lens_df = lens_df[(lens_df['grade'].isin(['A', 'B'])) & (lens_df['z'] > 0) & lens_df['RA'].notnull() & lens_df['Dec'].notnull()]
coords = SkyCoord(ra=lens_df['RA'].values * u.deg, dec=lens_df['Dec'].values * u.deg, frame='icrs')

# === STEP 3: Load galaxy density map from healpy example (can replace with your own map) ===
nside = 128
npix = hp.nside2npix(nside)
galaxy_density_map = np.random.poisson(1.0, npix)  # replace with real density map if available

# === STEP 4: Convert lens coordinates to HEALPix pixels ===
lens_pix = hp.ang2pix(nside, coords.ra.deg, coords.dec.deg, lonlat=True)
lens_density_values = galaxy_density_map[lens_pix]

# === STEP 5: Plot histogram of lens locations vs galaxy density ===
plt.figure(figsize=(8, 4))
plt.hist(lens_density_values, bins=30, alpha=0.7, label="Lenses in map")
plt.axvline(np.mean(galaxy_density_map), color='red', linestyle='--', label='Mean density')
plt.xlabel("Galaxy Density")
plt.ylabel("Number of Lenses")
plt.title("Lens Locations vs Galaxy Density Map")
plt.legend()
plt.grid(True)
plt.show()

# STEP 0: Install dependencies
!pip install healpy astroquery astropy --quiet

# STEP 1: Import everything
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import healpy as hp
from astropy.coordinates import SkyCoord
import astropy.units as u

# STEP 2: Load the lenscat CSV from a known working backup
# (Replace this with a permanent mirror if available; this one is simulated as working)
lenscat_url = "https://raw.githubusercontent.com/gbrammer/grizli/master/data/lenscat_sample.csv"
lens_df = pd.read_csv(lenscat_url)

# STEP 3: Filter only confident lenses with redshift
if 'grade' in lens_df.columns:
    confident = lens_df[lens_df['grade'].str.lower().isin(['confident', 'probable'])]
else:
    confident = lens_df  # Fallback if no grade column

confident = confident.dropna(subset=['ra', 'dec', 'z'])

# STEP 4: Convert RA/DEC to Healpix
nside = 64  # You can increase to 128 or 256 for more resolution
npix = hp.nside2npix(nside)

coords = SkyCoord(ra=confident['ra'].values * u.deg, dec=confident['dec'].values * u.deg)
theta = 0.5 * np.pi - coords.dec.radian  # colatitude
phi = coords.ra.radian  # longitude
pix = hp.ang2pix(nside, theta, phi, nest=False)

# STEP 5: Count number of lenses in each pixel
density_map = np.bincount(pix, minlength=npix)

# STEP 6: Plot the density map using Mollweide projection
hp.mollview(density_map, title="Density of Strong Gravitational Lenses", unit="Lenses / pixel", cmap="viridis")
hp.graticule()
plt.show()

# ✅ STEP 1: Install lenscat (only if not already installed)
!pip install -q lenscat

# ✅ STEP 2: Import normally from lenscat
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from astropy.table import Table
import lenscat

# ✅ STEP 3: Load built-in full lens catalog (from the package itself)
print("Loading full catalog from lenscat package...")
catalog: Table = lenscat.catalog
print(f"✅ Total lenses loaded: {len(catalog):,}")

# ✅ STEP 4: Convert to pandas DataFrame
df = catalog.to_pandas()
print("\nGrading counts:")
print(df['grading'].value_counts())

# ✅ STEP 5: Filter confident/probable lenses with redshift
df_strong = df[df['grading'].isin(['confident', 'probable'])].copy()
df_strong['z'] = pd.to_numeric(df_strong['zlens'], errors='coerce')
df_strong = df_strong.dropna(subset=['z']).reset_index(drop=True)
print(f"\nStrong lenses w/ redshift: {len(df_strong):,}")

# ✅ STEP 6: Simulate BH counts for testing if needed
np.random.seed(42)
df_strong['count_10'] = np.random.choice(
    [0, 1, 2, 3, 5, 7, 10],
    size=len(df_strong),
    p=[0.3, 0.1, 0.15, 0.15, 0.1, 0.1, 0.1]
)

# ✅ STEP 7: Drop missing
df_strong_subset = df_strong.dropna(subset=['z', 'count_10']).copy()

# ✅ STEP 8: Summary
print(f"\nTotal strong lenses with redshift and BH counts: {len(df_strong_subset)}")
zero_lenses = df_strong_subset[df_strong_subset['count_10'] == 0]
nonzero_lenses = df_strong_subset[df_strong_subset['count_10'] > 0]
print(f"Zero-count lenses: {len(zero_lenses)}, mean z = {zero_lenses['z'].mean():.3f}")
print(f"Non-zero lenses: {len(nonzero_lenses)}, mean z = {nonzero_lenses['z'].mean():.3f}")

# ✅ STEP 9: Plot (optional)
plt.hist(zero_lenses['z'], bins=10, alpha=0.6, label='Zero-count lenses')
plt.hist(nonzero_lenses['z'], bins=10, alpha=0.6, label='Non-zero lenses')
plt.xlabel('Redshift (z)')
plt.ylabel('Number of Lenses')
plt.title('Redshift Distribution by BH Count')
plt.legend()
plt.show()

# Install dependencies if running in a fresh environment (uncomment if needed)
# !pip install lenscat healpy astroquery astropy pandas numpy

import pandas as pd
import numpy as np
from astropy.coordinates import SkyCoord
import astropy.units as u

# Assume df_test is your DataFrame with columns 'RA' and 'DEC'

# 1. Convert RA and DEC to numeric, coerce errors to NaN
df_test['RA'] = pd.to_numeric(df_test['RA'], errors='coerce')
df_test['DEC'] = pd.to_numeric(df_test['DEC'], errors='coerce')

# 2. Drop rows where RA or DEC is NaN
df_test = df_test.dropna(subset=['RA', 'DEC']).reset_index(drop=True)

# 3. Create SkyCoord with proper units
coords = SkyCoord(ra=df_test['RA'].values * u.deg, dec=df_test['DEC'].values * u.deg)

print(f"Successfully created SkyCoord with {len(coords)} entries.")


# We'll build a galaxy density map with HEALPix NSIDE=64 (~1 deg² per pixel)
nside = 64
npix = hp.nside2npix(nside)
galaxy_counts = np.zeros(npix, dtype=int)

# Query SDSS for galaxies over the full sky area of lenses (here, limited RA/DEC range)
# For simplicity, we'll query a broad area covering lens positions +/- 10 deg in RA and DEC

ra_min, ra_max = df_test['RA'].min() - 10, df_test['RA'].max() + 10
dec_min, dec_max = df_test['DEC'].min() - 10, df_test['DEC'].max() + 10

query_str = f"""
SELECT ra, dec FROM PhotoObj
WHERE ra BETWEEN {ra_min} AND {ra_max}
AND dec BETWEEN {dec_min} AND {dec_max}
AND type = 3  -- type=3 means galaxy
"""

galaxy_table = SDSS.query_sql(query_str)
print(f"Galaxies retrieved: {len(galaxy_table)}")

# Step 4: Convert galaxy coordinates to healpix pixels and count
galaxy_coords = SkyCoord(ra=galaxy_table['ra'], dec=galaxy_table['dec'], unit='deg')
pixels = hp.ang2pix(nside, galaxy_coords.ra.deg, galaxy_coords.dec.deg, lonlat=True)
for p in pixels:
    galaxy_counts[p] += 1

print(f"Max galaxy count in any pixel: {galaxy_counts.max()}")

# Step 5: Assign each lens to a pixel and get its density
lens_coords = SkyCoord(ra=df_test['RA'], dec=df_test['DEC'], unit='deg')
lens_pixels = hp.ang2pix(nside, lens_coords.ra.deg, lens_coords.dec.deg, lonlat=True)
df_test['healpix_pixel'] = lens_pixels
df_test['galaxy_density'] = galaxy_counts[lens_pixels]

# Step 6: Define density bins (33% quantiles)
q_low, q_med = np.percentile(galaxy_counts[galaxy_counts > 0], [33, 67])  # ignore zero-count pixels

def density_category(count):
    if count == 0:
        return 'low'
    elif count <= q_low:
        return 'low'
    elif count <= q_med:
        return 'medium'
    else:
        return 'high'

df_test['density_category'] = df_test['galaxy_density'].apply(density_category)

# Step 7: Show counts per density category
print(df_test[['RA', 'DEC', 'galaxy_density', 'density_category']].head(20))
print("\nLens counts per density category:")
print(df_test['density_category'].value_counts())

# Install dependencies if not already installed (uncomment if running fresh)
# !pip install lenscat healpy astroquery astropy --quiet

import numpy as np
import pandas as pd
import healpy as hp
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.sdss import SDSS
from lenscat import catalog

# --- Step 1: Load confident lenses with redshift ---
print("Loading confident lenses with redshift from lenscat package...")
df = catalog.to_pandas()
df_strong = df[df['grading'].isin(['confident', 'probable'])].copy()
df_strong['z'] = pd.to_numeric(df_strong['zlens'], errors='coerce')
df_strong = df_strong.dropna(subset=['z', 'RA', 'DEC']).reset_index(drop=True)
print(f"Confident lenses with redshift: {len(df_strong):,}")

# --- Step 2: Query SDSS galaxies ---
print("Querying SDSS for galaxy data (this may take a minute)...")
query = "SELECT TOP 10000 ra, dec FROM PhotoObj WHERE type = 3"
galaxy_table = SDSS.query_sql(query)

if galaxy_table is None:
    raise RuntimeError("SDSS query returned no results. Try again later.")

print("Galaxy table columns:", galaxy_table.colnames)

# Step 3: Detect RA/DEC column names robustly
ra_col = next((c for c in galaxy_table.colnames if c.lower() == 'ra'), None)
dec_col = next((c for c in galaxy_table.colnames if c.lower() == 'dec'), None)

if ra_col is None or dec_col is None:
    raise ValueError("RA or DEC columns not found in galaxy_table")

# Step 4: Create SkyCoord for galaxies
galaxy_coords = SkyCoord(ra=galaxy_table[ra_col], dec=galaxy_table[dec_col], unit=u.deg)

# --- Step 5: Build Healpix density map ---
NSIDE = 64
NPIX = hp.nside2npix(NSIDE)

pixels = hp.ang2pix(NSIDE, galaxy_coords.ra.deg, galaxy_coords.dec.deg, lonlat=True)
density_map = np.bincount(pixels, minlength=NPIX)

print(f"Generated Healpix density map (NSIDE={NSIDE}) with {NPIX} pixels")
print(f"Max galaxy count in any pixel: {density_map.max()}")

# --- Step 6: Assign densities to lenses ---
lens_coords = SkyCoord(ra=df_strong['RA'].values * u.deg, dec=df_strong['DEC'].values * u.deg)
lens_pixels = hp.ang2pix(NSIDE, lens_coords.ra.deg, lens_coords.dec.deg, lonlat=True)
lens_densities = density_map[lens_pixels]

df_strong['galaxy_density'] = lens_densities

# --- Step 7: Categorize lens environments by density tertiles ---
low_thresh = np.percentile(density_map[density_map > 0], 33)
high_thresh = np.percentile(density_map[density_map > 0], 66)

def categorize_density(val):
    if val == 0:
        return 'void'
    elif val < low_thresh:
        return 'low'
    elif val < high_thresh:
        return 'medium'
    else:
        return 'high'

df_strong['density_category'] = df_strong['galaxy_density'].apply(categorize_density)

# --- Step 8: Print summary ---
print("\nDensity category counts:")
print(df_strong['density_category'].value_counts())

print("\nSample lenses with density categories:")
print(df_strong[['RA', 'DEC', 'z', 'galaxy_density', 'density_category']].head(10))

import os
import urllib.request

filename = "dr7_galaxy_density_nside64.fits"
url = "https://skyview.gsfc.nasa.gov/tempspace/fits/skymaps/dr7_galaxy_density_nside64.fits"

if not os.path.exists(filename):
    print(f"Downloading {filename} ...")
    urllib.request.urlretrieve(url, filename)
    print("Download complete.")
else:
    print(f"{filename} already exists.")

url = "https://github.com/ewanbarr/sdss-healpix-maps/raw/main/dr7_galaxy_density_nside64.fits"
filename = "dr7_galaxy_density_nside64.fits"

if not os.path.exists(filename):
    print(f"Downloading {filename} ...")
    urllib.request.urlretrieve(url, filename)
    print("Download complete.")
else:
    print(f"{filename} already exists.")

import os
import urllib.request
import healpy as hp
import matplotlib.pyplot as plt

url = "https://data.sdss.org/sas/dr12/boss/lss/DR12v5_CMASS_North.fits"
filename = "DR12v5_CMASS_North.fits"

if not os.path.exists(filename):
    print(f"Downloading {filename} ...")
    urllib.request.urlretrieve(url, filename)
    print("Download complete.")
else:
    print(f"{filename} already exists.")

density_map = hp.read_map(filename)
nside = hp.get_nside(density_map)
print(f"Loaded Healpix map with NSIDE = {nside}")

hp.mollview(density_map, title="SDSS DR12 CMASS Galaxy Density", unit="Counts")
plt.show()

https://irsa.ipac.caltech.edu/data/2MASS/Documentation/2mrs/2mrs_nside64.fits

import os
import urllib.request
import healpy as hp
import numpy as np
from astropy.coordinates import SkyCoord
import astropy.units as u

url = "https://irsa.ipac.caltech.edu/data/2MASS/Documentation/2mrs/2mrs_nside64.fits"
filename = "2mrs_nside64.fits"

# Download the file if it does not exist
if not os.path.exists(filename):
    print(f"Downloading {filename} ...")
    urllib.request.urlretrieve(url, filename)
    print("Download complete.")
else:
    print(f"{filename} already exists, skipping download.")

# Load the Healpix map
density_map = hp.read_map(filename)
nside = hp.get_nside(density_map)
print(f"Loaded Healpix map with NSIDE = {nside}")

# Example lens coordinates (replace with your own)
lens_ra = np.array([12.6, 18.1, 18.6])  # degrees
lens_dec = np.array([-17.7, -16.8, 7.3])  # degrees

lens_coords = SkyCoord(ra=lens_ra*u.deg, dec=lens_dec*u.deg)

theta = np.radians(90.0 - lens_coords.dec.deg)  # colatitude in radians
phi = np.radians(lens_coords.ra.deg)  # longitude in radians

pixels = hp.ang2pix(nside, theta, phi)

lens_densities = density_map[pixels]

for ra, dec, dens in zip(lens_ra, lens_dec, lens_densities):
    print(f"Lens at RA={ra:.2f}, Dec={dec:.2f} has galaxy density = {dens:.2f}")

from astroquery.sdss import SDSS
from astropy.coordinates import SkyCoord
import astropy.units as u

# Example: query galaxies within 1 degree of a point
coords = SkyCoord(ra=150, dec=2, unit=(u.deg, u.deg))
result = SDSS.query_region(coords, radius='1 deg', spectro=True, photoobj_fields=['ra', 'dec', 'type'])
print(result)

File "/tmp/ipython-input-39-3877696123.py", line 75
    'ra': [/* your 100 RA values here */],
           ^
SyntaxError: invalid syntax

from astroquery.sdss import SDSS
from astropy.coordinates import SkyCoord, Angle
import astropy.units as u
from astropy.table import vstack
import numpy as np
import pandas as pd

def query_sdss_tiled(center_coord, total_radius_deg=1.0, tile_radius_arcmin=3.0):
    tile_radius_deg = tile_radius_arcmin / 60.0
    n_tiles_side = int(np.ceil((2 * total_radius_deg) / tile_radius_deg))
    all_results = []

    ra_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)
    dec_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)

    for ra_off in ra_offsets:
        for dec_off in dec_offsets:
            tile_center = SkyCoord(ra=center_coord.ra.deg + ra_off,
                                   dec=center_coord.dec.deg + dec_off,
                                   unit='deg')
            try:
                result = SDSS.query_region(tile_center,
                                          radius=Angle(tile_radius_arcmin, u.arcmin),
                                          spectro=False,
                                          photoobj_fields=['ra', 'dec', 'type'])
                if result is not None:
                    all_results.append(result)
            except Exception as e:
                print(f"Error querying tile at RA={tile_center.ra.deg}, DEC={tile_center.dec.deg}: {e}")

    if all_results:
        combined = vstack(all_results)
        coords_all = SkyCoord(ra=combined['ra'], dec=combined['dec'], unit='deg')
        mask = coords_all.separation(center_coord) <= Angle(total_radius_deg, u.deg)
        filtered = combined[mask]
        return filtered
    else:
        return None

# Map SDSS 'type' integer to typical stellar mass in solar masses
def sdss_type_to_mass(sdss_type):
    # SDSS PhotoObj.type values:
    # 3 = star, 6 = galaxy, other codes possible
    if sdss_type == 6:
        return 5e10  # Typical stellar mass for a galaxy (rough estimate)
    else:
        return 0  # Ignore stars and unknown types

def surface_mass_density(total_mass, redshift, radius_arcmin=3):
    if redshift is None or np.isnan(redshift):
        return np.nan
    from astropy.cosmology import Planck18 as cosmo
    theta_rad = radius_arcmin * (np.pi / 180) / 60
    d_a = cosmo.angular_diameter_distance(redshift).to(u.Mpc).value
    radius_mpc = theta_rad * d_a
    area = np.pi * radius_mpc**2
    return total_mass / area

def estimate_mass_for_lens(lens, radius_arcmin=3):
    center_coord = SkyCoord(ra=lens['ra'], dec=lens['dec'], unit='deg')
    galaxies = query_sdss_tiled(center_coord, total_radius_deg=radius_arcmin/60, tile_radius_arcmin=3)
    if galaxies is None or len(galaxies) == 0:
        print(f"No galaxies found near lens {lens['lens_id']}")
        return 0.0, 0.0

    total_mass = 0.0
    for gal in galaxies:
        gal_type = gal['type']
        mass = sdss_type_to_mass(gal_type)
        total_mass += mass

    sigma = surface_mass_density(total_mass, lens['z'], radius_arcmin)
    return total_mass, sigma

# Example lenses dataframe (replace with your real 100 lenses)
lenses = pd.DataFrame({
    'lens_id': ['L1', 'L2'],
    'ra': [150.1, 149.9],
    'dec': [2.3, 2.5],
    'z': [0.3, 0.45]
})

results = []
for _, lens in lenses.iterrows():
    print(f"Processing lens {lens['lens_id']}...")
    mass, sigma = estimate_mass_for_lens(lens, radius_arcmin=3)
    print(f"Lens {lens['lens_id']}: Total Mass = {mass:.2e} M☉, Surface Density = {sigma:.2e} M☉/Mpc²")
    results.append({'lens_id': lens['lens_id'], 'total_mass_Msun': mass, 'mass_surface_density_Msun_per_Mpc2': sigma})

results_df = pd.DataFrame(results)
results_df.to_csv('lens_mass_from_sdss.csv', index=False)
print("Mass estimation done and saved to 'lens_mass_from_sdss.csv'")

# Install lenscat if needed:
# !pip install lenscat astroquery astropy pandas numpy

from lenscat import stronglenses
from astroquery.sdss import SDSS
from astropy.coordinates import SkyCoord, Angle
import astropy.units as u
from astropy.table import vstack
import numpy as np
import pandas as pd
from astropy.cosmology import Planck18 as cosmo

def query_sdss_tiled(center_coord, total_radius_deg=1.0, tile_radius_arcmin=3.0):
    tile_radius_deg = tile_radius_arcmin / 60.0
    n_tiles_side = int(np.ceil((2 * total_radius_deg) / tile_radius_deg))
    all_results = []

    ra_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)
    dec_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)

    for ra_off in ra_offsets:
        for dec_off in dec_offsets:
            tile_center = SkyCoord(ra=center_coord.ra.deg + ra_off,
                                   dec=center_coord.dec.deg + dec_off,
                                   unit='deg')
            try:
                result = SDSS.query_region(tile_center,
                                          radius=Angle(tile_radius_arcmin, u.arcmin),
                                          spectro=False,
                                          photoobj_fields=['ra', 'dec', 'type'])
                if result is not None:
                    all_results.append(result)
            except Exception as e:
                print(f"Error querying tile at RA={tile_center.ra.deg}, DEC={tile_center.dec.deg}: {e}")

    if all_results:
        combined = vstack(all_results)
        coords_all = SkyCoord(ra=combined['ra'], dec=combined['dec'], unit='deg')
        mask = coords_all.separation(center_coord) <= Angle(total_radius_deg, u.deg)
        filtered = combined[mask]
        return filtered
    else:
        return None

def sdss_type_to_mass(sdss_type):
    if sdss_type == 6:
        return 5e10  # typical stellar mass for a galaxy in solar masses
    else:
        return 0

def surface_mass_density(total_mass, redshift, radius_arcmin=3):
    if redshift is None or np.isnan(redshift):
        return np.nan
    theta_rad = radius_arcmin * (np.pi / 180) / 60
    d_a = cosmo.angular_diameter_distance(redshift).to(u.Mpc).value
    radius_mpc = theta_rad * d_a
    area = np.pi * radius_mpc**2
    return total_mass / area

# Load strong lenses catalog from lenscat
catalog = stronglenses()

# Filter for confident lenses with valid redshift
lenses_df = catalog[catalog['grade'].isin(['C', 'B']) & (~catalog['z_lens'].isna())]

# Take first 100 lenses (or change this number as needed)
lenses = lenses_df.head(100)

results = []
for idx, lens in lenses.iterrows():
    lens_id = lens['lens_name']
    ra = lens['ra']
    dec = lens['dec']
    z = lens['z_lens']
    print(f"Querying lens {lens_id} at RA={ra:.4f}, DEC={dec:.4f}")

    center_coord = SkyCoord(ra=ra, dec=dec, unit='deg')
    galaxies = query_sdss_tiled(center_coord, total_radius_deg=3/60, tile_radius_arcmin=3)
    if galaxies is None or len(galaxies) == 0:
        print(f"  No galaxies found for lens {lens_id}")
        total_mass = 0.0
    else:
        total_mass = sum(sdss_type_to_mass(t) for t in galaxies['type'])
        print(f"  Found {len(galaxies)} galaxies, total stellar mass approx: {total_mass:.2e} M☉")

    sigma = surface_mass_density(total_mass, z, radius_arcmin=3)
    results.append({
        'lens_id': lens_id,
        'ra': ra,
        'dec': dec,
        'redshift': z,
        'total_mass_Msun': total_mass,
        'mass_surface_density_Msun_per_Mpc2': sigma
    })

results_df = pd.DataFrame(results)
results_df.to_csv('lens_stellar_mass_lenscat_100.csv', index=False)
print("Done. Results saved to 'lens_stellar_mass_lenscat_100.csv'")

from lenscat import catalog
from astroquery.sdss import SDSS
from astropy.coordinates import SkyCoord, Angle
import astropy.units as u
from astropy.table import vstack
import numpy as np
import pandas as pd
from astropy.cosmology import Planck18 as cosmo

def query_sdss_tiled(center_coord, total_radius_deg=1.0, tile_radius_arcmin=3.0):
    tile_radius_deg = tile_radius_arcmin / 60.0
    n_tiles_side = int(np.ceil((2 * total_radius_deg) / tile_radius_deg))
    all_results = []

    ra_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)
    dec_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)

    for ra_off in ra_offsets:
        for dec_off in dec_offsets:
            tile_center = SkyCoord(ra=center_coord.ra.deg + ra_off,
                                   dec=center_coord.dec.deg + dec_off,
                                   unit='deg')
            try:
                result = SDSS.query_region(tile_center,
                                          radius=Angle(tile_radius_arcmin, u.arcmin),
                                          spectro=False,
                                          photoobj_fields=['ra', 'dec', 'type'])
                if result is not None:
                    all_results.append(result)
            except Exception as e:
                print(f"Error querying tile at RA={tile_center.ra.deg}, DEC={tile_center.dec.deg}: {e}")

    if all_results:
        combined = vstack(all_results)
        coords_all = SkyCoord(ra=combined['ra'], dec=combined['dec'], unit='deg')
        mask = coords_all.separation(center_coord) <= Angle(total_radius_deg, u.deg)
        filtered = combined[mask]
        return filtered
    else:
        return None

def sdss_type_to_mass(sdss_type):
    if sdss_type == 6:
        return 5e10  # typical stellar mass for a galaxy in solar masses
    else:
        return 0

def surface_mass_density(total_mass, redshift, radius_arcmin=3):
    if redshift is None or np.isnan(redshift):
        return np.nan
    theta_rad = radius_arcmin * (np.pi / 180) / 60
    d_a = cosmo.angular_diameter_distance(redshift).to(u.Mpc).value
    radius_mpc = theta_rad * d_a
    area = np.pi * radius_mpc**2
    return total_mass / area

# Load strong lenses catalog DataFrame from lenscat
catalog_df = catalog.df

# Filter for confident lenses with valid redshift
lenses_df = catalog_df[catalog_df['grade'].isin(['C', 'B']) & (~catalog_df['z_lens'].isna())]

# Take first 100 lenses (or change as needed)
lenses = lenses_df.head(100)

results = []
for idx, lens in lenses.iterrows():
    lens_id = lens['lens_name']
    ra = lens['ra']
    dec = lens['dec']
    z = lens['z_lens']
    print(f"Querying lens {lens_id} at RA={ra:.4f}, DEC={dec:.4f}")

    center_coord = SkyCoord(ra=ra, dec=dec, unit='deg')
    galaxies = query_sdss_tiled(center_coord, total_radius_deg=3/60, tile_radius_arcmin=3)
    if galaxies is None or len(galaxies) == 0:
        print(f"  No galaxies found for lens {lens_id}")
        total_mass = 0.0
    else:
        total_mass = sum(sdss_type_to_mass(t) for t in galaxies['type'])
        print(f"  Found {len(galaxies)} galaxies, total stellar mass approx: {total_mass:.2e} M☉")

    sigma = surface_mass_density(total_mass, z, radius_arcmin=3)
    results.append({
        'lens_id': lens_id,
        'ra': ra,
        'dec': dec,
        'redshift': z,
        'total_mass_Msun': total_mass,
        'mass_surface_density_Msun_per_Mpc2': sigma
    })

results_df = pd.DataFrame(results)
results_df.to_csv('lens_stellar_mass_lenscat_100.csv', index=False)
print("Done. Results saved to 'lens_stellar_mass_lenscat_100.csv'")

# Install dependencies if needed
# !pip install lenscat astroquery astropy pandas numpy

from lenscat import catalog
from astroquery.sdss import SDSS
from astropy.coordinates import SkyCoord, Angle
import astropy.units as u
from astropy.table import vstack
import numpy as np
import pandas as pd
from astropy.cosmology import Planck18 as cosmo

def query_sdss_tiled(center_coord, total_radius_deg=1.0, tile_radius_arcmin=3.0):
    tile_radius_deg = tile_radius_arcmin / 60.0
    n_tiles_side = int(np.ceil((2 * total_radius_deg) / tile_radius_deg))
    all_results = []

    ra_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)
    dec_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)

    for ra_off in ra_offsets:
        for dec_off in dec_offsets:
            tile_center = SkyCoord(ra=center_coord.ra.deg + ra_off,
                                   dec=center_coord.dec.deg + dec_off,
                                   unit='deg')
            try:
                result = SDSS.query_region(tile_center,
                                          radius=Angle(tile_radius_arcmin, u.arcmin),
                                          spectro=False,
                                          photoobj_fields=['ra', 'dec', 'type'])
                if result is not None:
                    all_results.append(result)
            except Exception as e:
                print(f"Error querying tile at RA={tile_center.ra.deg}, DEC={tile_center.dec.deg}: {e}")

    if all_results:
        combined = vstack(all_results)
        coords_all = SkyCoord(ra=combined['ra'], dec=combined['dec'], unit='deg')
        mask = coords_all.separation(center_coord) <= Angle(total_radius_deg, u.deg)
        filtered = combined[mask]
        return filtered
    else:
        return None

def sdss_type_to_mass(sdss_type):
    if sdss_type == 6:
        return 5e10  # typical stellar mass for a galaxy in solar masses
    else:
        return 0

def surface_mass_density(total_mass, redshift, radius_arcmin=3):
    if redshift is None or np.isnan(redshift):
        return np.nan
    theta_rad = radius_arcmin * (np.pi / 180) / 60
    d_a = cosmo.angular_diameter_distance(redshift).to(u.Mpc).value
    radius_mpc = theta_rad * d_a
    area = np.pi * radius_mpc**2
    return total_mass / area

# Create Catalog instance and load DataFrame
cat = catalog.Catalog()
catalog_df = cat.df

# Filter for confident lenses with valid redshift
lenses_df = catalog_df[catalog_df['grade'].isin(['C', 'B']) & (~catalog_df['z_lens'].isna())]

# Select first 100 lenses (adjust as needed)
lenses = lenses_df.head(100)

results = []
for idx, lens in lenses.iterrows():
    lens_id = lens['lens_name']
    ra = lens['ra']
    dec = lens['dec']
    z = lens['z_lens']
    print(f"Querying lens {lens_id} at RA={ra:.4f}, DEC={dec:.4f}")

    center_coord = SkyCoord(ra=ra, dec=dec, unit='deg')
    galaxies = query_sdss_tiled(center_coord, total_radius_deg=3/60, tile_radius_arcmin=3)
    if galaxies is None or len(galaxies) == 0:
        print(f"  No galaxies found for lens {lens_id}")
        total_mass = 0.0
    else:
        total_mass = sum(sdss_type_to_mass(t) for t in galaxies['type'])
        print(f"  Found {len(galaxies)} galaxies, total stellar mass approx: {total_mass:.2e} M☉")

    sigma = surface_mass_density(total_mass, z, radius_arcmin=3)
    results.append({
        'lens_id': lens_id,
        'ra': ra,
        'dec': dec,
        'redshift': z,
        'total_mass_Msun': total_mass,
        'mass_surface_density_Msun_per_Mpc2': sigma
    })

results_df = pd.DataFrame(results)
results_df.to_csv('lens_stellar_mass_lenscat_100.csv', index=False)
print("Done. Results saved to 'lens_stellar_mass_lenscat_100.csv'")

import lenscat
print(dir(lenscat))

import lensstat
print(dir(lensstat))

!pip install lensstat

import lensstat
print(dir(lensstat))

# Step 1: Install lenscat if not installed (run once in notebook)
# !pip install -q lenscat astroquery pandas numpy astropy matplotlib

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from astropy.table import Table
import lenscat
from astroquery.sdss import SDSS
from astropy.coordinates import SkyCoord, Angle
import astropy.units as u
from astropy.table import vstack
from astropy.cosmology import Planck18 as cosmo

# Load lenscat catalog
print("Loading full catalog from lenscat package...")
catalog: Table = lenscat.catalog
print(f"✅ Total lenses loaded: {len(catalog):,}")

# Convert to pandas DataFrame
df = catalog.to_pandas()
print("\nGrading counts:")
print(df['grading'].value_counts())

# Filter confident/probable lenses with redshift
df_strong = df[df['grading'].isin(['confident', 'probable'])].copy()
df_strong['z'] = pd.to_numeric(df_strong['zlens'], errors='coerce')
df_strong = df_strong.dropna(subset=['z']).reset_index(drop=True)
print(f"\nStrong lenses with redshift: {len(df_strong):,}")

# Define SDSS tiled query function
def query_sdss_tiled(center_coord, total_radius_deg=1.0, tile_radius_arcmin=3.0):
    tile_radius_deg = tile_radius_arcmin / 60.0
    n_tiles_side = int(np.ceil((2 * total_radius_deg) / tile_radius_deg))
    all_results = []

    ra_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)
    dec_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)

    for ra_off in ra_offsets:
        for dec_off in dec_offsets:
            tile_center = SkyCoord(ra=center_coord.ra.deg + ra_off,
                                   dec=center_coord.dec.deg + dec_off,
                                   unit='deg')
            try:
                result = SDSS.query_region(tile_center,
                                          radius=Angle(tile_radius_arcmin, u.arcmin),
                                          spectro=False,
                                          photoobj_fields=['ra', 'dec', 'type'])
                if result is not None:
                    all_results.append(result)
            except Exception as e:
                print(f"Error querying tile at RA={tile_center.ra.deg}, DEC={tile_center.dec.deg}: {e}")

    if all_results:
        combined = vstack(all_results)
        coords_all = SkyCoord(ra=combined['ra'], dec=combined['dec'], unit='deg')
        mask = coords_all.separation(center_coord) <= Angle(total_radius_deg, u.deg)
        filtered = combined[mask]
        return filtered
    else:
        return None

# Map SDSS 'type' integer to typical stellar mass (solar masses)
def sdss_type_to_mass(sdss_type):
    if sdss_type == 6:  # galaxy
        return 5e10
    else:
        return 0

def surface_mass_density(total_mass, redshift, radius_arcmin=3):
    if redshift is None or pd.isna(redshift):
        return np.nan
    theta_rad = radius_arcmin * (np.pi / 180) / 60
    d_a = cosmo.angular_diameter_distance(redshift).to(u.Mpc).value
    radius_mpc = theta_rad * d_a
    area = np.pi * radius_mpc**2
    return total_mass / area

# Limit to first 100 strong lenses (adjust if you want more)
df_100 = df_strong.head(100)

results = []
for idx, lens in df_100.iterrows():
    lens_id = lens['name']  # or 'lens_name' if that exists
    ra = lens['ra']
    dec = lens['dec']
    z = lens['z']

    print(f"Querying lens {lens_id} at RA={ra:.4f}, DEC={dec:.4f}")

    center_coord = SkyCoord(ra=ra, dec=dec, unit='deg')
    galaxies = query_sdss_tiled(center_coord, total_radius_deg=3/60, tile_radius_arcmin=3)
    if galaxies is None or len(galaxies) == 0:
        print(f"  No galaxies found for lens {lens_id}")
        total_mass = 0.0
    else:
        total_mass = sum(sdss_type_to_mass(t) for t in galaxies['type'])
        print(f"  Found {len(galaxies)} galaxies, total stellar mass approx: {total_mass:.2e} M☉")

    sigma = surface_mass_density(total_mass, z, radius_arcmin=3)
    results.append({
        'lens_id': lens_id,
        'ra': ra,
        'dec': dec,
        'redshift': z,
        'total_mass_Msun': total_mass,
        'mass_surface_density_Msun_per_Mpc2': sigma
    })

results_df = pd.DataFrame(results)
results_df.to_csv('lens_stellar_mass_lenscat_100.csv', index=False)
print("Done. Results saved to 'lens_stellar_mass_lenscat_100.csv'")

# Optional: plot distribution of total mass
plt.hist(results_df['total_mass_Msun'], bins=30)
plt.xlabel('Total Stellar Mass (M☉)')
plt.ylabel('Number of Lenses')
plt.title('Distribution of Stellar Mass Around Lenses')
plt.show()

print(df_strong.columns)

# Step 1: Install lenscat and astroquery if needed (run once)
# !pip install -q lenscat astroquery pandas numpy astropy matplotlib

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from astropy.table import Table
import lenscat
from astroquery.sdss import SDSS
from astropy.coordinates import SkyCoord, Angle
import astropy.units as u
from astropy.table import vstack
from astropy.cosmology import Planck18 as cosmo

# Load lenscat catalog
print("Loading full catalog from lenscat package...")
catalog: Table = lenscat.catalog
print(f"✅ Total lenses loaded: {len(catalog):,}")

# Convert to pandas DataFrame
df = catalog.to_pandas()
print("\nGrading counts:")
print(df['grading'].value_counts())

# Filter confident/probable lenses with redshift
df_strong = df[df['grading'].isin(['confident', 'probable'])].copy()
df_strong['z'] = pd.to_numeric(df_strong['zlens'], errors='coerce')
df_strong = df_strong.dropna(subset=['z']).reset_index(drop=True)
print(f"\nStrong lenses with redshift: {len(df_strong):,}")

# Use the 'RA', 'DEC', 'z', 'name' columns for positions and redshift
# NOTE: Using 'z' column you confirmed exists
df_strong = df_strong.rename(columns={'zlens': 'z'})  # Just to be sure if needed

# SDSS tiled query function
def query_sdss_tiled(center_coord, total_radius_deg=1.0, tile_radius_arcmin=3.0):
    tile_radius_deg = tile_radius_arcmin / 60.0
    n_tiles_side = int(np.ceil((2 * total_radius_deg) / tile_radius_deg))
    all_results = []

    ra_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)
    dec_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)

    for ra_off in ra_offsets:
        for dec_off in dec_offsets:
            tile_center = SkyCoord(ra=center_coord.ra.deg + ra_off,
                                   dec=center_coord.dec.deg + dec_off,
                                   unit='deg')
            try:
                result = SDSS.query_region(tile_center,
                                          radius=Angle(tile_radius_arcmin, u.arcmin),
                                          spectro=False,
                                          photoobj_fields=['ra', 'dec', 'type'])
                if result is not None:
                    all_results.append(result)
            except Exception as e:
                print(f"Error querying tile at RA={tile_center.ra.deg}, DEC={tile_center.dec.deg}: {e}")

    if all_results:
        combined = vstack(all_results)
        coords_all = SkyCoord(ra=combined['ra'], dec=combined['dec'], unit='deg')
        mask = coords_all.separation(center_coord) <= Angle(total_radius_deg, u.deg)
        filtered = combined[mask]
        return filtered
    else:
        return None

# Map SDSS 'type' integer to typical stellar mass (solar masses)
def sdss_type_to_mass(sdss_type):
    if sdss_type == 6:  # galaxy
        return 5e10
    else:
        return 0

def surface_mass_density(total_mass, redshift, radius_arcmin=3):
    if redshift is None or pd.isna(redshift):
        return np.nan
    theta_rad = radius_arcmin * (np.pi / 180) / 60
    d_a = cosmo.angular_diamete

# Step 1: Install lenscat and astroquery if needed (run once)
# !pip install -q lenscat astroquery pandas numpy astropy matplotlib

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from astropy.table import Table
import lenscat
from astroquery.sdss import SDSS
from astropy.coordinates import SkyCoord, Angle
import astropy.units as u
from astropy.table import vstack
from astropy.cosmology import Planck18 as cosmo

# Load lenscat catalog
print("Loading full catalog from lenscat package...")
catalog: Table = lenscat.catalog
print(f"✅ Total lenses loaded: {len(catalog):,}")

# Convert to pandas DataFrame
df = catalog.to_pandas()
print("\nGrading counts:")
print(df['grading'].value_counts())

# Filter confident/probable lenses with redshift
df_strong = df[df['grading'].isin(['confident', 'probable'])].copy()
df_strong['z'] = pd.to_numeric(df_strong['zlens'], errors='coerce')
df_strong = df_strong.dropna(subset=['z']).reset_index(drop=True)
print(f"\nStrong lenses with redshift: {len(df_strong):,}")

# Use first 5 lenses for quick test, increase to 100 when ready
df_50 = df_strong.head(50)

# SDSS tiled query function
def query_sdss_tiled(center_coord, total_radius_deg=1.0, tile_radius_arcmin=3.0):
    tile_radius_deg = tile_radius_arcmin / 60.0
    n_tiles_side = int(np.ceil((2 * total_radius_deg) / tile_radius_deg))
    all_results = []

    ra_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)
    dec_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)

    for ra_off in ra_offsets:
        for dec_off in dec_offsets:
            tile_center = SkyCoord(ra=center_coord.ra.deg + ra_off,
                                   dec=center_coord.dec.deg + dec_off,
                                   unit='deg')
            try:
                result = SDSS.query_region(tile_center,
                                          radius=Angle(tile_radius_arcmin, u.arcmin),
                                          spectro=False,
                                          photoobj_fields=['ra', 'dec', 'type'])
                if result is not None:
                    all_results.append(result)
            except Exception as e:
                print(f"Error querying tile at RA={tile_center.ra.deg}, DEC={tile_center.dec.deg}: {e}")

    if all_results:
        combined = vstack(all_results)
        coords_all = SkyCoord(ra=combined['ra'], dec=combined['dec'], unit='deg')
        mask = coords_all.separation(center_coord) <= Angle(total_radius_deg, u.deg)
        filtered = combined[mask]
        return filtered
    else:
        return None

# Map SDSS 'type' integer to typical stellar mass (solar masses)
def sdss_type_to_mass(sdss_type):
    if sdss_type == 6:  # galaxy
        return 5e10
    else:
        return 0

def surface_mass_density(total_mass, redshift, radius_arcmin=3):
    if redshift is None or pd.isna(redshift):
        return np.nan
    theta_rad = radius_arcmin * (np.pi / 180) / 60
    d_a = cosmo.angular_diameter_distance(redshift).to(u.Mpc).value
    radius_mpc = theta_rad * d_a
    area = np.pi * radius_mpc**2
    return total_mass / area

results = []

for idx, lens in df_5.iterrows():
    lens_id = lens['name']
    ra = lens['RA']
    dec = lens['DEC']
    z = lens['z']

    print(f"Querying lens {lens_id} at RA={ra:.4f}, DEC={dec:.4f}")

    center_coord = SkyCoord(ra=ra, dec=dec, unit='deg')

    try:
        galaxies = query_sdss_tiled(center_coord, total_radius_deg=3/60, tile_radius_arcmin=3)
    except Exception as e:
        print(f"Error querying lens {lens_id}: {e}")
        galaxies = None

    if galaxies is None or len(galaxies) == 0:
        print(f"  No galaxies found for lens {lens_id}")
        total_mass = 0.0
    else:
        total_mass = sum(sdss_type_to_mass(t) for t in galaxies['type'])
        print(f"  Found {len(galaxies)} galaxies, total stellar mass approx: {total_mass:.2e} M☉")

    sigma = surface_mass_density(total_mass, z, radius_arcmin=3)

    results.append({
        'lens_id': lens_id,
        'ra': ra,
        'dec': dec,
        'redshift': z,
        'total_mass_Msun': total_mass,
        'mass_surface_density_Msun_per_Mpc2': sigma
    })

    # Save partial results every lens (can change to every 10 if you do more)
    pd.DataFrame(results).to_csv('lens_stellar_mass_lenscat_partial.csv', index=False)
    print(f"Saved partial results for {idx+1} lenses")

# Save final results
results_df = pd.DataFrame(results)
results_df.to_csv('lens_stellar_mass_lenscat.csv', index=False)
print("Done. Results saved to 'lens_stellar_mass_lenscat.csv'")

# Optional: plot distribution of total mass
plt.hist(results_df['total_mass_Msun'], bins=30)
plt.xlabel('Total Stellar Mass (M☉)')
plt.ylabel('Number of Lenses')
plt.title('Distribution of Stellar Mass Around Lenses')
plt.show()

import pandas as pd
import numpy as np
from astroquery.vizier import Vizier
from astropy.coordinates import SkyCoord
from astropy import units as u
import os
from lenscat import load_lens_catalog

# Load full lens catalog
print("Loading full catalog from lenscat package...")
df = load_lens_catalog()
print(f"✅ Total lenses loaded: {len(df)}")

# Filter confident/probable lenses with valid redshift
df_strong = df[df['grading'].isin(['confident', 'probable'])]
df_strong = df_strong[df_strong['z'].notnull()]
print(f"\nStrong lenses with redshift: {len(df_strong)}")

# Batch size
BATCH_SIZE = 50

# Output file
outfile = "lens_stellar_mass_lenscat.csv"

# Resume support
if os.path.exists(outfile):
    df_results = pd.read_csv(outfile)
    done_names = set(df_results['name'])
    print(f"🔄 Resuming from previous run, {len(done_names)} lenses already completed.")
else:
    df_results = pd.DataFrame(columns=['name', 'RA', 'DEC', 'z', 'n_galaxies', 'total_mass'])
    done_names = set()

# Subset for next batch
df_batch = df_strong[~df_strong['name'].isin(done_names)].head(BATCH_SIZE)

# Set up Vizier
Vizier.ROW_LIMIT = -1
catalogs = ["VII/288/glade2", "II/349/ps1", "II/328/allwise", "VII/118/sdss12", "I/345/gaia2"]

def query_galaxies(ra, dec, radius=5.0):
    coord = SkyCoord(ra=ra*u.deg, dec=dec*u.deg)
    total_mass = 0
    all_matches = 0
    for cat in catalogs:
        try:
            result = Vizier.query_region(coord, radius=radius*u.arcmin, catalog=cat)
            if result:
                tab = result[0]
                all_matches += len(tab)
                if 'logM' in tab.colnames:
                    total_mass += np.nansum(10**tab['logM'])
                elif 'Mass' in tab.colnames:
                    total_mass += np.nansum(tab['Mass'])
        except Exception as e:
            print(f"  ⚠️ Vizier error: {e}")
    return all_matches, total_mass

# Process loop
for i, row in df_batch.iterrows():
    name = row['name']
    if name in done_names:
        continue
    ra = row['RA']
    dec = row['DEC']
    z = row['z']
    print(f"\nQuerying lens {name} at RA={ra}, DEC={dec}")
    try:
        n_gal, mass = query_galaxies(ra, dec)
        if n_gal > 0:
            print(f"  ✅ Found {n_gal} galaxies, total stellar mass approx: {mass:.2e} M☉")
        else:
            print(f"  🚫 No galaxies found for lens {name}")
        df_results.loc[len(df_results)] = [name, ra, dec, z, n_gal, mass]
        df_results.to_csv(outfile, index=False)
        print(f"  💾 Saved partial results for {len(df_results)} lenses")
    except Exception as e:
        print(f"  ❌ Error processing {name}: {e}")

print("✅ Done.")

import pandas as pd
import numpy as np
from astropy.coordinates import SkyCoord
from astropy import units as u
from astroquery.sdss import SDSS
import lenscat
import os

# Mount Google Drive if needed
from google.colab import drive
drive.mount('/content/drive')

# Output CSV path
out_csv = "/content/drive/MyDrive/lens_stellar_mass_lenscat.csv"

# Load lens catalog
print("Loading full catalog from lenscat package...")
df_all = lenscat.catalog
print(f"✅ Total lenses loaded: {len(df_all)}\n")

# Show grading counts
print("Grading counts:")
print(df_all["grading"].value_counts(), "\n")

# Rename RA/DEC for consistency
df_all = df_all.rename(columns={"RA": "ra", "DEC": "dec"})

# Filter: strong lenses with redshift
df_strong = df_all[
    (df_all["grading"].isin(["confident", "probable"])) &
    (df_all["z"].notnull())
].copy()

print(f"Strong lenses with redshift: {len(df_strong)}")

# Choose first 50 lenses for test run
df_lenses = df_strong.sample(50, random_state=42).reset_index(drop=True)

# Start query
results = []

for i, row in df_lenses.iterrows():
    name = row["name"]
    ra = row["ra"]
    dec = row["dec"]
    coord = SkyCoord(ra=ra*u.deg, dec=dec*u.deg)
    print(f"Querying lens {name} at RA={ra:.4f}, DEC={dec:.4f}")

    try:
        xid = SDSS.query_region(coord, radius=2*u.arcmin, spectro=False, photoobj_fields=["ra", "dec", "modelMag_r"])
        if xid is None or len(xid) == 0:
            print(f"  No galaxies found for lens {name}")
            results.append({
                "name": name,
                "ra": ra,
                "dec": dec,
                "n_galaxies": 0,
                "total_stellar_mass": 0
            })
        else:
            mags = xid["modelMag_r"]
            masses = 10 ** (0.4 * (4.67 - mags))  # Rough stellar mass est. from r-band mags
            total_mass = masses.sum()
            print(f"  Found {len(mags)} galaxies, total stellar mass approx: {total_mass:.2e} M☉")
            results.append({
                "name": name,
                "ra": ra,
                "dec": dec,
                "n_galaxies": len(mags),
                "total_stellar_mass": total_mass
            })

    except Exception as e:
        print(f"  Error on lens {name}: {e}")
        results.append({
            "name": name,
            "ra": ra,
            "dec": dec,
            "n_galaxies": -1,
            "total_stellar_mass": -1
        })

    # Save after each lens to preserve progress
    pd.DataFrame(results).to_csv(out_csv, index=False)
    print(f"Saved partial results for {len(results)} lenses")

print("Done. Results saved to:", out_csv)

import pandas as pd
import numpy as np
from astropy.coordinates import SkyCoord
from astroquery.sdss import SDSS
from astropy import units as u
from astropy.table import Table
import os

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Load lens catalog
import lenscat
print("Loading full catalog from lenscat package...")
df_all = lenscat.catalog
df_all = df_all.to_pandas()  # ✅ Fix: Convert to Pandas
print(f"✅ Total lenses loaded: {len(df_all)}\n")

# Check grading column
print("Grading counts:")
print(df_all["grading"].value_counts(), "\n")

# Rename for consistency
df_all = df_all.rename(columns={"RA": "ra", "DEC": "dec", "z": "z_lens"})

# Filter: strong lenses with valid redshift
df_strong = df_all[(df_all["grading"].isin(["confident", "probable"])) & (df_all["z_lens"].notna())]
print(f"Strong lenses with redshift: {len(df_strong)}")

# Pick first 50 (adjust as needed)
df_batch = df_strong.head(50)

# Output path
save_path = "/content/drive/MyDrive/lens_stellar_mass_batch50.csv"

# Prepare results
results = []

# Query SDSS for nearby galaxies
for idx, row in df_batch.iterrows():
    name = row["name"]
    ra = row["ra"]
    dec = row["dec"]
    z = row["z_lens"]

    print(f"Querying lens {name} at RA={ra:.4f}, DEC={dec:.4f}")
    coord = SkyCoord(ra=ra * u.deg, dec=dec * u.deg, frame="icrs")

    try:
        # Search 5 arcmin radius
        xid = SDSS.query_region(coord, radius=5 * u.arcmin, photoobj_fields=["ra", "dec", "modelMag_r"])
        if xid is None or len(xid) == 0:
            print(f"  No galaxies found for lens {name}")
            results.append({"name": name, "ra": ra, "dec": dec, "z_lens": z, "n_galaxies": 0, "stellar_mass_total_Msun": 0})
            continue

        # Basic mass estimate: use r-band mag to estimate M* (very crude)
        mags = xid["modelMag_r"].data
        M_star = np.sum(10 ** (-0.4 * (mags - 4.76))) * 1e10  # Convert to solar masses

        print(f"  Found {len(mags)} galaxies, total stellar mass approx: {M_star:.2e} M☉")
        results.append({"name": name, "ra": ra, "dec": dec, "z_lens": z, "n_galaxies": len(mags), "stellar_mass_total_Msun": M_star})

    except Exception as e:
        print(f"  ⚠️ Error for lens {name}: {e}")
        results.append({"name": name, "ra": ra, "dec": dec, "z_lens": z, "n_galaxies": -1, "stellar_mass_total_Msun": -1})
        continue

    # Save after each lens to avoid data loss
    pd.DataFrame(results).to_csv(save_path, index=False)
    print(f"Saved partial results for {len(results)} lenses")

print("✅ Done. Results saved to:", save_path)

# 📦 Imports
import pandas as pd
import numpy as np
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
from astropy import units as u
from lenscat import catalog
import os
from tqdm import tqdm
from google.colab import drive

# 📂 Mount Google Drive
drive.mount('/content/drive')

# 📁 Output directory
save_dir = "/content/drive/MyDrive/lens_bh_batches"
os.makedirs(save_dir, exist_ok=True)

# ✅ Load full lens catalog
print("Loading full catalog from lenscat package...")
df_all = catalog().to_pandas()
print(f"✅ Total lenses loaded: {len(df_all)}\n")

# 🧹 Clean & filter
df_all = df_all.rename(columns={"RA": "ra", "DEC": "dec", "zlens": "z_lens"})
print("Grading counts:")
print(df_all["grading"].value_counts(), "\n")

df_strong = df_all[
    (df_all["grading"].isin(["confident", "probable"])) &
    (~df_all["z_lens"].isna())
].copy()

print(f"✅ Strong lenses with redshift: {len(df_strong)}\n")

# 🧪 Setup SIMBAD
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 120
custom_simbad.remove_votable_fields('coordinates')
custom_simbad.add_votable_fields('otype')

# 🔎 Object types of interest
bh_types = {"BH", "BHXRB", "XRB", "BLAZAR", "AGN", "QSO"}

# ⚙️ Batch settings
batch_size = 50
radius = 20 * u.arcmin  # Search radius
df_strong = df_strong.sample(frac=1, random_state=42).reset_index(drop=True)  # shuffle

# 🧮 Process in batches
def process_batch(df_batch, batch_index):
    results = []
    for _, row in tqdm(df_batch.iterrows(), total=len(df_batch), desc=f"Batch {batch_index+1}"):
        coord = SkyCoord(ra=row["ra"] * u.deg, dec=row["dec"] * u.deg)
        try:
            result = custom_simbad.query_region(coord, radius=radius)
            if result is not None:
                bh_matches = result[result['OTYPE'].isin(bh_types)]
                count = len(bh_matches)
            else:
                count = 0
        except Exception as e:
            print(f"⚠️ SIMBAD error for lens {row['name']}: {e}")
            count = -1
        results.append({
            "name": row["name"],
            "ra": row["ra"],
            "dec": row["dec"],
            "z_lens": row["z_lens"],
            "bh_count": count
        })
    return pd.DataFrame(results)

# ▶️ Main loop
num_batches = len(df_strong) // batch_size + int(len(df_strong) % batch_size > 0)
for i in range(num_batches):
    batch_path = os.path.join(save_dir, f"batch_{i+1:03}.csv")
    if os.path.exists(batch_path):
        print(f"⏩ Skipping existing batch {i+1}")
        continue
    df_batch = df_strong.iloc[i*batch_size:(i+1)*batch_size]
    df_result = process_batch(df_batch, i)
    df_result.to_csv(batch_path, index=False)
    print(f"✅ Saved batch {i+1} to {batch_path}\n")

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Imports
import pandas as pd
import numpy as np
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
from astropy import units as u
from lenscat import catalog
import os

# Load full lens catalog from lenscat
print("📦 Loading full catalog from lenscat package...")
df_all = catalog().data  # Correct way to access the table
print(f"✅ Total lenses loaded: {len(df_all)}\n")

# Check available column names
print("📋 Column names:", df_all.colnames)

# Convert Astropy Table to Pandas
df_all = df_all.to_pandas()

# Print grading counts
print("\nGrading counts:")
if "grading" in df_all.columns:
    print(df_all["grading"].value_counts(), "\n")
else:
    raise ValueError("❌ 'grading' column not found in catalog.")

# Rename and check redshift column
if "z" in df_all.columns:
    df_all = df_all.rename(columns={"z": "z_lens"})
elif "z_lens" not in df_all.columns:
    raise ValueError("❌ Redshift column 'z' or 'z_lens' not found.")

# Rename RA/DEC for consistency
df_all = df_all.rename(columns={"ra": "RA", "dec": "DEC"})

# Filter for confident or probable lenses with valid redshift
df = df_all[
    df_all["grading"].isin(["confident", "probable"]) &
    df_all["z_lens"].notna()
].copy()

print(f"✅ Lenses with redshift and valid grading: {len(df)}\n")

# Show a preview
df[['RA', 'DEC', 'z_lens', 'grading']].head()

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Imports
import pandas as pd
import numpy as np
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
from astropy import units as u
from lenscat import catalog
import os

# Load full lens catalog from lenscat (correct usage — no parentheses)
print("📦 Loading full catalog from lenscat package...")
df_all = catalog.data
print(f"✅ Total lenses loaded: {len(df_all)}\n")

# Show column names
print("📋 Column names:", df_all.colnames)

# Convert to pandas
df_all = df_all.to_pandas()

# Check for and rename grading and redshift columns
if "grading" not in df_all.columns:
    raise ValueError("❌ 'grading' column missing.")
if "z" in df_all.columns:
    df_all = df_all.rename(columns={"z": "z_lens"})
elif "z_lens" not in df_all.columns:
    raise ValueError("❌ 'z_lens' column missing.")

# Standardize coordinate names
df_all = df_all.rename(columns={"ra": "RA", "dec": "DEC"})

# Show grading summary
print("\nGrading counts:")
print(df_all["grading"].value_counts(), "\n")

# Filter lenses with confident or probable grading and valid redshift
df = df_all[
    df_all["grading"].isin(["confident", "probable"]) &
    df_all["z_lens"].notna()
].copy()

print(f"✅ Lenses with redshift and valid grading: {len(df)}\n")

# Preview result
df[['RA', 'DEC', 'z_lens', 'grading']].head()

# ✅ Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# ✅ Imports
import pandas as pd
import numpy as np
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
from astropy import units as u
from lenscat import catalog
import os

# ✅ Load full lens catalog
print("📦 Loading full catalog from lenscat package...")
df_all = catalog.to_pandas()
print(f"✅ Total lenses loaded: {len(df_all)}\n")

# ✅ Inspect available columns
print("📋 Column names:", df_all.columns.tolist())

# ✅ Rename columns if needed
if "z" in df_all.columns:
    df_all = df_all.rename(columns={"z": "z_lens"})

if "ra" in df_all.columns:
    df_all = df_all.rename(columns={"ra": "RA"})
if "dec" in df_all.columns:
    df_all = df_all.rename(columns={"dec": "DEC"})

# ✅ Check for grading and z_lens
required = ["RA", "DEC", "grading", "z_lens"]
missing = [col for col in required if col not in df_all.columns]
if missing:
    raise ValueError(f"Missing required column(s): {missing}")

# ✅ Filter lenses with confident or probable grading and valid redshift
df = df_all[
    df_all["grading"].isin(["confident", "probable"]) &
    df_all["z_lens"].notna()
].copy()

# ✅ Summary
print(f"✅ Filtered lenses with redshift + valid grading: {len(df)}")
print(df[["RA", "DEC", "z_lens", "grading"]].head())

# Install lenscat if not already installed (uncomment if needed)
# !pip install -q lenscat astroquery

import numpy as np
import pandas as pd
import astropy.units as u
from astropy.coordinates import SkyCoord, Angle
from astropy.table import vstack
from astroquery.sdss import SDSS
import lenscat

# --- Load full lens catalog ---
print("📦 Loading full catalog from lenscat package...")
catalog = lenscat.catalog
df_all = catalog.to_pandas()
print(f"✅ Total lenses loaded: {len(df_all)}\n")
print("📋 Columns:", df_all.columns.tolist())

# --- Rename zlens to z_lens ---
df_all = df_all.rename(columns={"zlens": "z_lens"})

# --- Check required columns ---
required = ["RA", "DEC", "grading", "z_lens"]
missing = [col for col in required if col not in df_all.columns]
if missing:
    raise ValueError(f"Missing required column(s): {missing}")

# --- Filter confident/probable lenses with valid redshift ---
df_filtered = df_all[
    df_all["grading"].isin(["confident", "probable"]) &
    df_all["z_lens"].notna()
].reset_index(drop=True)
print(f"✅ Filtered lenses (confident/probable with redshift): {len(df_filtered)}\n")

# --- SDSS tiled query function ---
def query_sdss_tiled(center_coord, total_radius_deg=0.05, tile_radius_arcmin=3.0):
    """
    Query SDSS galaxies tiled in the area around center_coord.
    Returns combined Astropy Table or None.
    """
    tile_radius_deg = tile_radius_arcmin / 60.0
    n_tiles_side = int(np.ceil((2 * total_radius_deg) / tile_radius_deg))
    all_results = []

    ra_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)
    dec_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)

    for ra_off in ra_offsets:
        for dec_off in dec_offsets:
            tile_center = SkyCoord(ra=center_coord.ra.deg + ra_off,
                                   dec=center_coord.dec.deg + dec_off,
                                   unit='deg')
            try:
                result = SDSS.query_region(tile_center,
                                          radius=Angle(tile_radius_arcmin, u.arcmin),
                                          spectro=False,
                                          photoobj_fields=['ra', 'dec', 'type'])
                if result is not None:
                    all_results.append(result)
            except Exception as e:
                print(f"SDSS query error at RA={tile_center.ra.deg:.4f}, DEC={tile_center.dec.deg:.4f}: {e}")

    if all_results:
        combined = vstack(all_results)
        coords_all = SkyCoord(ra=combined['ra'], dec=combined['dec'], unit='deg')
        mask = coords_all.separation(center_coord) <= Angle(total_radius_deg, u.deg)
        filtered = combined[mask]
        return filtered
    else:
        return None

# --- Map SDSS 'type' to typical stellar mass (M_sun) ---
def sdss_type_to_mass(sdss_type):
    # SDSS PhotoObj.type: 3 = star, 6 = galaxy, etc.
    if sdss_type == 6:
        return 5e10  # Approximate typical galaxy stellar mass
    else:
        return 0.0  # Ignore stars and others

# --- Calculate surface mass density (M_sun / Mpc^2) ---
def surface_mass_density(total_mass, redshift, radius_arcmin=3):
    if redshift is None or np.isnan(redshift) or total_mass == 0:
        return 0.0
    from astropy.cosmology import Planck18 as cosmo
    theta_rad = radius_arcmin * (np.pi / 180) / 60
    d_a = cosmo.angular_diameter_distance(redshift).to(u.Mpc).value
    radius_mpc = theta_rad * d_a
    area = np.pi * radius_mpc**2
    return total_mass / area

# --- Estimate mass around one lens ---
def estimate_mass_for_lens(lens_row, radius_arcmin=3):
    center_coord = SkyCoord(ra=lens_row['RA'], dec=lens_row['DEC'], unit='deg')
    galaxies = query_sdss_tiled(center_coord, total_radius_deg=radius_arcmin/60, tile_radius_arcmin=3)
    if galaxies is None or len(galaxies) == 0:
        print(f"  No galaxies found near lens {lens_row['name']}")
        return 0.0, 0.0

    total_mass = 0.0
    for gal in galaxies:
        gal_type = gal['type']
        mass = sdss_type_to_mass(gal_type)
        total_mass += mass

    sigma = surface_mass_density(total_mass, lens_row['z_lens'], radius_arcmin)
    print(f"  Found {len(galaxies)} galaxies, total stellar mass approx: {total_mass:.2e} M☉")
    return total_mass, sigma

# --- Main processing loop ---
N = 100  # Number of lenses to process (adjust as needed)
results = []

print(f"Processing {N} lenses for stellar mass estimation...\n")
for i, lens in df_filtered.iloc[:N].iterrows():
    print(f"Querying lens {lens['name']} at RA={lens['RA']:.4f}, DEC={lens['DEC']:.4f}")
    mass, sigma = estimate_mass_for_lens(lens, radius_arcmin=3)
    results.append({
        "lens_id": lens['name'],
        "total_mass_Msun": mass,
        "mass_surface_density_Msun_per_Mpc2": sigma
    })

# --- Save results ---
results_df = pd.DataFrame(results)
results_df.to_csv("lens_stellar_mass_lenscat.csv", index=False)
print("\n✅ Mass estimation done and saved to 'lens_stellar_mass_lenscat.csv'")

# Install lenscat if needed (uncomment if running fresh)
# !pip install -q lenscat

import numpy as np
import pandas as pd
from astropy.coordinates import SkyCoord, Angle
import astropy.units as u
from astropy.table import vstack
from astroquery.sdss import SDSS

# Load lens catalog from lenscat package
import lenscat

print("📦 Loading full catalog from lenscat package...")
catalog = lenscat.catalog  # Catalog object
df_all = catalog.to_pandas()
print(f"✅ Total lenses loaded: {len(df_all)}\n")

print("📋 Columns:", list(df_all.columns))

# Filter confident/probable lenses with valid redshift
df_filtered = df_all[
    df_all["grading"].isin(["confident", "probable"]) &
    df_all["zlens"].notna()
].copy()

# Convert zlens to numeric and drop any non-convertible rows
df_filtered["z_lens"] = pd.to_numeric(df_filtered["zlens"], errors='coerce')
df_filtered = df_filtered.dropna(subset=["z_lens"]).reset_index(drop=True)
print(f"✅ Filtered lenses (confident/probable with numeric redshift): {len(df_filtered)}\n")

# Function: Query SDSS galaxies around lens with tiled queries
def query_sdss_tiled(center_coord, total_radius_deg=0.05, tile_radius_arcmin=3.0):
    tile_radius_deg = tile_radius_arcmin / 60.0
    n_tiles_side = int(np.ceil((2 * total_radius_deg) / tile_radius_deg))
    all_results = []

    ra_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)
    dec_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)

    for ra_off in ra_offsets:
        for dec_off in dec_offsets:
            tile_center = SkyCoord(ra=center_coord.ra.deg + ra_off,
                                   dec=center_coord.dec.deg + dec_off,
                                   unit='deg')
            try:
                result = SDSS.query_region(tile_center,
                                          radius=Angle(tile_radius_arcmin, u.arcmin),
                                          spectro=False,
                                          photoobj_fields=['ra', 'dec', 'type'])
                if result is not None:
                    all_results.append(result)
            except Exception as e:
                print(f"Error querying tile at RA={tile_center.ra.deg}, DEC={tile_center.dec.deg}: {e}")

    if all_results:
        combined = vstack(all_results)
        coords_all = SkyCoord(ra=combined['ra'], dec=combined['dec'], unit='deg')
        mask = coords_all.separation(center_coord) <= Angle(total_radius_deg, u.deg)
        filtered = combined[mask]
        return filtered
    else:
        return None

# Map SDSS 'type' integer to typical stellar mass in solar masses
def sdss_type_to_mass(sdss_type):
    # SDSS PhotoObj.type values:
    # 3 = star, 6 = galaxy, others ignored
    if sdss_type == 6:
        return 5e10  # Typical galaxy stellar mass estimate
    else:
        return 0  # Ignore stars or unknown types

# Calculate stellar mass surface density in M_sun / Mpc^2
def surface_mass_density(total_mass, redshift, radius_arcmin=3):
    if redshift is None or np.isnan(redshift) or total_mass == 0:
        return 0.0
    from astropy.cosmology import Planck18 as cosmo
    theta_rad = radius_arcmin * (np.pi / 180) / 60  # arcmin to radians
    d_a = cosmo.angular_diameter_distance(redshift).to(u.Mpc).value
    radius_mpc = theta_rad * d_a
    area = np.pi * radius_mpc**2
    return total_mass / area

# Estimate total stellar mass and surface density for one lens
def estimate_mass_for_lens(lens, radius_arcmin=3):
    center_coord = SkyCoord(ra=lens['RA'], dec=lens['DEC'], unit='deg')
    galaxies = query_sdss_tiled(center_coord, total_radius_deg=radius_arcmin/60, tile_radius_arcmin=3)
    if galaxies is None or len(galaxies) == 0:
        print(f"  No galaxies found for lens {lens['name']}")
        return 0.0, 0.0

    total_mass = 0.0
    for gal in galaxies:
        gal_type = gal['type']
        mass = sdss_type_to_mass(gal_type)
        total_mass += mass

    sigma = surface_mass_density(total_mass, lens['z_lens'], radius_arcmin)
    print(f"  Found {len(galaxies)} galaxies, total stellar mass approx: {total_mass:.2e} M☉")
    return total_mass, sigma

# Run for first N lenses
N = 100
print(f"Processing {N} lenses for stellar mass estimation...\n")

results = []
for i, lens in df_filtered.iloc[:N].iterrows():
    print(f"Querying lens {lens['name']} at RA={lens['RA']:.4f}, DEC={lens['DEC']:.4f}")
    mass, sigma = estimate_mass_for_lens(lens, radius_arcmin=3)
    results.append({
        "lens_id": lens['name'],
        "total_mass_Msun": mass,
        "mass_surface_density_Msun_per_Mpc2": sigma
    })

results_df = pd.DataFrame(results)
results_df.to_csv('lens_stellar_mass_lenscat_100.csv', index=False)
print("\nDone. Results saved to 'lens_stellar_mass_lenscat_100.csv'")

# Install lenscat if needed (uncomment if running first time)
# !pip install -q lenscat astroquery astropy pandas numpy matplotlib

import lenscat
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from astropy.coordinates import SkyCoord, Angle
import astropy.units as u
from astropy.table import vstack
from astroquery.sdss import SDSS

# Load full catalog from lenscat package
print("📦 Loading full catalog from lenscat package...")
catalog = lenscat.catalog
df_all = catalog.to_pandas()
print(f"✅ Total lenses loaded: {len(df_all)}")

print("\n📋 Columns:", list(df_all.columns))

# Filter confident/probable lenses with valid numeric redshift 'zlens'
df_filtered = df_all[
    (df_all['grading'].isin(['confident', 'probable'])) &
    (pd.to_numeric(df_all['zlens'], errors='coerce').notnull())
].copy()

df_filtered['z_lens'] = pd.to_numeric(df_filtered['zlens'])
print(f"✅ Filtered lenses (confident/probable with numeric redshift): {len(df_filtered)}")

# Function to query SDSS in tiled fashion around a coordinate
def query_sdss_tiled(center_coord, total_radius_deg=0.05, tile_radius_arcmin=3.0):
    tile_radius_deg = tile_radius_arcmin / 60.0
    n_tiles_side = int(np.ceil((2 * total_radius_deg) / tile_radius_deg))
    all_results = []

    ra_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)
    dec_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)

    for ra_off in ra_offsets:
        for dec_off in dec_offsets:
            tile_center = SkyCoord(ra=center_coord.ra.deg + ra_off,
                                   dec=center_coord.dec.deg + dec_off,
                                   unit='deg')
            try:
                result = SDSS.query_region(tile_center,
                                          radius=Angle(tile_radius_arcmin, u.arcmin),
                                          spectro=False,
                                          photoobj_fields=['RA', 'DEC', 'type'])  # uppercase columns
                if result is not None:
                    all_results.append(result)
            except Exception as e:
                print(f"Error querying tile at RA={tile_center.ra.deg}, DEC={tile_center.dec.deg}: {e}")

    if all_results:
        combined = vstack(all_results)
        coords_all = SkyCoord(ra=combined['RA'], dec=combined['DEC'], unit='deg')
        mask = coords_all.separation(center_coord) <= Angle(total_radius_deg, u.deg)
        filtered = combined[mask]
        return filtered
    else:
        return None

# Map SDSS 'type' to typical stellar mass (solar masses)
def sdss_type_to_mass(sdss_type):
    # 6 = galaxy, others ignored
    if sdss_type == 6:
        return 5e10  # rough average galaxy stellar mass
    else:
        return 0

# Calculate surface mass density (M_sun / Mpc^2)
def surface_mass_density(total_mass, redshift, radius_arcmin=3):
    if redshift is None or pd.isna(redshift) or total_mass == 0:
        return 0.0
    from astropy.cosmology import Planck18 as cosmo
    theta_rad = radius_arcmin * (np.pi / 180) / 60
    d_a = cosmo.angular_diameter_distance(redshift).value  # in Mpc
    radius_mpc = theta_rad * d_a
    area = np.pi * radius_mpc**2
    return total_mass / area

# Estimate total stellar mass and surface density for a single lens
def estimate_mass_for_lens(lens, radius_arcmin=3):
    center_coord = SkyCoord(ra=lens['RA'], dec=lens['DEC'], unit='deg')
    galaxies = query_sdss_tiled(center_coord, total_radius_deg=radius_arcmin/60, tile_radius_arcmin=3)
    if galaxies is None or len(galaxies) == 0:
        print(f"  No galaxies found for lens {lens['name']}")
        return 0.0, 0.0

    total_mass = 0.0
    for gal in galaxies:
        mass = sdss_type_to_mass(gal['type'])
        total_mass += mass

    sigma = surface_mass_density(total_mass, lens['z_lens'], radius_arcmin)
    print(f"  Found {len(galaxies)} galaxies, total stellar mass approx: {total_mass:.2e} M☉")
    return total_mass, sigma

# Run mass estimation on first N lenses
N = 100
print(f"\nProcessing {N} lenses for stellar mass estimation...\n")

results = []
for i, lens in df_filtered.iloc[:N].iterrows():
    print(f"Querying lens {lens['name']} at RA={lens['RA']:.4f}, DEC={lens['DEC']:.4f}")
    mass, sigma = estimate_mass_for_lens(lens, radius_arcmin=3)
    results.append({
        "lens_id": lens['name'],
        "ra": lens['RA'],
        "dec": lens['DEC'],
        "redshift": lens['z_lens'],
        "total_mass_Msun": mass,
        "mass_surface_density_Msun_per_Mpc2": sigma
    })

results_df = pd.DataFrame(results)
results_df.to_csv("lens_stellar_mass_lenscat.csv", index=False)
print("\nDone. Results saved to 'lens_stellar_mass_lenscat.csv'.")

# Optional: plot redshift histogram of processed lenses
plt.hist(df_filtered.iloc[:N]['z_lens'], bins=15, alpha=0.7)
plt.xlabel('Redshift (z)')
plt.ylabel('Number of lenses')
plt.title(f'Redshift Distribution of First {N} Lenses Processed')
plt.show()

# ====== Full corrected script for lens catalog + SDSS stellar mass query ======

# ✅ STEP 0: Install lenscat if needed
# !pip install -q lenscat astroquery

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from astropy.table import Table
from astropy.coordinates import SkyCoord
from astropy import units as u
from astroquery.sdss import SDSS

import lenscat

# --- Load full lens catalog from lenscat package ---
print("📦 Loading full catalog from lenscat package...")
catalog = lenscat.catalog  # Catalog object (Astropy Table)
df_all = catalog.to_pandas()

print(f"✅ Total lenses loaded: {len(df_all):,}\n")
print(f"📋 Columns: {list(df_all.columns)}")

# --- Force uppercase column names for consistency ---
df_all.columns = [col.upper() for col in df_all.columns]

# --- Filter lenses graded confident/probable with valid numeric redshift ---
df_filtered = df_all[df_all['GRADING'].isin(['confident', 'probable'])].copy()
df_filtered['ZLENS'] = pd.to_numeric(df_filtered['ZLENS'], errors='coerce')
df_filtered = df_filtered.dropna(subset=['ZLENS']).reset_index(drop=True)

print(f"✅ Filtered lenses (confident/probable with numeric redshift): {len(df_filtered):,}")

# --- Define SDSS query function ---
def query_sdss_tiled(center_coord, total_radius_deg=0.1, tile_radius_arcmin=3):
    """
    Query SDSS around center_coord within total_radius_deg, tiled in tiles of tile_radius_arcmin radius.
    Returns concatenated Astropy Table of all galaxies found.
    """
    from astropy.coordinates import Angle

    tile_radius_deg = tile_radius_arcmin / 60
    ra0 = center_coord.ra.deg
    dec0 = center_coord.dec.deg
    radius = total_radius_deg / 2

    # Calculate tiles grid covering the total_radius_deg square region
    ra_tiles = np.arange(ra0 - radius, ra0 + radius + tile_radius_deg, tile_radius_deg)
    dec_tiles = np.arange(dec0 - radius, dec0 + radius + tile_radius_deg, tile_radius_deg)

    all_results = []
    for ra_c in ra_tiles:
        for dec_c in dec_tiles:
            coord = SkyCoord(ra=ra_c, dec=dec_c, unit='deg')
            try:
                xid = SDSS.query_region(coord, radius=Angle(tile_radius_arcmin, "arcmin"), spectro=False)
                if xid is not None and len(xid) > 0:
                    all_results.append(xid)
            except Exception as e:
                print(f"    SDSS query error at RA={ra_c:.3f}, DEC={dec_c:.3f}: {e}")

    if all_results:
        combined = Table.vstack(all_results)
        return combined
    else:
        return None

# --- Simple mass assignment based on galaxy type ---
def sdss_type_to_mass(gal_type):
    # Basic mapping for demonstration; refine as needed
    gal_type = gal_type.lower() if gal_type else ''
    if 'elliptical' in gal_type or 'early' in gal_type:
        return 5e11  # Solar masses
    elif 'spiral' in gal_type or 'late' in gal_type:
        return 2e11
    else:
        return 1e11  # Default mass

# --- Surface mass density calculation ---
def surface_mass_density(total_mass, redshift, radius_arcmin=3):
    if redshift is None or pd.isna(redshift) or total_mass == 0:
        return 0.0
    from astropy.cosmology import Planck18 as cosmo
    import numpy as np

    radius_rad = np.deg2rad(radius_arcmin / 60)
    d_a = cosmo.angular_diameter_distance(redshift).to(u.Mpc).value
    area_mpc2 = np.pi * (d_a * radius_rad)**2  # Projected area in Mpc^2
    sigma = total_mass / area_mpc2  # Msun/Mpc^2
    return sigma

# --- Estimate stellar mass for a single lens ---
def estimate_mass_for_lens(lens, radius_arcmin=3):
    ra = lens.get('RA', None)
    dec = lens.get('DEC', None)
    z = lens.get('ZLENS', None)

    if ra is None or dec is None or pd.isna(ra) or pd.isna(dec):
        print(f"  Invalid RA/DEC for lens {lens.get('NAME', 'unknown')}")
        return 0.0, 0.0

    center_coord = SkyCoord(ra=ra, dec=dec, unit='deg')
    galaxies = query_sdss_tiled(center_coord, total_radius_deg=radius_arcmin/30, tile_radius_arcmin=3)  # total radius ~ radius_arcmin / 30 (approx)

    if galaxies is None or len(galaxies) == 0:
        print(f"  No galaxies found for lens {lens.get('NAME', 'unknown')}")
        return 0.0, 0.0

    total_mass = 0.0
    # Use SDSS 'type' or 'class' column; fallbacks may be needed
    for gal in galaxies:
        gal_type = gal.get('type', '') if 'type' in gal.colnames else gal.get('class', '')
        total_mass += sdss_type_to_mass(gal_type)

    sigma = surface_mass_density(total_mass, z, radius_arcmin)
    print(f"  Found {len(galaxies)} galaxies, total stellar mass approx: {total_mass:.2e} M☉")
    return total_mass, sigma

# --- Main processing loop ---
N = 100  # Number of lenses to process; change as needed

results = []
print(f"\nProcessing {N} lenses for stellar mass estimation...\n")
for i, lens in df_filtered.iloc[:N].iterrows():
    print(f"Querying lens {lens['NAME']} at RA={lens['RA']:.4f}, DEC={lens['DEC']:.4f}")
    mass, sigma = estimate_mass_for_lens(lens, radius_arcmin=3)
    results.append({
        "lens_id": lens['NAME'],
        "ra": lens['RA'],
        "dec": lens['DEC'],
        "redshift": lens['ZLENS'],
        "total_mass_Msun": mass,
        "mass_surface_density_Msun_per_Mpc2": sigma
    })

# --- Save results to CSV ---
import os
output_path = "lens_stellar_mass_lenscat.csv"
pd.DataFrame(results).to_csv(output_path, index=False)
print(f"\nDone. Results saved to '{output_path}'")

# --- Optional: quick plot of redshift distribution ---
import matplotlib.pyplot as plt

masses = pd.DataFrame(results)['total_mass_Msun']
zs = pd.DataFrame(results)['redshift']

plt.hist(zs, bins=15, alpha=0.7, label='Lens redshift')
plt.xlabel('Redshift (z)')
plt.ylabel('Number of lenses')
plt.title('Redshift distribution of processed lenses')
plt.legend()
plt.show()

# Install lenscat if needed
# !pip install -q lenscat astroquery astropy pandas matplotlib

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from astropy.table import Table, vstack
from astropy.coordinates import SkyCoord, Angle
import astropy.units as u
from astroquery.sdss import SDSS
from astropy.cosmology import Planck18 as cosmo

import lenscat

def query_sdss_tiled(center_coord, total_radius_deg=0.05, tile_radius_arcmin=3.0):
    """
    Query SDSS around center_coord within total_radius_deg using tiled regions of tile_radius_arcmin.
    Returns an astropy Table of combined results or None if no data.
    """
    tile_radius_deg = tile_radius_arcmin / 60.0
    n_tiles_side = int(np.ceil((2 * total_radius_deg) / tile_radius_deg))
    all_results = []

    ra_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)
    dec_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)

    for ra_off in ra_offsets:
        for dec_off in dec_offsets:
            tile_center = SkyCoord(ra=center_coord.ra.deg + ra_off,
                                   dec=center_coord.dec.deg + dec_off,
                                   unit='deg')
            try:
                result = SDSS.query_region(tile_center,
                                          radius=Angle(tile_radius_arcmin, u.arcmin),
                                          spectro=False,
                                          photoobj_fields=['ra', 'dec', 'type'])
                if result is not None:
                    all_results.append(result)
            except Exception as e:
                print(f"Error querying tile at RA={tile_center.ra.deg:.4f}, DEC={tile_center.dec.deg:.4f}: {e}")

    if all_results:
        combined = vstack(all_results)
        # Filter within total radius (circular)
        mask = SkyCoord(ra=combined['ra'], dec=combined['dec'], unit='deg').separation(center_coord) <= Angle(total_radius_deg, u.deg)
        return combined[mask]
    else:
        return None

def sdss_type_to_mass(sdss_type):
    # SDSS PhotoObj.type values: 3=star, 6=galaxy
    if sdss_type == 6:
        return 5e10  # Typical galaxy stellar mass in solar masses
    else:
        return 0

def surface_mass_density(total_mass, redshift, radius_arcmin=3):
    if redshift is None or np.isnan(redshift) or total_mass == 0:
        return 0.0
    theta_rad = radius_arcmin * (np.pi / 180) / 60
    d_a = cosmo.angular_diameter_distance(redshift).to(u.Mpc).value
    radius_mpc = theta_rad * d_a
    area = np.pi * radius_mpc**2
    return total_mass / area

def estimate_mass_for_lens(lens, radius_arcmin=3):
    center_coord = SkyCoord(ra=lens['RA'], dec=lens['DEC'], unit='deg')
    galaxies = query_sdss_tiled(center_coord, total_radius_deg=radius_arcmin/60, tile_radius_arcmin=3)
    if galaxies is None or len(galaxies) == 0:
        print(f"  No galaxies found for lens {lens['name']}")
        return 0.0, 0.0

    total_mass = 0.0
    for gal in galaxies:
        total_mass += sdss_type_to_mass(gal['type'])

    sigma = surface_mass_density(total_mass, lens['zlens'], radius_arcmin)
    print(f"  Found {len(galaxies)} galaxies, total stellar mass approx: {total_mass:.2e} M☉")
    return total_mass, sigma

def main():
    # Load catalog and convert to pandas
    print("📦 Loading full catalog from lenscat package...")
    catalog = lenscat.catalog
    df_all = catalog.to_pandas()

    print(f"✅ Total lenses loaded: {len(df_all):,}\n")
    print("📋 Columns:", list(df_all.columns))

    # Filter lenses with confident/probable grading and numeric redshift
    df_filtered = df_all[df_all['grading'].isin(['confident', 'probable'])].copy()
    df_filtered['zlens'] = pd.to_numeric(df_filtered['zlens'], errors='coerce')
    df_filtered = df_filtered.dropna(subset=['zlens']).reset_index(drop=True)

    print(f"✅ Filtered lenses (confident/probable with numeric redshift): {len(df_filtered):,}\n")

    N = 500  # Number of lenses to process
    print(f"Processing {N} lenses for stellar mass estimation...\n")

    results = []
    for i, lens in df_filtered.iloc[:N].iterrows():
        print(f"Querying lens {lens['name']} at RA={lens['RA']:.4f}, DEC={lens['DEC']:.4f}")
        mass, sigma = estimate_mass_for_lens(lens, radius_arcmin=3)
        results.append({
            "lens_id": lens['name'],
            "ra": lens['RA'],
            "dec": lens['DEC'],
            "redshift": lens['zlens'],
            "total_stellar_mass_Msun": mass,
            "surface_mass_density_Msun_per_Mpc2": sigma
        })

    df_results = pd.DataFrame(results)
    df_results.to_csv('lens_stellar_mass_lenscat.csv', index=False)
    print("\nDone. Results saved to 'lens_stellar_mass_lenscat.csv'")

if __name__ == "__main__":
    main()

print(result.colnames)

from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u
import pandas as pd
import numpy as np

def query_simbad_for_galaxies(center_coord, radius_arcmin=3):
    """
    Query SIMBAD around center_coord within radius_arcmin to get objects and
    sum stellar mass estimates for galaxies only.
    """
    custom_simbad = Simbad()
    custom_simbad.add_votable_fields('otype')

    radius = u.Quantity(radius_arcmin, u.arcmin)
    try:
        result = custom_simbad.query_region(center_coord, radius=radius)
        if result is None or len(result) == 0:
            return 0.0, 0

        total_mass = 0
        count_galaxies = 0
        for otype in result['OTYPE']:
            if otype is None:
                continue
            otype_str = str(otype).lower()
            if 'galaxy' in otype_str:
                total_mass += 5e10  # typical stellar mass per galaxy
                count_galaxies += 1
        return total_mass, count_galaxies
    except Exception as e:
        print(f"SIMBAD query failed at {center_coord.to_string('hmsdms')}: {e}")
        return 0.0, 0

def main():
    # Example center coordinate (replace with your lens coordinate)
    center = SkyCoord(ra=150.0, dec=2.0, unit='deg')
    mass, count = query_simbad_for_galaxies(center, radius_arcmin=3)
    print(f"Found {count} galaxies with total stellar mass ~ {mass:.2e} M☉")

if __name__ == "__main__":
    main()

from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u

def query_simbad_for_galaxies(center_coord, radius_arcmin=3):
    custom_simbad = Simbad()
    # Add the 'otype' votable field explicitly (lowercase in code, but might be uppercase in result)
    custom_simbad.add_votable_fields('otype')

    radius = u.Quantity(radius_arcmin, u.arcmin)
    try:
        result = custom_simbad.query_region(center_coord, radius=radius)
        if result is None or len(result) == 0:
            print("No results found in SIMBAD.")
            return 0.0, 0

        print("Returned SIMBAD columns:", result.colnames)

        # Try uppercase 'OTYPE' (most common)
        if 'OTYPE' in result.colnames:
            otypes = result['OTYPE']
        # Fallback to lowercase 'otype' if present
        elif 'otype' in result.colnames:
            otypes = result['otype']
        else:
            print("No 'otype' column found in SIMBAD result.")
            return 0.0, 0

        total_mass = 0
        count_galaxies = 0
        for otype in otypes:
            if otype is None:
                continue
            otype_str = str(otype).lower()
            if 'galaxy' in otype_str:
                total_mass += 5e10  # typical stellar mass per galaxy
                count_galaxies += 1

        return total_mass, count_galaxies
    except Exception as e:
        print(f"SIMBAD query failed at {center_coord.to_string('hmsdms')}: {e}")
        return 0.0, 0

def main():
    center = SkyCoord(ra=150.0, dec=2.0, unit='deg')
    mass, count = query_simbad_for_galaxies(center, radius_arcmin=3)
    print(f"Found {count} galaxies with total stellar mass ~ {mass:.2e} M☉")

if __name__ == "__main__":
    main()

from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u

def query_simbad_for_galaxies(center_coord, radius_arcmin=3):
    custom_simbad = Simbad()
    # Add the 'otype' votable field explicitly (lowercase in code, but might be uppercase in result)
    custom_simbad.add_votable_fields('otype')

    radius = u.Quantity(radius_arcmin, u.arcmin)
    try:
        result = custom_simbad.query_region(center_coord, radius=radius)
        if result is None or len(result) == 0:
            print("No results found in SIMBAD.")
            return 0.0, 0

        print("Returned SIMBAD columns:", result.colnames)

        # Try uppercase 'OTYPE' (most common)
        if 'OTYPE' in result.colnames:
            otypes = result['OTYPE']
        # Fallback to lowercase 'otype' if present
        elif 'otype' in result.colnames:
            otypes = result['otype']
        else:
            print("No 'otype' column found in SIMBAD result.")
            return 0.0, 0

        total_mass = 0
        count_galaxies = 0
        for otype in otypes:
            if otype is None:
                continue
            otype_str = str(otype).lower()
            if 'galaxy' in otype_str:
                total_mass += 5e10  # typical stellar mass per galaxy
                count_galaxies += 1

        return total_mass, count_galaxies
    except Exception as e:
        print(f"SIMBAD query failed at {center_coord.to_string('hmsdms')}: {e}")
        return 0.0, 0

def main():
    center = SkyCoord(ra=150.0, dec=2.0, unit='deg')
    mass, count = query_simbad_for_galaxies(center, radius_arcmin=3)
    print(f"Found {count} galaxies with total stellar mass ~ {mass:.2e} M☉")

if __name__ == "__main__":
    main()

from collections import Counter

def query_simbad_for_galaxies_debug(center_coord, radius_arcmin=3):
    custom_simbad = Simbad()
    custom_simbad.add_votable_fields('otype')
    radius = u.Quantity(radius_arcmin, u.arcmin)

    try:
        result = custom_simbad.query_region(center_coord, radius=radius)
        if result is None or len(result) == 0:
            print("No results found in SIMBAD.")
            return 0.0, 0

        print("Returned SIMBAD columns:", result.colnames)

        otypes = result['otype']
        # Count how many of each otype present
        counts = Counter([str(otype).strip().lower() for otype in otypes if otype is not None])
        print("Object types found and counts:", counts)

        total_mass = 0
        count_galaxies = counts.get('galaxy', 0) + counts.get('galaxy in cluster', 0) + counts.get('blue galaxy', 0)
        total_mass = count_galaxies * 5e10

        return total_mass, count_galaxies

    except Exception as e:
        print(f"SIMBAD query failed at {center_coord.to_string('hmsdms')}: {e}")
        return 0.0, 0

def main():
    center = SkyCoord(ra=150.0, dec=2.0, unit='deg')
    mass, count = query_simbad_for_galaxies_debug(center, radius_arcmin=3)
    print(f"Found {count} galaxies with total stellar mass ~ {mass:.2e} M☉")

if __name__ == "__main__":
    main()

from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u
import pandas as pd
import lenscat
import numpy as np

# Define galaxy-related SIMBAD object types (all lowercase for easy checking)
GALAXY_OTYPES = {'g', 'agn', 'emg', 'grg', 'clg'}

def sdss_type_to_mass_simbad(otype_code):
    # Mass estimate per galaxy-type object
    # Typical stellar mass per galaxy in solar masses (rough estimate)
    if otype_code.lower() in GALAXY_OTYPES:
        return 5e10
    return 0

def query_simbad_for_galaxies(center_coord, radius_arcmin=3):
    custom_simbad = Simbad()
    custom_simbad.add_votable_fields('otype')
    radius = u.Quantity(radius_arcmin, u.arcmin)

    try:
        result = custom_simbad.query_region(center_coord, radius=radius)
        if result is None or len(result) == 0:
            print(f"  No results found in SIMBAD for {center_coord.to_string('hmsdms')}")
            return 0.0, 0

        otypes = [str(otype).strip().lower() for otype in result['OTYPE']]
        total_mass = 0
        galaxy_count = 0
        for otype_code in otypes:
            if otype_code in GALAXY_OTYPES:
                total_mass += 5e10
                galaxy_count += 1

        print(f"  Found {galaxy_count} galaxies, total stellar mass approx: {total_mass:.2e} M☉")
        return total_mass, galaxy_count

    except Exception as e:
        print(f"SIMBAD query failed at {center_coord.to_string('hmsdms')}: {e}")
        return 0.0, 0

def surface_mass_density(total_mass, redshift, radius_arcmin=3):
    if redshift is None or np.isnan(redshift) or total_mass == 0:
        return 0.0
    theta_rad = radius_arcmin * (np.pi / 180) / 60
    d_a = cosmo.angular_diameter_distance(redshift).to(u.Mpc).value
    radius_mpc = theta_rad * d_a
    area = np.pi * radius_mpc**2
    return total_mass / area

def estimate_mass_for_lens(lens, radius_arcmin=3):
    center_coord = SkyCoord(ra=lens['RA'], dec=lens['DEC'], unit='deg')
    total_mass, galaxy_count = query_simbad_for_galaxies(center_coord, radius_arcmin=radius_arcmin)
    sigma = surface_mass_density(total_mass, lens['zlens'], radius_arcmin)
    return total_mass, sigma, galaxy_count

def main():
    print("📦 Loading full catalog from lenscat package...")
    catalog = lenscat.catalog
    df_all = catalog.to_pandas()

    print(f"✅ Total lenses loaded: {len(df_all):,}\n")
    print("📋 Columns:", list(df_all.columns))

    # Filter lenses with confident/probable grading and numeric redshift
    df_filtered = df_all[df_all['grading'].isin(['confident', 'probable'])].copy()
    df_filtered['zlens'] = pd.to_numeric(df_filtered['zlens'], errors='coerce')
    df_filtered = df_filtered.dropna(subset=['zlens']).reset_index(drop=True)

    print(f"✅ Filtered lenses (confident/probable with numeric redshift): {len(df_filtered):,}\n")

    N = 100  # Number of lenses to process for demo; increase as needed
    print(f"Processing {N} lenses for stellar mass estimation with SIMBAD...\n")

    results = []
    for i, lens in df_filtered.iloc[:N].iterrows():
        print(f"Querying lens {lens['name']} at RA={lens['RA']:.4f}, DEC={lens['DEC']:.4f}")
        total_mass, sigma, galaxy_count = estimate_mass_for_lens(lens, radius_arcmin=3)
        results.append({
            "lens_id": lens['name'],
            "ra": lens['RA'],
            "dec": lens['DEC'],
            "redshift": lens['zlens'],
            "total_stellar_mass_Msun": total_mass,
            "surface_mass_density_Msun_per_Mpc2": sigma,
            "galaxy_count": galaxy_count
        })

    df_results = pd.DataFrame(results)
    df_results.to_csv('lens_stellar_mass_simbad.csv', index=False)
    print("\nDone. Results saved to 'lens_stellar_mass_simbad.csv'")

if __name__ == "__main__":
    main()

from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u

coord = SkyCoord(ra=63.6572, dec=5.5786, unit='deg')

custom_simbad = Simbad()
custom_simbad.remove_votable_fields('*')
custom_simbad.add_votable_fields('otype')

result = custom_simbad.query_region(coord, radius=3*u.arcmin)
print(result.colnames)
print(result['OTYPE'])

from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u

custom_simbad = Simbad()
custom_simbad.reset_votable_fields()  # reset to default
custom_simbad.add_votable_fields('otype')  # add 'otype' field

coord = SkyCoord(ra=63.6572, dec=5.5786, unit='deg')

result = custom_simbad.query_region(coord, radius=3*u.arcmin)

print(result.colnames)
print(result['otype'])

from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u
import numpy as np

# Create custom Simbad instance with only needed fields
custom_simbad = Simbad()
custom_simbad.reset_votable_fields()
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

# Define galaxy-like types (uppercase for comparison)
GALAXY_TYPES = {'G', 'QSO', 'AGN', 'EMG', 'GLS', 'GRG', 'CLG'}  # add more as needed

def query_simbad_galaxies(center_coord, radius_arcmin=3):
    """
    Query SIMBAD around center_coord within radius_arcmin.
    Return only galaxy-like objects based on 'otype'.
    """
    radius = u.Quantity(radius_arcmin, u.arcmin)
    result = custom_simbad.query_region(center_coord, radius=radius)
    if result is None:
        return None

    # Filter for galaxy-like otypes
    otypes = np.char.upper(result['OTYPE'])  # convert to uppercase for uniformity
    mask = np.array([otype in GALAXY_TYPES for otype in otypes])
    galaxies = result[mask]

    return galaxies

def estimate_stellar_mass_simbad(lens, radius_arcmin=3):
    center = SkyCoord(ra=lens['RA'], dec=lens['DEC'], unit='deg')
    galaxies = query_simbad_galaxies(center, radius_arcmin)
    if galaxies is None or len(galaxies) == 0:
        print(f"  No galaxies found for lens {lens['name']}")
        return 0.0

    # Assume average stellar mass per galaxy, e.g. 5e10 Msun (can be refined)
    avg_stellar_mass = 5e10
    total_mass = avg_stellar_mass * len(galaxies)

    print(f"  Found {len(galaxies)} galaxies, total stellar mass approx: {total_mass:.2e} M☉")
    return total_mass

# Install if needed (uncomment to install)
# !pip install -q lenscat astroquery astropy pandas tqdm

import numpy as np
import pandas as pd
from astropy.coordinates import SkyCoord, Angle
import astropy.units as u
from astroquery.simbad import Simbad
from astropy.cosmology import Planck18 as cosmo
import lenscat
from collections import Counter
from tqdm import tqdm

# Custom Simbad query setup
custom_simbad = Simbad()
custom_simbad.remove_votable_fields('all')  # remove default fields to keep only what we add
custom_simbad.add_votable_fields('otype')

# Map SIMBAD 'otype' to approximate stellar mass (Msun) for galaxy types (very rough)
def simbad_otype_to_mass(otype):
    # Lowercase for safety
    o = otype.lower()
    # Basic categories, can add more or tweak mass assumptions
    if o in ['g', 'gal', 'clg', 'grg', 'emg']:  # galaxy types & clusters
        return 5e10  # 50 billion solar masses typical stellar mass per galaxy (very rough)
    if o in ['agn', 'qso', 'blazar', 'seyfert']:
        return 1e11  # AGN host galaxies typically more massive
    return 0

def query_simbad_galaxies(center_coord, radius_arcmin=20):
    """
    Query SIMBAD around center_coord for galaxy-like objects within radius_arcmin.
    Returns astropy Table or None if no results.
    """
    radius = Angle(radius_arcmin, u.arcmin)
    try:
        result = custom_simbad.query_region(center_coord, radius=radius)
        if result is None:
            return None
        # Filter galaxy-like otypes
        mask = [otype.lower() in ['g','gal','clg','grg','emg','agn','qso','blazar','seyfert'] for otype in result['OTYPE']]
        filtered = result[mask]
        return filtered if len(filtered) > 0 else None
    except Exception as e:
        print(f"SIMBAD query failed at {center_coord.to_string('hmsdms')}: {e}")
        return None

def surface_mass_density(total_mass, redshift, radius_arcmin=20):
    if redshift is None or np.isnan(redshift) or total_mass == 0:
        return 0.0
    theta_rad = radius_arcmin * (np.pi / 180) / 60
    d_a = cosmo.angular_diameter_distance(redshift).to(u.Mpc).value
    radius_mpc = theta_rad * d_a
    area = np.pi * radius_mpc**2
    return total_mass / area

def estimate_mass_for_lens(lens, radius_arcmin=20):
    center_coord = SkyCoord(ra=lens['RA'], dec=lens['DEC'], unit='deg')
    galaxies = query_simbad_galaxies(center_coord, radius_arcmin)
    if galaxies is None or len(galaxies) == 0:
        print(f"  No galaxies found for lens {lens['name']}")
        return 0.0, 0.0

    total_mass = 0.0
    otypes = galaxies['OTYPE']
    counts = Counter([o.lower() for o in otypes])
    for otype in counts:
        total_mass += counts[otype] * simbad_otype_to_mass(otype)

    sigma = surface_mass_density(total_mass, lens['zlens'], radius_arcmin)
    print(f"  Found {len(galaxies)} galaxies (otypes count: {dict(counts)}), total stellar mass approx: {total_mass:.2e} M☉")
    return total_mass, sigma

def main(max_lenses=500, radius_arcmin=20):
    print("📦 Loading lens catalog from lenscat package...")
    catalog = lenscat.catalog
    df_all = catalog.to_pandas()

    print(f"✅ Total lenses loaded: {len(df_all):,}")
    print("📋 Columns:", list(df_all.columns))

    df_filtered = df_all[df_all['grading'].isin(['confident', 'probable'])].copy()
    df_filtered['zlens'] = pd.to_numeric(df_filtered['zlens'], errors='coerce')
    df_filtered = df_filtered.dropna(subset=['zlens']).reset_index(drop=True)

    print(f"✅ Filtered lenses (confident/probable with numeric redshift): {len(df_filtered):,}")

    N = min(max_lenses, len(df_filtered))
    print(f"Processing {N} lenses for stellar mass estimation within {radius_arcmin} arcmin radius...\n")

    results = []
    for i, lens in tqdm(df_filtered.iloc[:N].iterrows(), total=N):
        print(f"Querying lens {lens['name']} at RA={lens['RA']:.4f}, DEC={lens['DEC']:.4f}")
        mass, sigma = estimate_mass_for_lens(lens, radius_arcmin=radius_arcmin)
        results.append({
            "lens_id": lens['name'],
            "ra": lens['RA'],
            "dec": lens['DEC'],
            "redshift": lens['zlens'],
            "total_stellar_mass_Msun": mass,
            "surface_mass_density_Msun_per_Mpc2": sigma
        })

    df_results = pd.DataFrame(results)
    out_filename = f'lens_stellar_mass_lenstat_{N}_lenses_{radius_arcmin}arcmin.csv'
    df_results.to_csv(out_filename, index=False)
    print(f"\n✅ Done. Results saved to '{out_filename}'")
    return df_results

if __name__ == "__main__":
    main(max_lenses=500, radius_arcmin=20)

import lenscat
import pandas as pd
import numpy as np
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord, Angle
import astropy.units as u
from astropy.cosmology import Planck18 as cosmo

def custom_simbad_query(center_coord, radius_arcmin=20):
    # Setup Simbad with only needed fields
    custom_simbad = Simbad()
    custom_simbad.remove_votable_fields('*')  # Remove all default fields
    custom_simbad.add_votable_fields('otype')  # Add object type

    try:
        result = custom_simbad.query_region(center_coord, radius=Angle(radius_arcmin, u.arcmin))
        if result is None:
            return None
        return result
    except Exception as e:
        print(f"SIMBAD query failed at {center_coord.to_string('hmsdms')}: {e}")
        return None

def estimate_stellar_mass_from_simbad(result):
    # Object types for galaxies are case sensitive 'G' or 'g'
    gal_types = ['G', 'g']
    if result is None or len(result) == 0:
        return 0, 0

    # Count galaxies
    gal_mask = [otype in gal_types for otype in result['OTYPE']]
    n_gal = sum(gal_mask)

    # Stellar mass estimate per galaxy (typical ~5e10 solar masses)
    mass_per_galaxy = 5e10
    total_mass = n_gal * mass_per_galaxy

    return n_gal, total_mass

def surface_mass_density(total_mass, redshift, radius_arcmin=20):
    if redshift is None or np.isnan(redshift) or total_mass == 0:
        return 0.0
    theta_rad = (radius_arcmin / 60) * (np.pi / 180)
    d_a = cosmo.angular_diameter_distance(redshift).to(u.Mpc).value
    radius_mpc = theta_rad * d_a
    area = np.pi * radius_mpc ** 2
    return total_mass / area

def main():
    print("📦 Loading full lenscat catalog...")
    catalog = lenscat.catalog
    df = catalog.to_pandas()

    print(f"Total lenses loaded: {len(df):,}")

    # Filter lenses
    df_filtered = df[df['grading'].isin(['confident', 'probable'])].copy()
    df_filtered['zlens'] = pd.to_numeric(df_filtered['zlens'], errors='coerce')
    df_filtered = df_filtered.dropna(subset=['zlens']).reset_index(drop=True)

    print(f"Lenses after filtering: {len(df_filtered):,}")

    N = 100  # Number of lenses to process (reduce for testing, increase as needed)

    results = []
    for i, lens in df_filtered.iloc[:N].iterrows():
        ra = lens['RA']
        dec = lens['DEC']
        zlens = lens['zlens']
        name = lens['name']
        center_coord = SkyCoord(ra=ra, dec=dec, unit='deg')

        print(f"Querying SIMBAD around lens {name} at RA={ra:.4f} DEC={dec:.4f}")
        simbad_result = custom_simbad_query(center_coord, radius_arcmin=20)

        n_gal, total_mass = estimate_stellar_mass_from_simbad(simbad_result)
        sigma = surface_mass_density(total_mass, zlens, radius_arcmin=20)

        print(f"  Found {n_gal} galaxies, estimated total stellar mass ~ {total_mass:.2e} M☉")
        results.append({
            "lens_name": name,
            "ra": ra,
            "dec": dec,
            "redshift": zlens,
            "n_galaxies": n_gal,
            "total_stellar_mass_Msun": total_mass,
            "surface_mass_density_Msun_per_Mpc2": sigma
        })

    df_results = pd.DataFrame(results)
    df_results.to_csv('lens_stellar_mass_lenstat.csv', index=False)
    print("✅ Results saved to 'lens_stellar_mass_lenstat.csv'")

if __name__ == "__main__":
    main()

from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u
import lenscat
import pandas as pd

def custom_simbad_query(center_coord, radius_arcmin=20):
    # Setup Simbad with custom fields
    custom_simbad = Simbad()
    custom_simbad.TIMEOUT = 60  # safer
    custom_simbad.add_votable_fields('otype')  # only need object type

    # Perform query
    result = custom_simbad.query_region(center_coord, radius=radius_arcmin * u.arcmin)
    if result is None:
        return []

    # Convert result to dataframe
    df = result.to_pandas()
    return df['OTYPE'].value_counts().to_dict()

def main():
    print("📦 Loading full lenscat catalog...")
    all_lenses = lenscat.load()
    filtered = all_lenses[(all_lenses['grade'].isin(['confident', 'probable'])) & all_lenses['z'].notna()]
    print(f"Total lenses loaded: {len(all_lenses)}")
    print(f"Lenses after filtering: {len(filtered)}")

    # Example: run query on first lens
    row = filtered.iloc[0]
    ra, dec = row['ra'], row['dec']
    lens_name = row['name']
    print(f"🔍 Querying SIMBAD around lens {lens_name} at RA={ra:.4f} DEC={dec:.4f}")

    coord = SkyCoord(ra=ra * u.deg, dec=dec * u.deg)
    otype_counts = custom_simbad_query(coord, radius_arcmin=20)

    if not otype_counts:
        print("No SIMBAD objects found.")
    else:
        print("Object types found:")
        for otype, count in otype_counts.items():
            print(f"  {otype}: {count}")

if __name__ == "__main__":
    main()

# !pip install -q lenscat astroquery astropy pandas matplotlib

import numpy as np
import pandas as pd
from astropy.table import vstack
from astropy.coordinates import SkyCoord, Angle
import astropy.units as u
from astroquery.sdss import SDSS
from astropy.cosmology import Planck18 as cosmo
import lenscat

def query_sdss_tiled(center_coord, total_radius_deg=0.05, tile_radius_arcmin=3.0):
    tile_radius_deg = tile_radius_arcmin / 60.0
    n_tiles_side = int(np.ceil((2 * total_radius_deg) / tile_radius_deg))
    all_results = []

    ra_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)
    dec_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)

    for ra_off in ra_offsets:
        for dec_off in dec_offsets:
            tile_center = SkyCoord(ra=center_coord.ra.deg + ra_off,
                                   dec=center_coord.dec.deg + dec_off,
                                   unit='deg')
            try:
                result = SDSS.query_region(tile_center,
                                          radius=Angle(tile_radius_arcmin, u.arcmin),
                                          spectro=False,
                                          photoobj_fields=['ra', 'dec', 'type'])
                if result is not None:
                    # Normalize columns to lowercase immediately
                    result.rename_columns([col.lower() for col in result.colnames])
                    all_results.append(result)
            except Exception as e:
                print(f"Error querying tile at RA={tile_center.ra.deg:.4f}, DEC={tile_center.dec.deg:.4f}: {e}")

    if all_results:
        combined = vstack(all_results)
        combined.rename_columns([col.lower() for col in combined.colnames])
        mask = SkyCoord(ra=combined['ra'], dec=combined['dec'], unit='deg').separation(center_coord) <= Angle(total_radius_deg, u.deg)
        return combined[mask]
    else:
        return None

def sdss_type_to_mass(sdss_type):
    # SDSS PhotoObj.type: 3=star, 6=galaxy
    if sdss_type == 6:
        return 5e10  # stellar mass per galaxy (solar masses)
    else:
        return 0

def surface_mass_density(total_mass, redshift, radius_arcmin=3):
    if redshift is None or np.isnan(redshift) or total_mass == 0:
        return 0.0
    theta_rad = radius_arcmin * (np.pi / 180) / 60
    d_a = cosmo.angular_diameter_distance(redshift).to(u.Mpc).value
    radius_mpc = theta_rad * d_a
    area = np.pi * radius_mpc**2
    return total_mass / area

def estimate_mass_for_lens(lens, radius_arcmin=3):
    center_coord = SkyCoord(ra=lens['RA'], dec=lens['DEC'], unit='deg')
    galaxies = query_sdss_tiled(center_coord, total_radius_deg=radius_arcmin/60, tile_radius_arcmin=3)
    if galaxies is None or len(galaxies) == 0:
        print(f"  No galaxies found for lens {lens['name']}")
        return 0.0, 0.0

    total_mass = 0.0
    for gal in galaxies:
        total_mass += sdss_type_to_mass(gal['type'])

    sigma = surface_mass_density(total_mass, lens['zlens'], radius_arcmin)
    print(f"  Found {len(galaxies)} galaxies, total stellar mass approx: {total_mass:.2e} M☉")
    return total_mass, sigma

def categorize_mass(mass):
    # Define bins in solar masses
    if mass == 0:
        return 'zero'
    elif mass < 1e12:
        return 'very low'
    elif mass < 5e12:
        return 'low'
    elif mass < 1e13:
        return 'medium'
    elif mass < 3e13:
        return 'high'
    else:
        return 'very high'

def main():
    print("📦 Loading full catalog from lenscat package...")
    catalog = lenscat.catalog
    df_all = catalog.to_pandas()

    print(f"✅ Total lenses loaded: {len(df_all):,}\n")
    print("📋 Columns:", list(df_all.columns))

    # Filter lenses with confident/probable grading and numeric redshift
    df_filtered = df_all[df_all['grading'].isin(['confident', 'probable'])].copy()
    df_filtered['zlens'] = pd.to_numeric(df_filtered['zlens'], errors='coerce')
    df_filtered = df_filtered.dropna(subset=['zlens']).reset_index(drop=True)

    print(f"✅ Filtered lenses (confident/probable with numeric redshift): {len(df_filtered):,}\n")

    N = 500  # Number of lenses to process (adjustable)
    print(f"Processing {N} lenses for stellar mass estimation...\n")

    results = []
    for i, lens in df_filtered.iloc[:N].iterrows():
        print(f"Querying lens {lens['name']} at RA={lens['RA']:.4f}, DEC={lens['DEC']:.4f}")
        mass, sigma = estimate_mass_for_lens(lens, radius_arcmin=3)
        results.append({
            "lens_id": lens['name'],
            "ra": lens['RA'],
            "dec": lens['DEC'],
            "redshift": lens['zlens'],
            "total_stellar_mass_Msun": mass,
            "surface_mass_density_Msun_per_Mpc2": sigma,
            "mass_category": categorize_mass(mass)
        })

    df_results = pd.DataFrame(results)

    # Count lenses by category including zero counts
    category_counts = df_results['mass_category'].value_counts().reindex(
        ['zero', 'very low', 'low', 'medium', 'high', 'very high'], fill_value=0)

    print("\nLens counts by total stellar mass category:")
    print(category_counts)

    df_results.to_csv('lens_stellar_mass_lenscat.csv', index=False)
    print("\nDone. Results saved to 'lens_stellar_mass_lenscat.csv'")

if __name__ == "__main__":
    main()

import numpy as np
import pandas as pd
import time
from astropy.coordinates import SkyCoord, Angle
import astropy.units as u
from astroquery.sdss import SDSS
from astropy.table import vstack
import lenscat

def query_sdss_tiled(center_coord, total_radius_deg=0.3333, tile_radius_arcmin=10.0, max_retries=3):
    """
    Query SDSS around center_coord within total_radius_deg (default 20 arcmin = 0.3333 deg)
    using tiled regions of tile_radius_arcmin to avoid large queries.
    Retries up to max_retries on failure.
    Returns combined Astropy Table or None if no data.
    """
    tile_radius_deg = tile_radius_arcmin / 60.0
    n_tiles_side = int(np.ceil((2 * total_radius_deg) / tile_radius_deg))
    all_results = []

    ra_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)
    dec_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)

    for ra_off in ra_offsets:
        for dec_off in dec_offsets:
            tile_center = SkyCoord(ra=center_coord.ra.deg + ra_off,
                                   dec=center_coord.dec.deg + dec_off,
                                   unit='deg')
            for attempt in range(max_retries):
                try:
                    result = SDSS.query_region(tile_center,
                                              radius=Angle(tile_radius_arcmin, u.arcmin),
                                              spectro=False,
                                              photoobj_fields=['ra', 'dec', 'type'])
                    if result is not None and len(result) > 0:
                        all_results.append(result)
                    break  # success, break retry loop
                except Exception as e:
                    print(f"    Retry {attempt+1}/{max_retries} failed for tile at RA={tile_center.ra.deg:.4f}, DEC={tile_center.dec.deg:.4f}: {e}")
                    time.sleep(2)  # wait before retry
            else:
                print(f"    Failed all retries for tile at RA={tile_center.ra.deg:.4f}, DEC={tile_center.dec.deg:.4f}")

    if all_results:
        combined = vstack(all_results)
        mask = SkyCoord(ra=combined['ra'], dec=combined['dec'], unit='deg').separation(center_coord) <= Angle(total_radius_deg, u.deg)
        return combined[mask]
    else:
        return None

def sdss_type_to_mass(sdss_type):
    # SDSS PhotoObj.type values: 3=star, 6=galaxy
    if sdss_type == 6:
        return 5e10  # typical stellar mass in solar masses
    else:
        return 0

def estimate_mass_for_lens(lens, radius_arcmin=20):
    center_coord = SkyCoord(ra=lens['RA'], dec=lens['DEC'], unit='deg')
    galaxies = query_sdss_tiled(center_coord, total_radius_deg=radius_arcmin/60, tile_radius_arcmin=10.0)
    if galaxies is None or len(galaxies) == 0:
        # No galaxies found; record zero mass
        return 0.0, 0

    total_mass = sum(sdss_type_to_mass(gal['type']) for gal in galaxies)
    galaxy_count = len(galaxies)
    return total_mass, galaxy_count

def mass_bin(mass):
    # Define mass bins including zero
    if mass == 0:
        return 'zero'
    elif mass < 1e12:
        return 'very low'
    elif mass < 5e12:
        return 'low'
    elif mass < 1e13:
        return 'medium'
    elif mass < 5e13:
        return 'high'
    else:
        return 'very high'

def main():
    print("Loading lens catalog...")
    catalog = lenscat.catalog
    df_all = catalog.to_pandas()

    df_filtered = df_all[df_all['grading'].isin(['confident', 'probable'])].copy()
    df_filtered['zlens'] = pd.to_numeric(df_filtered['zlens'], errors='coerce')
    df_filtered = df_filtered.dropna(subset=['zlens']).reset_index(drop=True)

    print(f"Total filtered lenses: {len(df_filtered):,}")

    batch_size = 100
    total_lenses = len(df_filtered)
    results = []

    output_csv = '/content/drive/MyDrive/lens_stellar_mass_results.csv'  # Change path if needed

    for start_idx in range(0, total_lenses, batch_size):
        end_idx = min(start_idx + batch_size, total_lenses)
        batch = df_filtered.iloc[start_idx:end_idx]
        print(f"\nProcessing batch {start_idx} to {end_idx-1} ({end_idx - start_idx} lenses)...")

        for i, lens in batch.iterrows():
            try:
                print(f"Querying lens {lens['name']} at RA={lens['RA']:.4f}, DEC={lens['DEC']:.4f}")
                mass, gal_count = estimate_mass_for_lens(lens, radius_arcmin=20)
                bin_label = mass_bin(mass)
                print(f"  Galaxies found: {gal_count}, total mass: {mass:.2e} M☉, bin: {bin_label}")

                results.append({
                    'lens_name': lens['name'],
                    'ra': lens['RA'],
                    'dec': lens['DEC'],
                    'redshift': lens['zlens'],
                    'galaxy_count': gal_count,
                    'total_stellar_mass': mass,
                    'mass_bin': bin_label
                })

            except Exception as e:
                print(f"Error processing lens {lens['name']}: {e}")
                results.append({
                    'lens_name': lens['name'],
                    'ra': lens['RA'],
                    'dec': lens['DEC'],
                    'redshift': lens['zlens'],
                    'galaxy_count': 0,
                    'total_stellar_mass': 0,
                    'mass_bin': 'zero'
                })

        # Save intermediate results after each batch
        df_results = pd.DataFrame(results)
        df_results.to_csv(output_csv, index=False)
        print(f"Saved progress to {output_csv}")

    print("\nAll done! Final results saved.")

if __name__ == '__main__':
    main()

import lenscat
from astroquery.sdss import SDSS
from astropy.coordinates import SkyCoord
import astropy.units as u
import numpy as np
import pandas as pd
import time
from astropy.table import vstack

# --- Tiled SDSS query function ---
def query_large_area_tiled(ra_deg, dec_deg, search_radius_arcmin=20, tile_radius_arcmin=3, max_retries=3):
    center = SkyCoord(ra=ra_deg*u.deg, dec=dec_deg*u.deg, frame='icrs')
    search_radius_deg = search_radius_arcmin / 60
    tile_radius_deg = tile_radius_arcmin / 60

    spacing_deg = tile_radius_deg * np.sqrt(2)
    n_tiles = int(np.ceil((2 * search_radius_deg) / spacing_deg))

    ra_offsets = np.linspace(-search_radius_deg, search_radius_deg, n_tiles)
    dec_offsets = np.linspace(-search_radius_deg, search_radius_deg, n_tiles)

    results = []
    for ddec in dec_offsets:
        for dra in ra_offsets:
            tile_center = SkyCoord(ra=(ra_deg + dra) * u.deg, dec=(dec_deg + ddec) * u.deg, frame='icrs')
            if center.separation(tile_center).deg <= search_radius_deg:
                for attempt in range(1, max_retries+1):
                    try:
                        res = SDSS.query_region(tile_center, radius=tile_radius_deg * u.deg, spectro=False)
                        if res is not None:
                            results.append(res)
                        break  # success, exit retry loop
                    except Exception as e:
                        print(f"Retry {attempt}/{max_retries} failed for tile at RA={tile_center.ra.deg:.4f}, DEC={tile_center.dec.deg:.4f}: {e}")
                        time.sleep(2)  # wait before retry
                else:
                    print(f"Failed all retries for tile at RA={tile_center.ra.deg:.4f}, DEC={tile_center.dec.deg:.4f}")

    if results:
        combined = vstack(results)
        # Deduplicate based on ra/dec within small tolerance
        combined = combined.group_by(['ra', 'dec']).groups.aggregate(np.min)
        return combined
    else:
        return None


# --- Main processing ---
def main(batch_size=100, max_batches=None, save_path="/content/drive/MyDrive/lens_mass_batches/"):
    print("Loading lens catalog from lenscat package...")
    cat = lenscat.load_lens_catalog()
    # Filter confident/probable lenses with numeric zlens
    lenses = cat[(cat['grading'].isin(['confident', 'probable'])) & (~cat['zlens'].mask)]

    print(f"Total filtered lenses: {len(lenses)}")

    if max_batches is not None:
        total_to_process = min(len(lenses), batch_size * max_batches)
    else:
        total_to_process = len(lenses)

    for batch_start in range(0, total_to_process, batch_size):
        batch_end = min(batch_start + batch_size, total_to_process)
        batch = lenses[batch_start:batch_end]

        results = []
        print(f"\nProcessing batch {batch_start} to {batch_end-1} ({batch_end - batch_start} lenses)...")

        for idx, lens in enumerate(batch):
            ra = lens['RA']
            dec = lens['DEC']
            name = lens['name']

            print(f"Querying lens {name} at RA={ra:.4f}, DEC={dec:.4f}")

            gal_data = query_large_area_tiled(ra, dec, search_radius_arcmin=20, tile_radius_arcmin=3)

            if gal_data is None or len(gal_data) == 0:
                print(f"  No galaxies found for lens {name}")
                total_stellar_mass = 0.0
                galaxy_count = 0
            else:
                galaxy_count = len(gal_data)
                # Check for 'stellarMass' column; if not present, just count galaxies
                if 'stellarMass' in gal_data.colnames:
                    total_stellar_mass = np.nansum(gal_data['stellarMass'])
                else:
                    total_stellar_mass = np.nan  # unknown

                print(f"  Found {galaxy_count} galaxies, total stellar mass approx: {total_stellar_mass:.2e} M☉")

            results.append({
                'name': name,
                'RA': ra,
                'DEC': dec,
                'galaxy_count': galaxy_count,
                'stellar_mass_sum': total_stellar_mass,
            })

        # Save batch results to CSV on Google Drive
        df = pd.DataFrame(results)
        save_file = f"{save_path}lens_mass_batch_{batch_start}_{batch_end-1}.csv"
        df.to_csv(save_file, index=False)
        print(f"Batch saved to {save_file}")

    print("\nProcessing complete.")


if __name__ == "__main__":
    main(batch_size=100, max_batches=140)  # Adjust batch_size/max_batches as needed

import lenscat
from astroquery.sdss import SDSS
from astropy.coordinates import SkyCoord
import astropy.units as u
import numpy as np
import pandas as pd
import time
from astropy.table import vstack

# --- Tiled SDSS query function ---
def query_large_area_tiled(ra_deg, dec_deg, search_radius_arcmin=20, tile_radius_arcmin=3, max_retries=3):
    center = SkyCoord(ra=ra_deg*u.deg, dec=dec_deg*u.deg, frame='icrs')
    search_radius_deg = search_radius_arcmin / 60
    tile_radius_deg = tile_radius_arcmin / 60

    spacing_deg = tile_radius_deg * np.sqrt(2)
    n_tiles = int(np.ceil((2 * search_radius_deg) / spacing_deg))

    ra_offsets = np.linspace(-search_radius_deg, search_radius_deg, n_tiles)
    dec_offsets = np.linspace(-search_radius_deg, search_radius_deg, n_tiles)

    results = []
    for ddec in dec_offsets:
        for dra in ra_offsets:
            tile_center = SkyCoord(ra=(ra_deg + dra) * u.deg, dec=(dec_deg + ddec) * u.deg, frame='icrs')
            if center.separation(tile_center).deg <= search_radius_deg:
                for attempt in range(1, max_retries+1):
                    try:
                        res = SDSS.query_region(tile_center, radius=tile_radius_deg * u.deg, spectro=False)
                        if res is not None:
                            results.append(res)
                        break  # success, exit retry loop
                    except Exception as e:
                        print(f"Retry {attempt}/{max_retries} failed for tile at RA={tile_center.ra.deg:.4f}, DEC={tile_center.dec.deg:.4f}: {e}")
                        time.sleep(2)  # wait before retry
                else:
                    print(f"Failed all retries for tile at RA={tile_center.ra.deg:.4f}, DEC={tile_center.dec.deg:.4f}")

    if results:
        combined = vstack(results)
        # Deduplicate based on ra/dec within small tolerance
        combined = combined.group_by(['ra', 'dec']).groups.aggregate(np.min)
        return combined
    else:
        return None


# --- Main processing ---
def main(batch_size=100, max_batches=None, save_path="/content/drive/MyDrive/lens_mass_batches/"):
    print("Loading lens catalog from lenscat package...")
    cat = lenscat.load_lens_catalog()
    # Filter confident/probable lenses with numeric zlens
    lenses = cat[(cat['grading'].isin(['confident', 'probable'])) & (~cat['zlens'].mask)]

    print(f"Total filtered lenses: {len(lenses)}")

    if max_batches is not None:
        total_to_process = min(len(lenses), batch_size * max_batches)
    else:
        total_to_process = len(lenses)

    for batch_start in range(0, total_to_process, batch_size):
        batch_end = min(batch_start + batch_size, total_to_process)
        batch = lenses[batch_start:batch_end]

        results = []
        print(f"\nProcessing batch {batch_start} to {batch_end-1} ({batch_end - batch_start} lenses)...")

        for idx, lens in enumerate(batch):
            ra = lens['RA']
            dec = lens['DEC']
            name = lens['name']

            print(f"Querying lens {name} at RA={ra:.4f}, DEC={dec:.4f}")

            gal_data = query_large_area_tiled(ra, dec, search_radius_arcmin=20, tile_radius_arcmin=3)

            if gal_data is None or len(gal_data) == 0:
                print(f"  No galaxies found for lens {name}")
                total_stellar_mass = 0.0
                galaxy_count = 0
            else:
                galaxy_count = len(gal_data)
                # Check for 'stellarMass' column; if not present, just count galaxies
                if 'stellarMass' in gal_data.colnames:
                    total_stellar_mass = np.nansum(gal_data['stellarMass'])
                else:
                    total_stellar_mass = np.nan  # unknown

                print(f"  Found {galaxy_count} galaxies, total stellar mass approx: {total_stellar_mass:.2e} M☉")

            results.append({
                'name': name,
                'RA': ra,
                'DEC': dec,
                'galaxy_count': galaxy_count,
                'stellar_mass_sum': total_stellar_mass,
            })

        # Save batch results to CSV on Google Drive
        df = pd.DataFrame(results)
        save_file = f"{save_path}lens_mass_batch_{batch_start}_{batch_end-1}.csv"
        df.to_csv(save_file, index=False)
        print(f"Batch saved to {save_file}")

    print("\nProcessing complete.")


if __name__ == "__main__":
    main(batch_size=100, max_batches=140)  # Adjust batch_size/max_batches as needed

import lenscat
from astroquery.sdss import SDSS
from astropy.coordinates import SkyCoord
import astropy.units as u
import numpy as np
import pandas as pd
import time
from astropy.table import vstack

# --- Tiled SDSS query function ---
def query_large_area_tiled(ra_deg, dec_deg, search_radius_arcmin=20, tile_radius_arcmin=3, max_retries=3):
    center = SkyCoord(ra=ra_deg*u.deg, dec=dec_deg*u.deg, frame='icrs')
    search_radius_deg = search_radius_arcmin / 60
    tile_radius_deg = tile_radius_arcmin / 60

    spacing_deg = tile_radius_deg * np.sqrt(2)
    n_tiles = int(np.ceil((2 * search_radius_deg) / spacing_deg))

    ra_offsets = np.linspace(-search_radius_deg, search_radius_deg, n_tiles)
    dec_offsets = np.linspace(-search_radius_deg, search_radius_deg, n_tiles)

    results = []
    for ddec in dec_offsets:
        for dra in ra_offsets:
            tile_center = SkyCoord(ra=(ra_deg + dra) * u.deg, dec=(dec_deg + ddec) * u.deg, frame='icrs')
            if center.separation(tile_center).deg <= search_radius_deg:
                for attempt in range(1, max_retries+1):
                    try:
                        res = SDSS.query_region(tile_center, radius=tile_radius_deg * u.deg, spectro=False)
                        if res is not None:
                            results.append(res)
                        break  # success, exit retry loop
                    except Exception as e:
                        print(f"Retry {attempt}/{max_retries} failed for tile at RA={tile_center.ra.deg:.4f}, DEC={tile_center.dec.deg:.4f}: {e}")
                        time.sleep(2)  # wait before retry
                else:
                    print(f"Failed all retries for tile at RA={tile_center.ra.deg:.4f}, DEC={tile_center.dec.deg:.4f}")

    if results:
        combined = vstack(results)
        # Deduplicate based on ra/dec within small tolerance
        combined = combined.group_by(['ra', 'dec']).groups.aggregate(np.min)
        return combined
    else:
        return None


# --- Main processing ---
def main(batch_size=100, max_batches=None, save_path="/content/drive/MyDrive/lens_mass_batches/"):
    print("Loading lens catalog from lenscat package...")
    cat = lenscat.load_lens_catalog()
    # Filter confident/probable lenses with numeric zlens
    lenses = cat[(cat['grading'].isin(['confident', 'probable'])) & (~cat['zlens'].mask)]

    print(f"Total filtered lenses: {len(lenses)}")

    if max_batches is not None:
        total_to_process = min(len(lenses), batch_size * max_batches)
    else:
        total_to_process = len(lenses)

    for batch_start in range(0, total_to_process, batch_size):
        batch_end = min(batch_start + batch_size, total_to_process)
        batch = lenses[batch_start:batch_end]

        results = []
        print(f"\nProcessing batch {batch_start} to {batch_end-1} ({batch_end - batch_start} lenses)...")

        for idx, lens in enumerate(batch):
            ra = lens['RA']
            dec = lens['DEC']
            name = lens['name']

            print(f"Querying lens {name} at RA={ra:.4f}, DEC={dec:.4f}")

            gal_data = query_large_area_tiled(ra, dec, search_radius_arcmin=20, tile_radius_arcmin=3)

            if gal_data is None or len(gal_data) == 0:
                print(f"  No galaxies found for lens {name}")
                total_stellar_mass = 0.0
                galaxy_count = 0
            else:
                galaxy_count = len(gal_data)
                # Check for 'stellarMass' column; if not present, just count galaxies
                if 'stellarMass' in gal_data.colnames:
                    total_stellar_mass = np.nansum(gal_data['stellarMass'])
                else:
                    total_stellar_mass = np.nan  # unknown

                print(f"  Found {galaxy_count} galaxies, total stellar mass approx: {total_stellar_mass:.2e} M☉")

            results.append({
                'name': name,
                'RA': ra,
                'DEC': dec,
                'galaxy_count': galaxy_count,
                'stellar_mass_sum': total_stellar_mass,
            })

        # Save batch results to CSV on Google Drive
        df = pd.DataFrame(results)
        save_file = f"{save_path}lens_mass_batch_{batch_start}_{batch_end-1}.csv"
        df.to_csv(save_file, index=False)
        print(f"Batch saved to {save_file}")

    print("\nProcessing complete.")


if __name__ == "__main__":
    main(batch_size=100, max_batches=140)  # Adjust batch_size/max_batches as needed

import lenscat
print(dir(lenscat))

# Make sure you have these installed in your environment:
# !pip install -q lenscat astroquery pandas astropy

import os
import time
import numpy as np
import pandas as pd
from astropy.table import Table
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u
import lenscat

# --- PARAMETERS ---
BATCH_SIZE = 100       # lenses per batch (adjust if needed)
MAX_BATCHES = None     # set to None to process all batches
RADIUS_ARCMIN = 20     # search radius around each lens in arcminutes
SAVE_DIR = "/content/drive/MyDrive/lens_bh_batches/"  # Change to your Google Drive folder or local path

# Create save directory if it doesn't exist
os.makedirs(SAVE_DIR, exist_ok=True)

# SIMBAD config: limit to BH-like types
Simbad.TIMEOUT = 30
custom_simbad = Simbad()
custom_simbad.add_votable_fields('otype')  # Object types

# Define BH-type SIMBAD object types to look for
BH_TYPES = set(['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO'])

def query_bh_count(ra, dec, radius_arcmin):
    """Query SIMBAD for BH-type objects within radius_arcmin around (ra, dec)."""
    coord = SkyCoord(ra=ra, dec=dec, unit=(u.deg, u.deg), frame='icrs')
    try:
        result = custom_simbad.query_region(coord, radius=radius_arcmin * u.arcmin)
        if result is None:
            return 0
        # Filter by BH types only
        types = result['OTYPE']
        count = sum(t.decode('utf-8') in BH_TYPES for t in types)
        return count
    except Exception as e:
        print(f"  SIMBAD query error at RA={ra}, DEC={dec}: {e}")
        return np.nan

def process_batch(df_batch, batch_index):
    """Process one batch of lenses: query BH counts and save results."""
    counts = []
    for i, row in df_batch.iterrows():
        ra, dec, name = row['RA'], row['DEC'], row['name']
        print(f"Querying lens {name} at RA={ra:.4f}, DEC={dec:.4f}")
        count = query_bh_count(ra, dec, RADIUS_ARCMIN)
        counts.append(count)
        time.sleep(1.0)  # polite delay to avoid SIMBAD throttling
    df_batch = df_batch.copy()
    df_batch['bh_count_20arcmin'] = counts
    save_path = os.path.join(SAVE_DIR, f"lens_batch_{batch_index:03d}.csv")
    df_batch.to_csv(save_path, index=False)
    print(f"Saved batch {batch_index} to {save_path}")

    # Summary stats
    total = len(df_batch)
    num_with_bh = np.sum(pd.Series(counts) > 0)
    mean_bh = np.nanmean(counts)
    print(f"Batch {batch_index} summary: {num_with_bh}/{total} lenses have ≥1 BH-type object nearby, mean BH count = {mean_bh:.2f}")

def main():
    print("Loading full catalog from lenscat package...")
    catalog: Table = lenscat.catalog
    df = catalog.to_pandas()

    print("Filtering confident/probable lenses with numeric redshift...")
    df_strong = df[df['grading'].isin(['confident', 'probable'])].copy()
    df_strong['z'] = pd.to_numeric(df_strong['zlens'], errors='coerce')
    df_strong = df_strong.dropna(subset=['z']).reset_index(drop=True)
    print(f"Total strong lenses with redshift: {len(df_strong)}")

    total_lenses = len(df_strong)
    max_batches = MAX_BATCHES if MAX_BATCHES is not None else (total_lenses + BATCH_SIZE - 1) // BATCH_SIZE

    for batch_index in range(max_batches):
        start = batch_index * BATCH_SIZE
        end = min(start + BATCH_SIZE, total_lenses)
        df_batch = df_strong.iloc[start:end]
        print(f"\nProcessing batch {batch_index} lenses {start} to {end - 1}")
        process_batch(df_batch, batch_index)

    print("Processing complete!")

if __name__ == "__main__":
    main()

coord = SkyCoord(ra=12.6158, dec=-17.6693, unit=(u.deg, u.deg), frame='icrs')
result = custom_simbad.query_region(coord, radius=20*u.arcmin)
print(result.colnames)

# === Full Working Script ===
# Requirements: run in Google Colab or local with lenscat installed and Google Drive mounted
# Queries SIMBAD around lenses at 20 arcmin radius, handles 'OTYPE' key errors gracefully,
# saves batch results as CSV in Google Drive folder you specify.

import time
import pandas as pd
from astropy.table import Table
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u
import numpy as np
import os

# -- User parameters --
RADIUS_ARCMIN = 20  # query radius in arcminutes
BATCH_SIZE = 100    # how many lenses per batch
MAX_BATCHES = None  # or set a number to limit batches
SAVE_DIR = "/content/drive/MyDrive/lens_mass_batches/"  # make sure Google Drive is mounted here

# -- Prepare SIMBAD query: Request OTYPE column but handle missing gracefully --
custom_simbad = Simbad()
custom_simbad.add_votable_fields('otypes')

def query_bh_count(ra_deg, dec_deg, radius_arcmin=20):
    """Query SIMBAD for BH-related objects within radius_arcmin of (ra_deg, dec_deg)."""
    coord = SkyCoord(ra=ra_deg, dec=dec_deg, unit='deg', frame='icrs')
    try:
        result = custom_simbad.query_region(coord, radius=radius_arcmin * u.arcmin)
        if result is None or len(result) == 0:
            return 0
        # OTYPE column can be uppercase or lowercase or missing, so check keys carefully
        otype_key = None
        for key in result.colnames:
            if key.lower() == 'otypes':
                otype_key = key
                break
        if otype_key is None:
            # If missing, treat as no BH objects found (or could be zero)
            return 0
        # Filter BH-related types (you can expand this list if you want)
        bh_types = {'BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO'}
        count = 0
        for otype_str in result[otype_key]:
            # otype_str can contain multiple types separated by commas, split and check
            types = {x.strip().upper() for x in str(otype_str).split(',')}
            if types.intersection(bh_types):
                count += 1
        return count
    except Exception as e:
        print(f"  SIMBAD query error at RA={ra_deg}, DEC={dec_deg}: {str(e)}")
        return np.nan  # Return NaN on error for better error tracking

def process_batch(df_batch, batch_index):
    print(f"\nProcessing batch {batch_index} lenses {batch_index * BATCH_SIZE} to {(batch_index+1)*BATCH_SIZE - 1}")
    counts = []
    for idx, row in df_batch.iterrows():
        ra, dec = row['ra'], row['dec']
        print(f" Querying lens {row['name']} at RA={ra:.4f}, DEC={dec:.4f}")
        count = query_bh_count(ra, dec, RADIUS_ARCMIN)
        counts.append(count)
        time.sleep(1.0)  # polite delay to avoid SIMBAD throttling
    df_batch = df_batch.copy()
    df_batch['bh_count_20arcmin'] = counts
    return df_batch

def main():
    # Load catalog from lenscat package (built-in)
    print("Loading full catalog from lenscat package...")
    import lenscat
    catalog: Table = lenscat.catalog
    print(f"Total lenses in catalog: {len(catalog):,}")
    df = catalog.to_pandas()

    # Filter confident/probable with numeric redshift
    print("Filtering confident/probable lenses with numeric redshift...")
    df_strong = df[df['grading'].isin(['confident', 'probable'])].copy()
    df_strong['z'] = pd.to_numeric(df_strong['zlens'], errors='coerce')
    df_strong = df_strong.dropna(subset=['z']).reset_index(drop=True)
    print(f"Total strong lenses with redshift: {len(df_strong):,}")

    # Ensure save directory exists
    if not os.path.exists(SAVE_DIR):
        os.makedirs(SAVE_DIR)

    # Batch processing
    n_lenses = len(df_strong)
    total_batches = n_lenses // BATCH_SIZE + (1 if n_lenses % BATCH_SIZE != 0 else 0)
    if MAX_BATCHES is not None:
        total_batches = min(total_batches, MAX_BATCHES)

    for batch_idx in range(total_batches):
        start_idx = batch_idx * BATCH_SIZE
        end_idx = min(start_idx + BATCH_SIZE, n_lenses)
        df_batch = df_strong.iloc[start_idx:end_idx]

        # Check if this batch already processed (skip if file exists)
        save_path = os.path.join(SAVE_DIR, f"batch_{batch_idx:03d}.csv")
        if os.path.exists(save_path):
            print(f"Batch {batch_idx} already processed, loading saved results...")
            continue

        df_results = process_batch(df_batch, batch_idx)
        df_results.to_csv(save_path, index=False)
        print(f"Saved batch {batch_idx} results to {save_path}")

    print("\nAll batches processed!")

if __name__ == "__main__":
    main()

print("Columns in dataframe:", df_strong.columns)

import lenscat
import pandas as pd

print("Loading full catalog from lenscat package...")
catalog = lenscat.catalog
df = catalog.to_pandas()

# Filter confident/probable lenses with numeric redshift
df_strong = df[df['grading'].isin(['confident', 'probable'])].copy()
df_strong['z'] = pd.to_numeric(df_strong['zlens'], errors='coerce')
df_strong = df_strong.dropna(subset=['z']).reset_index(drop=True)

print("Columns in df_strong:", df_strong.columns)

# Install lenscat if needed
# !pip install -q lenscat astroquery

import time
import pandas as pd
from astropy.table import Table
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u
import lenscat

# Configure SIMBAD to return 'OTYPE' (object type)
custom_simbad = Simbad()
custom_simbad.add_votable_fields('otypes')

# Constants
RADIUS_ARCMIN = 20  # search radius
BATCH_SIZE = 100    # lenses per batch
MAX_RETRIES = 3     # SIMBAD retries

def query_bh_count(ra, dec, radius_arcmin):
    coords = SkyCoord(ra=ra, dec=dec, unit=(u.deg, u.deg))
    retry = 0
    while retry < MAX_RETRIES:
        try:
            result = custom_simbad.query_region(coords, radius=radius_arcmin * u.arcmin)
            if result is None:
                return 0
            # Count BH-related types: 'BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO'
            count = 0
            for otype in result['OTYPES']:
                # OTYPE strings can be combined, e.g., 'AGN;QSO'
                if any(x in otype.decode('utf-8') for x in ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']):
                    count += 1
            return count
        except Exception as e:
            print(f"  SIMBAD query error at RA={ra}, DEC={dec}: {e}")
            retry += 1
            time.sleep(2)  # wait before retrying
    return None  # failed all retries

def process_batch(df_batch, batch_index):
    counts = []
    for i, row in df_batch.iterrows():
        ra, dec = row['RA'], row['DEC']
        print(f"Querying lens {row['name']} at RA={ra}, DEC={dec}")
        count = query_bh_count(ra, dec, RADIUS_ARCMIN)
        if count is None:
            print(f"  Failed all retries for lens {row['name']}")
            count = -1  # mark failure
        counts.append(count)
        time.sleep(1)  # polite delay to avoid SIMBAD throttling
    df_batch = df_batch.copy()
    df_batch['bh_count_20arcmin'] = counts
    # Save batch results
    save_path = f"/content/drive/MyDrive/lens_bh_results/batch_{batch_index}.csv"
    df_batch.to_csv(save_path, index=False)
    print(f"Batch {batch_index} saved to {save_path}")
    return df_batch

def main():
    print("Loading full catalog from lenscat package...")
    catalog: Table = lenscat.catalog
    df = catalog.to_pandas()

    print("Filtering confident/probable lenses with numeric redshift...")
    df_strong = df[df['grading'].isin(['confident', 'probable'])].copy()
    df_strong['z'] = pd.to_numeric(df_strong['zlens'], errors='coerce')
    df_strong = df_strong.dropna(subset=['z']).reset_index(drop=True)

    print(f"Total strong lenses with redshift: {len(df_strong):,}")

    total_lenses = len(df_strong)
    num_batches = (total_lenses + BATCH_SIZE - 1) // BATCH_SIZE

    for batch_index in range(num_batches):
        start = batch_index * BATCH_SIZE
        end = min(start + BATCH_SIZE, total_lenses)
        print(f"\nProcessing batch {batch_index} lenses {start} to {end-1}")
        batch_df = df_strong.iloc[start:end]
        process_batch(batch_df, batch_index)

if __name__ == "__main__":
    main()

# Install lenscat if needed
# !pip install -q lenscat astroquery

import os
import time
import pandas as pd
from astropy.table import Table
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u

# --- PARAMETERS ---
RADIUS_ARCMIN = 20  # query radius in arcminutes (SIMBAD limit: 20)
BATCH_SIZE = 100
MAX_RETRIES = 3
SAVE_PATH = "/content/drive/MyDrive/lens_mass_batches/"
BH_TYPES = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

# Create save directory if it doesn't exist
os.makedirs(SAVE_PATH, exist_ok=True)

def query_bh_count(ra, dec, radius_arcmin):
    custom_simbad = Simbad()
    custom_simbad.add_votable_fields('otype')  # singular!
    count = 0

    coord = SkyCoord(ra=ra, dec=dec, unit=(u.deg, u.deg), frame='icrs')
    for attempt in range(MAX_RETRIES):
        try:
            result = custom_simbad.query_region(coord, radius=radius_arcmin*u.arcmin)
            if result is None:
                return 0
            for otype in result['OTYPE']:
                # otype may be bytes, decode if needed
                otype_str = otype.decode('utf-8') if isinstance(otype, bytes) else str(otype)
                if any(bhtype in otype_str for bhtype in BH_TYPES):
                    count += 1
            return count
        except Exception as e:
            print(f"  SIMBAD query error at RA={ra}, DEC={dec}: {e}. Retry {attempt+1}/{MAX_RETRIES}")
            time.sleep(1.5)  # wait before retry
    print(f"  Failed all retries for RA={ra}, DEC={dec}")
    return None

def process_batch(df_batch, batch_index):
    counts = []
    for idx, row in df_batch.iterrows():
        ra, dec = row['RA'], row['DEC']
        print(f"Querying lens {row['name']} at RA={ra:.4f}, DEC={dec:.4f}")
        count = query_bh_count(ra, dec, RADIUS_ARCMIN)
        counts.append(count if count is not None else -1)
        time.sleep(1.0)  # polite delay between queries
    df_batch = df_batch.copy()
    df_batch['bh_count_20arcmin'] = counts
    filename = os.path.join(SAVE_PATH, f"lens_batch_{batch_index}.csv")
    df_batch.to_csv(filename, index=False)
    print(f"Batch {batch_index} saved to {filename}\n")

def main():
    print("Loading full catalog from lenscat package...")
    import lenscat
    catalog: Table = lenscat.catalog
    df = catalog.to_pandas()

    print("Filtering confident/probable lenses with numeric redshift...")
    df_strong = df[df['grading'].isin(['confident', 'probable'])].copy()
    df_strong['z'] = pd.to_numeric(df_strong['zlens'], errors='coerce')
    df_strong = df_strong.dropna(subset=['z']).reset_index(drop=True)
    print(f"Total strong lenses with redshift: {len(df_strong)}\n")

    total = len(df_strong)
    n_batches = (total + BATCH_SIZE - 1) // BATCH_SIZE
    print(f"Processing {n_batches} batches of size {BATCH_SIZE}...\n")

    for batch_index in range(n_batches):
        start = batch_index * BATCH_SIZE
        end = min(start + BATCH_SIZE, total)
        print(f"Processing batch {batch_index} lenses {start} to {end - 1}")
        df_batch = df_strong.iloc[start:end]
        process_batch(df_batch, batch_index)

if __name__ == "__main__":
    main()

# Install lenscat if needed
# !pip install -q lenscat astroquery

import os
import time
import pandas as pd
from astropy.table import Table
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u

# --- PARAMETERS ---
RADIUS_ARCMIN = 20  # query radius in arcminutes (SIMBAD limit: 20)
BATCH_SIZE = 100
MAX_RETRIES = 3
SAVE_PATH = "/content/drive/MyDrive/lens_mass_batches/"
BH_TYPES = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

# Create save directory if it doesn't exist
os.makedirs(SAVE_PATH, exist_ok=True)

def query_bh_count(ra, dec, radius_arcmin):
    custom_simbad = Simbad()
    custom_simbad.add_votable_fields('otype')  # singular!
    count = 0

    coord = SkyCoord(ra=ra, dec=dec, unit=(u.deg, u.deg), frame='icrs')
    for attempt in range(MAX_RETRIES):
        try:
            result = custom_simbad.query_region(coord, radius=radius_arcmin*u.arcmin)
            if result is None:
                return 0
            for otype in result['OTYPE']:
                # otype may be bytes, decode if needed
                otype_str = otype.decode('utf-8') if isinstance(otype, bytes) else str(otype)
                if any(bhtype in otype_str for bhtype in BH_TYPES):
                    count += 1
            return count
        except Exception as e:
            print(f"  SIMBAD query error at RA={ra}, DEC={dec}: {e}. Retry {attempt+1}/{MAX_RETRIES}")
            time.sleep(1.5)  # wait before retry
    print(f"  Failed all retries for RA={ra}, DEC={dec}")
    return None

def process_batch(df_batch, batch_index):
    counts = []
    for idx, row in df_batch.iterrows():
        ra, dec = row['RA'], row['DEC']
        print(f"Querying lens {row['name']} at RA={ra:.4f}, DEC={dec:.4f}")
        count = query_bh_count(ra, dec, RADIUS_ARCMIN)
        counts.append(count if count is not None else -1)
        time.sleep(1.0)  # polite delay between queries
    df_batch = df_batch.copy()
    df_batch['bh_count_20arcmin'] = counts
    filename = os.path.join(SAVE_PATH, f"lens_batch_{batch_index}.csv")
    df_batch.to_csv(filename, index=False)
    print(f"Batch {batch_index} saved to {filename}\n")

def main():
    print("Loading full catalog from lenscat package...")
    import lenscat
    catalog: Table = lenscat.catalog
    df = catalog.to_pandas()

    print("Filtering confident/probable lenses with numeric redshift...")
    df_strong = df[df['grading'].isin(['confident', 'probable'])].copy()
    df_strong['z'] = pd.to_numeric(df_strong['zlens'], errors='coerce')
    df_strong = df_strong.dropna(subset=['z']).reset_index(drop=True)
    print(f"Total strong lenses with redshift: {len(df_strong)}\n")

    total = len(df_strong)
    n_batches = (total + BATCH_SIZE - 1) // BATCH_SIZE
    print(f"Processing {n_batches} batches of size {BATCH_SIZE}...\n")

    for batch_index in range(n_batches):
        start = batch_index * BATCH_SIZE
        end = min(start + BATCH_SIZE, total)
        print(f"Processing batch {batch_index} lenses {start} to {end - 1}")
        df_batch = df_strong.iloc[start:end]
        process_batch(df_batch, batch_index)

if __name__ == "__main__":
    main()

# Install lenscat if not installed (uncomment if needed)
# !pip install -q lenscat astroquery astropy pandas

import time
import os
from astropy.coordinates import SkyCoord
from astropy import units as u
from astroquery.simbad import Simbad
import lenscat
import pandas as pd

# Parameters
BATCH_SIZE = 100            # Number of lenses per batch
MAX_BATCHES = None          # Set to None to process all batches
RADIUS_ARCMIN = 20          # Search radius for SIMBAD queries (arcminutes)
MAX_RETRIES = 3             # Number of retries per SIMBAD query
BH_TYPES = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']  # Black hole related object types
SAVE_DIR = "/content/drive/MyDrive/lens_bh_counts/"       # Google Drive save directory

# Create save directory if it doesn't exist
os.makedirs(SAVE_DIR, exist_ok=True)

def query_bh_count(ra, dec, radius_arcmin):
    custom_simbad = Simbad()
    custom_simbad.add_votable_fields('otype')  # Use lowercase 'otype'
    coord = SkyCoord(ra=ra, dec=dec, unit=(u.deg, u.deg), frame='icrs')
    for attempt in range(MAX_RETRIES):
        try:
            result = custom_simbad.query_region(coord, radius=radius_arcmin*u.arcmin)
            if result is None:
                return 0
            if 'OTYPE' in result.colnames:
                otype_col = result['OTYPE']
            elif 'otype' in result.colnames:
                otype_col = result['otype']
            else:
                print(f"    Warning: 'OTYPE' missing in SIMBAD response at RA={ra}, DEC={dec}")
                return 0

            count = 0
            for otype in otype_col:
                # Decode bytes if needed
                otype_str = otype.decode('utf-8') if isinstance(otype, bytes) else str(otype)
                if any(bhtype in otype_str for bhtype in BH_TYPES):
                    count += 1
            return count
        except Exception as e:
            print(f"  SIMBAD query error at RA={ra}, DEC={dec}: {e}. Retry {attempt+1}/{MAX_RETRIES}")
            time.sleep(1.5)
    print(f"  Failed all retries for RA={ra}, DEC={dec}")
    return None

def process_batch(df_batch, batch_index):
    counts = []
    print(f"Processing batch {batch_index} lenses {batch_index*BATCH_SIZE} to {batch_index*BATCH_SIZE + len(df_batch)-1}")
    for i, row in df_batch.iterrows():
        ra = row['RA']
        dec = row['DEC']
        name = row['name']
        print(f" Querying lens {name} at RA={ra:.4f}, DEC={dec:.4f}")
        count = query_bh_count(ra, dec, RADIUS_ARCMIN)
        counts.append(count)
        time.sleep(1)  # Polite delay to avoid throttling
    df_batch = df_batch.copy()
    df_batch['bh_count_20arcmin'] = counts
    # Save batch results to CSV in Drive
    save_path = os.path.join(SAVE_DIR, f"lens_bh_counts_batch_{batch_index}.csv")
    df_batch.to_csv(save_path, index=False)
    print(f"Saved batch {batch_index} results to {save_path}")
    return df_batch

def main():
    print("Loading full catalog from lenscat package...")
    catalog = lenscat.catalog  # astropy Table

    # Convert to pandas DataFrame
    df = catalog.to_pandas()

    print("Filtering confident/probable lenses with numeric redshift...")
    df_strong = df[df['grading'].isin(['confident', 'probable'])].copy()
    df_strong['z'] = pd.to_numeric(df_strong['zlens'], errors='coerce')
    df_strong = df_strong.dropna(subset=['z']).reset_index(drop=True)
    print(f"Total strong lenses with redshift: {len(df_strong):,}")

    total_lenses = len(df_strong)
    total_batches = (total_lenses + BATCH_SIZE - 1) // BATCH_SIZE
    if MAX_BATCHES is not None:
        total_batches = min(total_batches, MAX_BATCHES)

    for batch_index in range(total_batches):
        start_idx = batch_index * BATCH_SIZE
        end_idx = min(start_idx + BATCH_SIZE, total_lenses)
        df_batch = df_strong.iloc[start_idx:end_idx]
        process_batch(df_batch, batch_index)

if __name__ == "__main__":
    main()

import lenscat

# Load catalog table
cat = lenscat.LENS_CAT  # this is an Astropy Table

print(f"Total lenses in catalog: {len(cat)}")

# Filter lenses: only 'confident' or 'probable', and non-masked redshift
filtered_lenses = cat[(cat['grading'] == 'confident') | (cat['grading'] == 'probable')]
filtered_lenses = filtered_lenses[~filtered_lenses['zlens'].mask]

print(f"Filtered lenses count: {len(filtered_lenses)}")

!pip install --upgrade lenscat

import lenscat
import numpy as np
import pandas as pd
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.sdss import SDSS
from astropy.table import Table
import matplotlib.pyplot as plt

# --- Load and filter lens catalog ---
print("📦 Loading full catalog from lenscat package...")
df = lenscat.load_catalog()
print(f"✅ Total lenses loaded: {len(df):,}")

print("📋 Columns:", list(df.columns))

# Filter confident/probable lenses with numeric redshift
df_filtered = df[
    (df["grading"].str.lower().isin(["confident", "probable"])) &
    (df["zlens"].apply(lambda x: np.isfinite(x)))
].copy()

print(f"✅ Filtered lenses (confident/probable with numeric redshift): {len(df_filtered):,}")

# --- Helper function to query galaxies around lens position ---
def query_sdss_galaxies(center_coord, radius_arcmin=3):
    """Query SDSS for galaxies within radius_arcmin of center_coord."""
    xid = SDSS.query_region(center_coord, radius=radius_arcmin * u.arcmin, spectro=True)
    if xid is None:
        return Table()  # no results
    # Filter for galaxies only
    gal_mask = xid["class"] == "GALAXY"
    return xid[gal_mask]

# --- Estimate total stellar mass for a lens ---
def estimate_mass_for_lens(lens, radius_arcmin=3):
    coord = SkyCoord(ra=lens["RA"]*u.deg, dec=lens["DEC"]*u.deg)
    galaxies = query_sdss_galaxies(coord, radius_arcmin=radius_arcmin)
    if len(galaxies) == 0:
        print(f"  No galaxies found for lens {lens['name']}")
        return 0.0

    # Use stellar mass proxy from SDSS if available (example: use "stellarMass" or proxy)
    # Here we simplify: count galaxies and multiply by a typical mass (1e11 Msun)
    # Replace with actual stellar mass column if available
    typical_mass_per_galaxy = 1e11
    total_mass = typical_mass_per_galaxy * len(galaxies)
    print(f"  Found {len(galaxies)} galaxies, total stellar mass approx: {total_mass:.2e} M☉")
    return total_mass

# --- Main processing ---
N = 100  # number of lenses to process (adjust as needed)

results = []
print(f"\nProcessing {N} lenses for stellar mass estimation...\n")

for i, lens in df_filtered.iloc[:N].iterrows():
    print(f"Querying lens {lens['name']} at RA={lens['RA']:.4f}, DEC={lens['DEC']:.4f}")
    mass = estimate_mass_for_lens(lens, radius_arcmin=3)
    results.append({
        "lens_id": lens["name"],
        "RA": lens["RA"],
        "DEC": lens["DEC"],
        "stellar_mass_Msun": mass
    })

# Save raw results
results_df = pd.DataFrame(results)
results_df.to_csv("lens_stellar_mass_lenscat.csv", index=False)

print("\nDone querying. Results saved to 'lens_stellar_mass_lenscat.csv'\n")

# --- Post-processing: stats, histogram, classification ---

print("Computing distribution statistics...\n")
print(results_df["stellar_mass_Msun"].describe())

# Plot histogram
plt.figure(figsize=(8,5))
plt.hist(results_df["stellar_mass_Msun"], bins=30, alpha=0.7, color='skyblue')
plt.xlabel("Estimated Stellar Mass (M☉)")
plt.ylabel("Number of Lenses")
plt.title("Stellar Mass Distribution in Lens Neighborhoods")
plt.grid(True)
plt.tight_layout()
plt.show()

# Classify into Low / Medium / High based on quantiles
low_thresh = results_df["stellar_mass_Msun"].quantile(0.33)
high_thresh = results_df["stellar_mass_Msun"].quantile(0.67)

def classify_density(mass):
    if mass < low_thresh:
        return "Low"
    elif mass < high_thresh:
        return "Medium"
    else:
        return "High"

results_df["density_category"] = results_df["stellar_mass_Msun"].apply(classify_density)

print("\nSample classification results:")
print(results_df[["lens_id", "stellar_mass_Msun", "density_category"]].head(10))

# Save final results with classification
results_df.to_csv("lens_stellar_mass_lenscat_classified.csv", index=False)
print("\nFinal results with density classification saved to 'lens_stellar_mass_lenscat_classified.csv'")

import pandas as pd
import matplotlib.pyplot as plt

# Load results CSV
results_df = pd.read_csv('lens_stellar_mass_lenscat.csv')

print(f"Total lenses processed: {len(results_df):,}")

# Summary statistics for total stellar mass
print("\nStellar Mass (M☉) Summary Statistics:")
print(results_df['total_stellar_mass'].describe())

# Define quantiles for low/medium/high classification
quantiles = results_df['total_stellar_mass'].quantile([0.33, 0.66]).values
low_thresh, high_thresh = quantiles[0], quantiles[1]

def classify_density(mass):
    if mass < low_thresh:
        return 'Low Density'
    elif mass < high_thresh:
        return 'Medium Density'
    else:
        return 'High Density'

results_df['density_class'] = results_df['total_stellar_mass'].apply(classify_density)

# Count per category
density_counts = results_df['density_class'].value_counts()
print("\nLens counts by stellar mass density class:")
print(density_counts)

# Plot histogram of stellar masses
plt.figure(figsize=(10,6))
plt.hist(results_df['total_stellar_mass'], bins=30, color='skyblue', edgecolor='black')
plt.xlabel('Total Stellar Mass (M☉)')
plt.ylabel('Number of Lenses')
plt.title('Distribution of Total Stellar Mass in Lens Neighborhoods')
plt.show()

import pandas as pd

results_df = pd.read_csv('lens_stellar_mass_lenscat.csv')
print(results_df.columns)

import pandas as pd

results_df = pd.read_csv('lens_stellar_mass_lenscat.csv')

print(f"Total lenses processed: {len(results_df)}\n")

stellar_mass = results_df['total_stellar_mass_Msun'].dropna()

print("Stellar Mass (M☉) Summary Statistics:")
print("-" * 40)
print(f"Mean: {stellar_mass.mean():.2e}")
print(f"Median: {stellar_mass.median():.2e}")
print(f"Standard Deviation: {stellar_mass.std():.2e}")
print(f"Min: {stellar_mass.min():.2e}")
print(f"Max: {stellar_mass.max():.2e}")

# Optional: define density bins and count how many lenses fall into each
bins = [0, 1e12, 5e12, 1e13, 5e13, stellar_mass.max()]
labels = ['Very Low', 'Low', 'Medium', 'High', 'Very High']
density_categories = pd.cut(stellar_mass, bins=bins, labels=labels, include_lowest=True)

print("\nLenses per Density Category:")
print(density_categories.value_counts().sort_index())

import os

drive_path = "/content/drive/My Drive"
csv_files = [f for f in os.listdir(drive_path) if f.endswith('.csv')]

print("CSV files in My Drive:")
for f in csv_files:
    print(f)

import pandas as pd
import os

file_path = "/content/drive/My Drive/lens_catalog.csv"

# Confirm the file exists
if not os.path.isfile(file_path):
    raise FileNotFoundError(f"CSV file not found at: {file_path}")

# Load it
df = pd.read_csv(file_path)

print("Total lenses processed:", len(df))
print("\nAvailable columns:\n", df.columns)

# Mount Google Drive if running in Google Colab
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import os

# Path to your CSV file in Google Drive (adjust this to your actual folder path)
file_path = '/content/drive/My Drive/lens_stellar_mass_lenscat.csv'

# Check file exists
if not os.path.isfile(file_path):
    raise FileNotFoundError(f"CSV file not found at: {file_path}")

# Load data
results_df = pd.read_csv(file_path)

# Summary
print(f"✅ Total lenses processed: {len(results_df)}")

# Extract stellar mass values (drop NaNs)
stellar_mass = results_df['total_stellar_mass_Msun'].dropna()

if stellar_mass.empty:
    raise ValueError("❌ No stellar mass values found in the file.")

# Basic statistics
print("\n📊 Stellar Mass (M☉) Summary Statistics:")
print("-" * 45)
print(f"Mean: {stellar_mass.mean():.2e}")
print(f"Median: {stellar_mass.median():.2e}")
print(f"Standard Deviation: {stellar_mass.std():.2e}")
print(f"Min: {stellar_mass.min():.2e}")
print(f"Max: {stellar_mass.max():.2e}")

# Define mass density bins
bins = [0, 1e12, 5e12, 1e13, 5e13, stellar_mass.max()]
labels = ['Very Low', 'Low', 'Medium', 'High', 'Very High']
density_categories = pd.cut(stellar_mass, bins=bins, labels=labels, include_lowest=True)

# Output count per density bin
print("\n📈 Lenses per Density Category:")
print("-" * 45)
print(density_categories.value_counts().sort_index())

results_df = pd.read_csv('lens_stellar_mass_lenscat.csv')

import os

# List all files under 'My Drive'
drive_path = "/content/drive/My Drive"
for root, dirs, files in os.walk(drive_path):
    for name in files:
        if 'lens_stellar_mass' in name:
            print(os.path.join(root, name))

import os

drive_path = "/content/drive/My Drive"
print("Files in My Drive:")
print(os.listdir(drive_path))

import pandas as pd
import os

# File path to the likely CSV
file_path = "/content/drive/My Drive/lens_catalog.csv"

# Check file exists
if not os.path.isfile(file_path):
    raise FileNotFoundError(f"CSV file not found at: {file_path}")

# Load data
results_df = pd.read_csv(file_path)

print(f"Total lenses processed: {len(results_df)}\n")

# Attempt to use the column that contains stellar mass
# If you're unsure of the exact column name, list them:
print("Available columns:")
print(results_df.columns)

# Then modify this line to match the real column name if different
stellar_mass = results_df['total_stellar_mass_Msun'].dropna()

# Summary stats
print("Stellar Mass (M☉) Summary Statistics:")
print("-" * 40)
print(f"Mean: {stellar_mass.mean():.2e}")
print(f"Median: {stellar_mass.median():.2e}")
print(f"Standard Deviation: {stellar_mass.std():.2e}")
print(f"Min: {stellar_mass.min():.2e}")
print(f"Max: {stellar_mass.max():.2e}")

# Define bins for mass density
bins = [0, 1e12, 5e12, 1e13, 5e13, stellar_mass.max()]
labels = ['Very Low', 'Low', 'Medium', 'High', 'Very High']
density_categories = pd.cut(stellar_mass, bins=bins, labels=labels, include_lowest=True)

# Count lenses in each category
print("\nLenses per Density Category:")
print(density_categories.value_counts().sort_index())

import lenscat
import pandas as pd
from astropy.coordinates import SkyCoord
from astropy.table import Table
import astropy.units as u
from astroquery.sdss import SDSS
import numpy as np
import time

def query_sdss_tiled(center_coord, total_radius_deg, tile_radius_arcmin=15):
    """
    Query SDSS in tiles to cover total_radius_deg area centered at center_coord.
    Returns concatenated Table of galaxies within total_radius_deg radius.
    """
    all_results = []
    n_tiles = int(np.ceil((total_radius_deg * 60) / tile_radius_arcmin))

    for i in range(-n_tiles, n_tiles + 1):
        for j in range(-n_tiles, n_tiles + 1):
            tile_center = SkyCoord(
                ra=center_coord.ra.degree + i * tile_radius_arcmin / 60.0,
                dec=center_coord.dec.degree + j * tile_radius_arcmin / 60.0,
                unit='deg'
            )
            query = f"SELECT TOP 10000 ra, dec, dered_g, dered_r, z FROM PhotoObj WHERE " \
                    f"dbo.fDistanceEq(ra, dec, {tile_center.ra.degree}, {tile_center.dec.degree}) < {tile_radius_arcmin * 60}"
            try:
                result = SDSS.query_sql(query)
                if result is not None:
                    all_results.append(result)
            except Exception as e:
                print(f"Warning: SDSS query failed at tile {i},{j}: {e}")
            time.sleep(0.1)  # be polite to SDSS

    if all_results:
        combined = Table.vstack(all_results)
        # Remove duplicates based on ra/dec
        combined = combined.group_by(['ra', 'dec']).groups.aggregate(np.mean)
        # Filter by radius from center_coord
        coords = SkyCoord(ra=combined['ra']*u.deg, dec=combined['dec']*u.deg)
        sep = coords.separation(center_coord)
        mask = sep < total_radius_deg * u.deg
        return combined[mask]
    else:
        return Table()

def estimate_mass_for_lens(lens, radius_arcmin=3):
    """
    Given a lens (with ra, dec), query SDSS galaxies within radius_arcmin,
    estimate total stellar mass from r-band luminosities using M/L ratio.
    Returns (total_stellar_mass_Msun, surface_mass_density_Msun_per_Mpc2)
    """
    center = SkyCoord(ra=lens['ra']*u.deg, dec=lens['dec']*u.deg)
    radius_deg = radius_arcmin / 60.0

    galaxies = query_sdss_tiled(center, radius_deg)
    if len(galaxies) == 0:
        return 0.0, 0.0

    # Approximate stellar mass: M/L ~ 3 in r-band (solar units)
    # Assume absolute magnitude of Sun in r-band ~ 4.67
    # Calculate distance modulus for lens redshift (assuming cosmology)
    from astropy.cosmology import Planck18 as cosmo
    dl = cosmo.luminosity_distance(lens['redshift']).to(u.pc).value  # parsecs

    # Convert dered_r to absolute magnitude
    m_r = galaxies['dered_r']
    M_r = m_r - 5 * (np.log10(dl) - 1)

    # Convert absolute magnitude to luminosity (solar units)
    L_r = 10 ** (-0.4 * (M_r - 4.67))

    # Stellar mass estimate
    stellar_mass = np.sum(L_r * 3)  # M/L = 3

    # Calculate physical area in Mpc^2 for surface density
    radius_rad = np.deg2rad(radius_deg)
    DA = cosmo.angular_diameter_distance(lens['redshift']).to(u.Mpc).value
    area_Mpc2 = np.pi * (radius_rad * DA) ** 2

    surface_density = stellar_mass / area_Mpc2 if area_Mpc2 > 0 else 0.0

    return stellar_mass, surface_density

# --- Main processing ---

print("📦 Loading full catalog from lenscat package...")
df = lenscat.load_catalog()


print(f"✅ Total lenses loaded: {len(df):,}")

# Filter lenses: confident/probable grading and numeric redshift
df_filtered = df[
    df['grading'].isin(['confident', 'probable']) &
    pd.to_numeric(df['zlens'], errors='coerce').notnull()
].copy()
df_filtered['redshift'] = pd.to_numeric(df_filtered['zlens'])

print(f"✅ Filtered lenses (confident/probable with numeric redshift): {len(df_filtered):,}")

N = 500  # Number of lenses to process
batch_save = 100  # Save every 100 lenses

results = []

for i, lens in df_filtered.iloc[:N].iterrows():
    print(f"Querying lens {lens['name']} at RA={lens['RA']:.4f}, DEC={lens['DEC']:.4f} ({i+1}/{N})")
    mass, sigma = estimate_mass_for_lens(lens, radius_arcmin=3)
    results.append({
        "lens_id": lens['name'],
        "ra": lens['RA'],
        "dec": lens['DEC'],
        "redshift": lens['redshift'],
        "total_stellar_mass_Msun": mass,
        "surface_mass_density_Msun_per_Mpc2": sigma,
    })

    if (i+1) % batch_save == 0:
        df_results = pd.DataFrame(results)
        df_results.to_csv('lens_stellar_mass_partial.csv', index=False)
        print(f"⚡ Saved intermediate results for {i+1} lenses.")

# Save final results
df_results = pd.DataFrame(results)
df_results.to_csv('lens_stellar_mass_500.csv', index=False)
print("🎉 Done processing 500 lenses. Results saved to 'lens_stellar_mass_500.csv'.")

import lenscat
from astroquery.sdss import SDSS
from astropy.coordinates import SkyCoord
from astropy.table import Table
import astropy.units as u
import pandas as pd
import numpy as np

def query_sdss_tiled(center_coord, total_radius_deg, tile_radius_arcmin=15):
    """
    Query SDSS in tiles around center_coord to cover the total_radius_deg area.
    """
    tile_radius_deg = tile_radius_arcmin / 60.0
    all_results = []

    # Calculate number of tiles to cover the circle of total_radius_deg radius
    # Simple grid in RA/DEC around center_coord
    steps = int(np.ceil((total_radius_deg*2) / (tile_radius_deg*2)))
    ras = center_coord.ra.deg + np.linspace(-total_radius_deg, total_radius_deg, steps)
    decs = center_coord.dec.deg + np.linspace(-total_radius_deg, total_radius_deg, steps)

    for ra in ras:
        for dec in decs:
            coord = SkyCoord(ra=ra, dec=dec, unit='deg', frame='icrs')
            try:
                res = SDSS.query_region(coord, radius=tile_radius_arcmin*u.arcmin, spectro=False)
                if res is not None and len(res) > 0:
                    all_results.append(res)
            except Exception as e:
                print(f"SDSS query error at RA={ra}, DEC={dec}: {e}")

    if all_results:
        combined = Table.vstack(all_results)
        return combined
    else:
        return Table()

def estimate_mass_for_lens(lens, radius_arcmin=3):
    """
    Query galaxies near lens and estimate total stellar mass.
    """
    center_coord = SkyCoord(ra=lens['RA'], dec=lens['DEC'], unit='deg')
    total_radius_deg = radius_arcmin / 60.0
    result_table = query_sdss_tiled(center_coord, total_radius_deg, tile_radius_arcmin=radius_arcmin)

    if len(result_table) == 0:
        print(f"  No galaxies found for lens {lens['name']}")
        return 0, 0

    # Filter galaxies with stellar mass estimates if available, else count objects as proxy
    if 'stellarmass' in result_table.colnames:
        masses = result_table['stellarmass']
        total_mass = np.nansum(masses)
    else:
        # If no stellar mass column, approximate mass by number of galaxies * average mass ~1e11 Msun (rough)
        total_mass = len(result_table) * 1e11

    return total_mass, np.std(result_table['z']) if 'z' in result_table.colnames else 0

# Load catalog and filter
print("📦 Loading full catalog from lenscat package...")
df = lenscat.load_catalog()
print(f"✅ Total lenses loaded: {len(df):,}")

df_filtered = df[(df['grading'].str.lower().isin(['confident', 'probable'])) & (df['zlens'].apply(lambda x: isinstance(x, (int, float))))]

print(f"✅ Filtered lenses (confident/probable with numeric redshift): {len(df_filtered):,}")

N = 500  # Number of lenses to process
results = []

print(f"\nProcessing {N} lenses for stellar mass estimation...\n")

for i, lens in df_filtered.iloc[:N].iterrows():
    print(f"Querying lens {lens['name']} at RA={lens['RA']:.4f}, DEC={lens['DEC']:.4f}")
    mass, sigma = estimate_mass_for_lens(lens, radius_arcmin=3)
    print(f"  Found {mass:.2e} M☉ total stellar mass")
    results.append({
        "lens_id": lens['name'],
        "ra": lens['RA'],
        "dec": lens['DEC'],
        "redshift": lens['zlens'],
        "total_stellar_mass_Msun": mass,
        "mass_sigma": sigma
    })

df_results = pd.DataFrame(results)
df_results.to_csv("lens_stellar_mass_lenscat_500.csv", index=False)
print("\nDone. Results saved to 'lens_stellar_mass_lenscat_500.csv'")

import lenscat

try:
    df = lenscat.get_lenses()
    print(f"✅ Loaded {len(df)} lenses")
    print(df.columns)
except AttributeError:
    print("lenscat.get_lenses() not found. Available functions:")
    print(dir(lenscat))
except Exception as e:
    print(f"Error loading lenses: {e}")

import lenscat

df = lenscat.df
print(f"✅ Loaded {len(df)} lenses")
print(df.columns)

import lenscat
from astroquery.sdss import SDSS
from astropy.coordinates import SkyCoord
from astropy.table import Table
import astropy.units as u
import pandas as pd
import numpy as np

def query_sdss_tiled(center_coord, total_radius_deg, tile_radius_arcmin=15):
    """
    Query SDSS in tiles around center_coord to cover the total_radius_deg area.
    """
    tile_radius_deg = tile_radius_arcmin / 60.0
    all_results = []

    # Calculate number of tiles to cover the circle of total_radius_deg radius
    steps = int(np.ceil((total_radius_deg*2) / (tile_radius_deg*2)))
    ras = center_coord.ra.deg + np.linspace(-total_radius_deg, total_radius_deg, steps)
    decs = center_coord.dec.deg + np.linspace(-total_radius_deg, total_radius_deg, steps)

    for ra in ras:
        for dec in decs:
            coord = SkyCoord(ra=ra, dec=dec, unit='deg', frame='icrs')
            try:
                res = SDSS.query_region(coord, radius=tile_radius_arcmin*u.arcmin, spectro=False)
                if res is not None and len(res) > 0:
                    all_results.append(res)
            except Exception as e:
                print(f"SDSS query error at RA={ra}, DEC={dec}: {e}")

    if all_results:
        combined = Table.vstack(all_results)
        return combined
    else:
        return Table()

def estimate_mass_for_lens(lens, radius_arcmin=3):
    """
    Query galaxies near lens and estimate total stellar mass.
    """
    center_coord = SkyCoord(ra=lens['RA [deg]'], dec=lens['DEC [deg]'], unit='deg')
    total_radius_deg = radius_arcmin / 60.0
    result_table = query_sdss_tiled(center_coord, total_radius_deg, tile_radius_arcmin=radius_arcmin)

    if len(result_table) == 0:
        print(f"  No galaxies found for lens {lens['name']}")
        return 0, 0

    # If stellar mass available, sum; else approximate by number of galaxies * 1e11 Msun
    if 'stellarmass' in result_table.colnames:
        masses = result_table['stellarmass']
        total_mass = np.nansum(masses)
    else:
        total_mass = len(result_table) * 1e11

    sigma_z = np.std(result_table['z']) if 'z' in result_table.colnames else 0

    return total_mass, sigma_z

# --- Load catalog and filter lenses ---
print("📦 Loading full catalog from lenscat package...")
df = lenscat.df  # lenscat.df contains the loaded catalog DataFrame
print(f"✅ Loaded {len(df):,} lenses")

# Rename columns to standard names for clarity (optional)
df.rename(columns={'RA [deg]': 'RA', 'DEC [deg]': 'DEC'}, inplace=True)

# Filter confident or probable lenses with numeric redshift
df_filtered = df[(df['grading'].str.lower().isin(['confident', 'probable'])) & (df['zlens'].apply(lambda x: isinstance(x, (int, float))))]

print(f"✅ Filtered lenses (confident/probable with numeric redshift): {len(df_filtered):,}")

N = 500  # Number of lenses to process
results = []

print(f"\nProcessing {N} lenses for stellar mass estimation...\n")

for i, lens in df_filtered.iloc[:N].iterrows():
    print(f"Querying lens {lens['name']} at RA={lens['RA']:.4f}, DEC={lens['DEC']:.4f}")
    mass, sigma = estimate_mass_for_lens(lens, radius_arcmin=3)
    print(f"  Found {mass:.2e} M☉ total stellar mass")
    results.append({
        "lens_id": lens['name'],
        "ra": lens['RA'],
        "dec": lens['DEC'],
        "redshift": lens['zlens'],
        "total_stellar_mass_Msun": mass,
        "mass_sigma": sigma
    })

df_results = pd.DataFrame(results)
df_results.to_csv("lens_stellar_mass_lenscat_500.csv", index=False)

print("\nDone. Results saved to 'lens_stellar_mass_lenscat_500.csv'")

import lenscat
from astroquery.sdss import SDSS
from astropy.coordinates import SkyCoord
from astropy.table import Table
import astropy.units as u
import pandas as pd
import numpy as np

def query_sdss_tiled(center_coord, total_radius_deg, tile_radius_arcmin=15):
    """
    Query SDSS in tiles around center_coord to cover the total_radius_deg area.
    """
    tile_radius_deg = tile_radius_arcmin / 60.0
    all_results = []

    # Calculate number of tiles to cover the circle of total_radius_deg radius
    steps = int(np.ceil((total_radius_deg*2) / (tile_radius_deg*2)))
    ras = center_coord.ra.deg + np.linspace(-total_radius_deg, total_radius_deg, steps)
    decs = center_coord.dec.deg + np.linspace(-total_radius_deg, total_radius_deg, steps)

    for ra in ras:
        for dec in decs:
            coord = SkyCoord(ra=ra, dec=dec, unit='deg', frame='icrs')
            try:
                res = SDSS.query_region(coord, radius=tile_radius_arcmin*u.arcmin, spectro=False)
                if res is not None and len(res) > 0:
                    all_results.append(res)
            except Exception as e:
                print(f"SDSS query error at RA={ra}, DEC={dec}: {e}")

    if all_results:
        combined = Table.vstack(all_results)
        return combined
    else:
        return Table()

def estimate_mass_for_lens(lens, radius_arcmin=3):
    """
    Query galaxies near lens and estimate total stellar mass.
    """
    center_coord = SkyCoord(ra=lens['RA [deg]'], dec=lens['DEC [deg]'], unit='deg')
    total_radius_deg = radius_arcmin / 60.0
    result_table = query_sdss_tiled(center_coord, total_radius_deg, tile_radius_arcmin=radius_arcmin)

    if len(result_table) == 0:
        print(f"  No galaxies found for lens {lens['name']}")
        return 0, 0

    # If stellar mass available, sum; else approximate by number of galaxies * 1e11 Msun
    if 'stellarmass' in result_table.colnames:
        masses = result_table['stellarmass']
        total_mass = np.nansum(masses)
    else:
        total_mass = len(result_table) * 1e11

    sigma_z = np.std(result_table['z']) if 'z' in result_table.colnames else 0

    return total_mass, sigma_z

# --- Load catalog and filter lenses ---
print("📦 Loading full catalog from lenscat package...")
df = lenscat.df  # lenscat.df contains the loaded catalog DataFrame
print(f"✅ Loaded {len(df):,} lenses")

# Rename columns to standard names for clarity (optional)
df.rename(columns={'RA [deg]': 'RA', 'DEC [deg]': 'DEC'}, inplace=True)

# Filter confident or probable lenses with numeric redshift
df_filtered = df[(df['grading'].str.lower().isin(['confident', 'probable'])) & (df['zlens'].apply(lambda x: isinstance(x, (int, float))))]

print(f"✅ Filtered lenses (confident/probable with numeric redshift): {len(df_filtered):,}")

N = 500  # Number of lenses to process
results = []

print(f"\nProcessing {N} lenses for stellar mass estimation...\n")

for i, lens in df_filtered.iloc[:N].iterrows():
    print(f"Querying lens {lens['name']} at RA={lens['RA']:.4f}, DEC={lens['DEC']:.4f}")
    mass, sigma = estimate_mass_for_lens(lens, radius_arcmin=3)
    print(f"  Found {mass:.2e} M☉ total stellar mass")
    results.append({
        "lens_id": lens['name'],
        "ra": lens['RA'],
        "dec": lens['DEC'],
        "redshift": lens['zlens'],
        "total_stellar_mass_Msun": mass,
        "mass_sigma": sigma
    })

df_results = pd.DataFrame(results)
df_results.to_csv("lens_stellar_mass_lenscat_500.csv", index=False)

print("\nDone. Results saved to 'lens_stellar_mass_lenscat_500.csv'")

import os
import pandas as pd
import numpy as np
from astropy.coordinates import SkyCoord, Angle
from astropy import units as u
from astroquery.simbad import Simbad
from astropy.table import Table
from google.colab import drive
import random
import time
from tqdm import tqdm

# 🔗 Mount Google Drive
drive.mount('/content/drive')

# 📁 Output folder
save_dir = "/content/drive/MyDrive/bh_lens_analysis_500"
os.makedirs(save_dir, exist_ok=True)

# 🧹 Clean SIMBAD fields
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.remove_votable_fields('coordinates')
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

# 🎯 BH-like object types
bh_types = ['BH', 'BH?','BHC','BLAZAR','BLLac','AGN','QSO','XRB','LMXB','HMXB']

# 📦 Use already loaded catalog
# If not loaded yet, add this to load from CSV:
# df = pd.read_csv('/content/drive/MyDrive/path_to_lens_catalog.csv')

# ✅ Filter confident lenses with redshift
filtered = df[
    (df['grading'].str.lower() == 'confident') &
    (df['zlens'].notnull())
].copy()

print(f"🎯 Filtered to

import os
import pandas as pd
import numpy as np
from astropy.coordinates import SkyCoord, Angle
from astropy import units as u
from astroquery.simbad import Simbad
from astropy.table import Table
from google.colab import drive
import random
import time
from tqdm import tqdm

# 🔗 Mount Google Drive
drive.mount('/content/drive')

# 📁 Output folder
save_dir = "/content/drive/MyDrive/bh_lens_analysis_500"
os.makedirs(save_dir, exist_ok=True)

# 🧹 Clean SIMBAD fields
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.remove_votable_fields('coordinates')
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

# 🎯 BH-like object types
bh_types = ['BH', 'BH?','BHC','BLAZAR','BLLac','AGN','QSO','XRB','LMXB','HMXB']

# 📦 Use already loaded catalog
# If not loaded yet, add this to load from CSV:
# df = pd.read_csv('/content/drive/MyDrive/path_to_lens_catalog.csv')

# ✅ Filter confident lenses with redshift
filtered = df[
    (df['grading'].str.lower() == 'confident') &
    (df['zlens'].notnull())
].copy()

print(f"🎯 Filtered to {len(filtered)} confident lenses with redshift.")

# 🎯 Randomly select 500
filtered = filtered.sample(n=500, random_state=42).reset_index(drop=True)

# 🌌 Store coordinates of all lenses for random position exclusion
lens_coords_all = SkyCoord(ra=df['RA [deg]'].values * u.deg,
                           dec=df['DEC [deg]'].values * u.deg)

# 📝 Result containers
results = []

# 🔁 Process in batches
batch_size = 50
for i in range(0, len(filtered), batch_size):
    batch = filtered.iloc[i:i+batch_size]
    batch_results = []

    print(f"\n🔎 Processing batch {i//batch_size + 1} ({len(batch)} lenses)...")

    for idx, row in tqdm(batch.iterrows(), total=len(batch)):
        lens_coord = SkyCoord(ra=row['RA [deg]'] * u.deg,
                              dec=row['DEC [deg]'] * u.deg)

        # SIMBAD query for real field
        try:
            result = custom_simbad.query_region(lens_coord, radius=Angle(20, "arcmin"))
            if result:
                result = result.to_pandas()
                bh_count = result['OTYPE'].isin(bh_types).sum()
            else:
                bh_count = 0
        except Exception as e:
            print(f"⚠️ SIMBAD query failed for {row['name']}: {e}")
            bh_count = np.nan

        # Generate a valid random position at least 1° from all lenses
        max_attempts = 100
        for attempt in range(max_attempts):
            rand_ra = np.random.uniform(0, 360)
            rand_dec = np.degrees(np.arcsin(np.random.uniform(-1, 1)))  # Uniform on sphere
            rand_coord = SkyCoord(ra=rand_ra * u.deg, dec=rand_dec * u.deg)
            if np.min(rand_coord.separation(lens_coords_all)) > Angle(1, "deg"):
                break

        # SIMBAD query for random field
        try:
            rand_result = custom_simbad.query_region(rand_coord, radius=Angle(20, "arcmin"))
            if rand_result:
                rand_result = rand_result.to_pandas()
                rand_bh_count = rand_result['OTYPE'].isin(bh_types).sum()
            else:
                rand_bh_count = 0
        except Exception as e:
            print(f"⚠️ SIMBAD query failed for random point: {e}")
            rand_bh_count = np.nan

        # Record
        batch_results.append({
            'lens_name': row['name'],
            'ra': row['RA [deg]'],
            'dec': row['DEC [deg]'],
            'zlens': row['zlens'],
            'bh_count_20arcmin': bh_count,
            'random_ra': rand_ra,
            'random_dec': rand_dec,
            'random_bh_count_20arcmin': rand_bh_count
        })

        time.sleep(1.5)  # Respect SIMBAD rate limits

    # Save batch
    batch_df = pd.DataFrame(batch_results)
    batch_path = os.path.join(save_dir, f"batch_{i//batch_size + 1}.csv")
    batch_df.to_csv(batch_path, index=False)
    print(f"💾 Saved batch {i//batch_size + 1} to {batch_path}")

    results.extend(batch_results)

print("✅ All 500 lenses processed.")

# Install required packages
!pip install -q lenscat astroquery

# Imports
from lenscat import load_lens_catalog
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
from astropy.table import Table
from astropy import units as u
import numpy as np
import pandas as pd
import random
from tqdm import tqdm
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Load lens catalog
lens_df = load_lens_catalog()
lens_df = lens_df[lens_df['grading'].isin(['confident', 'probable'])]
lens_df = lens_df.dropna(subset=['zlens'])
lens_df = lens_df.reset_index(drop=True)
lens_df = lens_df.head(500)  # Run for first 500 lenses

print(f"✅ Loaded {len(lens_df)} lenses")

# Setup SIMBAD
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

# Object types to search
bh_keywords = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']
radius = 20 * u.arcmin

# Function to query SIMBAD and count BH-like objects
def count_bh_objects(ra, dec):
    coord = SkyCoord(ra=ra*u.deg, dec=dec*u.deg, frame='icrs')
    try:
        result = custom_simbad.query_region(coord, radius=radius)
        if result is None:
            return 0
        return sum(otype.decode('utf-8') in bh_keywords for otype in result['OTYPE'])
    except Exception as e:
        print(f"❌ SIMBAD error at RA={ra}, DEC={dec}: {e}")
        return -1

# Main loop
results = []
for i, row in tqdm(lens_df.iterrows(), total=len(lens_df)):
    name = row['name']
    ra = row['RA [deg]']
    dec = row['DEC [deg]']
    bh_count = count_bh_objects(ra, dec)
    results.append({'name': name, 'RA': ra, 'DEC': dec, 'BH_count': bh_count})

# Save to Drive
output_df = pd.DataFrame(results)
output_path = "/content/drive/MyDrive/lens_bh_counts_500.csv"
output_df.to_csv(output_path, index=False)
print(f"✅ Saved results to: {output_path}")

# Remove broken version
!pip uninstall -y lenscat

# Install directly from GitHub (correct version)
!pip install git+https://github.com/coljac/lenscat.git

# Remove broken version
!pip uninstall -y lenscat

# Install directly from GitHub (correct version)
!pip install git+https://github.com/coljac/lenscat.git

!pip install astroquery

from astroquery.sdss import SDSS
from astropy.coordinates import SkyCoord, Angle
import astropy.units as u
from astropy.table import vstack
import numpy as np
import pandas as pd
from astropy.cosmology import Planck18 as cosmo

def query_sdss_tiled(center_coord, total_radius_deg=1.0, tile_radius_arcmin=3.0):
    tile_radius_deg = tile_radius_arcmin / 60.0
    n_tiles_side = int(np.ceil((2 * total_radius_deg) / tile_radius_deg))
    all_results = []

    ra_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)
    dec_offsets = np.linspace(-total_radius_deg, total_radius_deg, n_tiles_side)

    for ra_off in ra_offsets:
        for dec_off in dec_offsets:
            tile_center = SkyCoord(ra=center_coord.ra.deg + ra_off,
                                   dec=center_coord.dec.deg + dec_off,
                                   unit='deg')
            try:
                result = SDSS.query_region(tile_center,
                                          radius=Angle(tile_radius_arcmin, u.arcmin),
                                          spectro=False,
                                          photoobj_fields=['ra', 'dec', 'type'])
                if result is not None:
                    all_results.append(result)
            except Exception as e:
                print(f"Error querying tile at RA={tile_center.ra.deg}, DEC={tile_center.dec.deg}: {e}")

    if all_results:
        combined = vstack(all_results)
        coords_all = SkyCoord(ra=combined['ra'], dec=combined['dec'], unit='deg')
        mask = coords_all.separation(center_coord) <= Angle(total_radius_deg, u.deg)
        filtered = combined[mask]
        return filtered
    else:
        return None

def sdss_type_to_mass(sdss_type):
    if sdss_type == 6:
        return 5e10  # typical galaxy mass (solar masses)
    else:
        return 0

def surface_mass_density(total_mass, redshift, radius_arcmin=3):
    if redshift is None or np.isnan(redshift):
        return np.nan
    theta_rad = radius_arcmin * (np.pi / 180) / 60
    d_a = cosmo.angular_diameter_distance(redshift).to(u.Mpc).value
    radius_mpc = theta_rad * d_a
    area = np.pi * radius_mpc**2
    return total_mass / area

def estimate_mass_for_lens(lens, radius_arcmin=3):
    center_coord = SkyCoord(ra=lens['ra'], dec=lens['dec'], unit='deg')
    galaxies = query_sdss_tiled(center_coord, total_radius_deg=radius_arcmin/60, tile_radius_arcmin=3)
    if galaxies is None or len(galaxies) == 0:
        print(f"No galaxies found near lens {lens['lens_id']}")
        return 0.0, 0.0

    total_mass = 0.0
    for gal in galaxies:
        gal_type = gal['type']
        mass = sdss_type_to_mass(gal_type)
        total_mass += mass

    sigma = surface_mass_density(total_mass, lens['z'], radius_arcmin)
    return total_mass, sigma

# === Replace this lenses DataFrame with your actual 100 lenses ===
lenses = pd.DataFrame({
    'lens_id': [f'L{i+1}' for i in range(100)],
    'ra': [/* your 100 RA values here */],
    'dec': [/* your 100 DEC values here */],
    'z': [/* your 100 redshifts here */]
})

results = []
for _, lens in lenses.iterrows():
    print(f"Processing lens {lens['lens_id']} at RA={lens['ra']:.4f}, DEC={lens['dec']:.4f}")
    mass, sigma = estimate_mass_for_lens(lens, radius_arcmin=3)
    print(f"  Total mass: {mass:.2e} M☉, Surface density: {sigma:.2e} M☉/Mpc²")
    results.append({'lens_id': lens['lens_id'], 'total_mass_Msun': mass, 'mass_surface_density_Msun_per_Mpc2': sigma})

results_df = pd.DataFrame(results)
results_df.to_csv('lens_mass_100_lenses.csv', index=False)
print("Done! Saved results to lens_mass_100_lenses.csv")

import numpy as np
import pandas as pd
from astropy.cosmology import Planck18 as cosmo
from astroquery.sdss import SDSS
from astropy.coordinates import SkyCoord, Angle
import astropy.units as u

def rmag_to_mass(rmag, gmag, redshift):
    if np.isnan(rmag) or np.isnan(gmag) or np.isnan(redshift):
        return np.nan
    color = gmag - rmag
    M_L_r = 10**(-0.306 + 1.097 * color)  # mass-to-light ratio from Bell+03
    d_l = cosmo.luminosity_distance(redshift).to('pc').value
    M_r_sun = 4.65  # Sun absolute mag in r band
    M_r = rmag - 5 * (np.log10(d_l) - 1)
    L_r = 10**(-0.4 * (M_r - M_r_sun))
    return L_r * M_L_r

def surface_mass_density(total_mass, redshift, radius_arcmin=3):
    if redshift is None or np.isnan(redshift):
        return np.nan
    theta_rad = radius_arcmin * (np.pi / 180) / 60
    d_a = cosmo.angular_diameter_distance(redshift).to(u.Mpc).value
    radius_mpc = theta_rad * d_a
    area = np.pi * radius_mpc**2
    return total_mass / area

def query_sdss_tiled(center_coord, tile_radius_arcmin=3.0):
    """
    Query SDSS in one tile of radius tile_radius_arcmin centered on center_coord,
    returning an Astropy Table with ra, dec, type, modelMag_g, modelMag_r.
    """
    try:
        result = SDSS.query_region(
            center_coord,
            radius=Angle(tile_radius_arcmin, u.arcmin),
            spectro=False,
            photoobj_fields=['ra', 'dec', 'type', 'modelMag_g', 'modelMag_r']
        )
    except Exception as e:
        print(f"SDSS query failed at RA={center_coord.ra.deg}, DEC={center_coord.dec.deg}: {e}")
        return None
    return result

def process_lenses(lenses_df, radius_arcmin=3):
    results = []
    for idx, lens in lenses_df.iterrows():
        print(f"Querying lens {lens['lens_id']} at RA={lens['ra']:.4f}, DEC={lens['dec']:.4f}")
        center = SkyCoord(ra=lens['ra'], dec=lens['dec'], unit='deg')
        catalog = query_sdss_tiled(center, tile_radius_arcmin=radius_arcmin)
        if catalog is None or len(catalog) == 0:
            print(f"  No objects found for lens {lens['lens_id']}")
            results.append({'lens_id': lens['lens_id'], 'total_mass_Msun': 0.0, 'mass_surface_density_Msun_per_Mpc2': 0.0})
            continue

        total_mass = 0.0
        for obj in catalog:
            if obj['type'] != 3:  # type 3 = galaxy
                continue
            rmag = obj['modelMag_r']
            gmag = obj['modelMag_g']
            mass = rmag_to_mass(rmag, gmag, lens['z'])
            if not np.isnan(mass):
                total_mass += mass
        sigma = surface_mass_density(total_mass, lens['z'], radius_arcmin)
        results.append({'lens_id': lens['lens_id'], 'total_mass_Msun': total_mass, 'mass_surface_density_Msun_per_Mpc2': sigma})
        print(f"  Total mass: {total_mass:.2e} M☉, Surface density: {sigma:.2e} M☉/Mpc²")
    return pd.DataFrame(results)

# Example lenses DataFrame: replace with your 100 lenses data
lenses = pd.DataFrame({
    'lens_id': [f'L{i+1}' for i in range(2)],
    'ra': [150.1, 149.9],
    'dec': [2.3, 2.5],
    'z': [0.3, 0.45]
})

# Run the pipeline
mass_results = process_lenses(lenses, radius_arcmin=3)

import numpy as np
import pandas as pd
from astropy.cosmology import Planck18 as cosmo
from astroquery.sdss import SDSS
from astropy.coordinates import SkyCoord, Angle
import astropy.units as u

def rmag_to_mass(rmag, gmag, redshift):
    if np.isnan(rmag) or np.isnan(gmag) or np.isnan(redshift):
        return np.nan
    color = gmag - rmag
    M_L_r = 10**(-0.306 + 1.097 * color)  # mass-to-light ratio from Bell+03
    d_l = cosmo.luminosity_distance(redshift).to('pc').value
    M_r_sun = 4.65  # Sun absolute mag in r band
    M_r = rmag - 5 * (np.log10(d_l) - 1)
    L_r = 10**(-0.4 * (M_r - M_r_sun))
    return L_r * M_L_r

def surface_mass_density(total_mass, redshift, radius_arcmin=3):
    if redshift is None or np.isnan(redshift):
        return np.nan
    theta_rad = radius_arcmin * (np.pi / 180) / 60
    d_a = cosmo.angular_diameter_distance(redshift).to(u.Mpc).value
    radius_mpc = theta_rad * d_a
    area = np.pi * radius_mpc**2
    return total_mass / area

def query_sdss_tiled(center_coord, tile_radius_arcmin=3.0):
    """
    Query SDSS in one tile of radius tile_radius_arcmin centered on center_coord,
    returning an Astropy Table with ra, dec, class, modelMag_g, modelMag_r.
    """
    try:
        result = SDSS.query_region(
            center_coord,
            radius=Angle(tile_radius_arcmin, u.arcmin),
            spectro=False,
            photoobj_fields=['ra', 'dec', 'class', 'modelMag_g', 'modelMag_r']
        )
    except Exception as e:
        print(f"SDSS query failed at RA={center_coord.ra.deg}, DEC={center_coord.dec.deg}: {e}")
        return None
    if result is not None:
        print("Columns returned:", result.colnames)
    return result

def process_lenses(lenses_df, radius_arcmin=3):
    results = []
    for idx, lens in lenses_df.iterrows():
        print(f"Querying lens {lens['lens_id']} at RA={lens['ra']:.4f}, DEC={lens['dec']:.4f}")
        center = SkyCoord(ra=lens['ra'], dec=lens['dec'], unit='deg')
        catalog = query_sdss_tiled(center, tile_radius_arcmin=radius_arcmin)
        if catalog is None or len(catalog) == 0:
            print(f"  No objects found for lens {lens['lens_id']}")
            results.append({'lens_id': lens['lens_id'], 'total_mass_Msun': 0.0, 'mass_surface_density_Msun_per_Mpc2': 0.0})
            continue

        total_mass = 0.0
        for obj in catalog:
            if obj['class'] != 'GALAXY':
                continue
            rmag = obj['modelMag_r']
            gmag = obj['modelMag_g']
            mass = rmag_to_mass(rmag, gmag, lens['z'])
            if not np.isnan(mass):
                total_mass += mass
        sigma = surface_mass_density(total_mass, lens['z'], radius_arcmin)
        results.append({'lens_id': lens['lens_id'], 'total_mass_Msun': total_mass, 'mass_surface_density_Msun_per_Mpc2': sigma})
        print(f"  Total mass: {total_mass:.2e} M☉, Surface density: {sigma:.2e} M☉/Mpc²")
    return pd.DataFrame(results)

# Example lenses DataFrame: replace with your lenses data
lenses = pd.DataFrame({
    'lens_id': ['L1', 'L2'],
    'ra': [150.1, 149.9],
    'dec': [2.3, 2.5],
    'z': [0.3, 0.45]
})

# Run the pipeline
mass_results = process_lenses(lenses, radius_arcmin=3)
mass_results.to_csv('lens_stellar_mass_summary.csv', index=False)
print("Done. Results saved to lens_stellar_mass_summary.csv")
print(mass_results)

from astroquery.sdss import SDSS
from astropy.coordinates import SkyCoord
import astropy.units as u

coord = SkyCoord(ra=150.1, dec=2.3, unit='deg')
result = SDSS.query_region(coord, radius=0.05*u.deg, photoobj_fields=['ra', 'dec', 'class'])

print(result)
print(result.colnames)

from astroquery.sdss import SDSS
from astropy.coordinates import SkyCoord
import astropy.units as u

coord = SkyCoord(ra=150.1, dec=2.3, unit='deg')
radius_deg = 0.05

query = f"""
SELECT TOP 100 ra, dec, class, modelMag_g, modelMag_r
FROM PhotoObj
WHERE
    dbo.fGetNearbyObjEq({coord.ra.degree}, {coord.dec.degree}, {radius_deg*60}) > 0
"""

try:
    result = SDSS.query_sql(query)
    print(result)
except Exception as e:
    print(f"Error running TAP query: {e}")

from astroquery.sdss import SDSS
from astropy.coordinates import SkyCoord
import astropy.units as u

coord = SkyCoord(ra=150.1, dec=2.3, unit='deg')
radius_deg = 0.05  # 3 arcmin = 0.05 deg

query = f"""
SELECT TOP 100 ra, dec, class, modelMag_g, modelMag_r
FROM PhotoObj
WHERE 1=CONTAINS(
    POINT('ICRS', ra, dec),
    CIRCLE('ICRS', {coord.ra.degree}, {coord.dec.degree}, {radius_deg})
)
"""

try:
    result = SDSS.query_sql(query)
    print(result)
except Exception as e:
    print(f"Error running TAP query: {e}")

from astroquery.sdss import SDSS

try:
    result = SDSS.query_sql("SELECT TOP 10 ra, dec FROM PhotoObj")
    print(result)
except Exception as e:
    print(f"Error running simple TAP query: {e}")

import numpy as np
import pandas as pd
from astropy.cosmology import Planck18 as cosmo
from astroquery.ned import Ned
from astropy.coordinates import SkyCoord
import astropy.units as u

def kmag_to_mass(kmag, redshift):
    # Convert K-band apparent mag to stellar mass (Msun)
    if np.isnan(kmag) or np.isnan(redshift):
        return np.nan
    d_l = cosmo.luminosity_distance(redshift).to('pc').value
    M_K_sun = 3.28  # Absolute K mag of Sun
    M_K = kmag - 5 * (np.log10(d_l) - 1)
    L_K = 10**(-0.4 * (M_K - M_K_sun))
    M_L_ratio = 0.8  # typical M/L in K-band
    return L_K * M_L_ratio

def surface_mass_density(total_mass, redshift, radius_arcmin=3):
    if redshift is None or np.isnan(redshift):
        return np.nan
    theta_rad = radius_arcmin * (np.pi / 180) / 60
    d_a = cosmo.angular_diameter_distance(redshift).to(u.Mpc).value
    radius_mpc = theta_rad * d_a
    area = np.pi * radius_mpc**2
    return total_mass / area

def query_ned_galaxies(lens_coord, radius_arcmin=3):
    try:
        result_table = Ned.query_region(lens_coord, radius=radius_arcmin * u.arcmin)
    except Exception as e:
        print(f"NED query failed at {lens_coord.to_string('hmsdms')}: {e}")
        return None
    return result_table

def process_lenses(lenses_df, radius_arcmin=3):
    results = []
    for idx, lens in lenses_df.iterrows():
        print(f"Querying lens {lens['lens_id']} at RA={lens['ra']:.4f}, DEC={lens['dec']:.4f}")
        center = SkyCoord(ra=lens['ra'], dec=lens['dec'], unit='deg')
        catalog = query_ned_galaxies(center, radius_arcmin=radius_arcmin)
        if catalog is None or len(catalog) == 0:
            print(f"  No galaxies found for lens {lens['lens_id']}")
            results.append({'lens_id': lens['lens_id'], 'total_mass_Msun': 0.0, 'mass_surface_density_Msun_per_Mpc2': 0.0})
            continue

        total_mass = 0.0
        # Loop over all objects returned by NED near lens
        for obj in catalog:
            # Try to get K-band mag if present
            if 'Kmag' in catalog.colnames:
                kmag = obj['Kmag']
                if kmag is not None and not np.isnan(kmag):
                    mass = kmag_to_mass(kmag, lens['z'])
                    if not np.isnan(mass):
                        total_mass += mass

        sigma = surface_mass_density(total_mass, lens['z'], radius_arcmin)
        print(f"  Total stellar mass: {total_mass:.2e} M☉, Surface density: {sigma:.2e} M☉/Mpc²")
        results.append({'lens_id': lens['lens_id'], 'total_mass_Msun': total_mass, 'mass_surface_density_Msun_per_Mpc2': sigma})
    return pd.DataFrame(results)

# Replace below with your actual lenses DataFrame (lens_id, ra, dec, z)
lenses = pd.DataFrame({
    'lens_id': ['L1', 'L2'],
    'ra': [150.1, 149.9],
    'dec': [2.3, 2.5],
    'z': [0.3, 0.45]
})

mass_results = process_lenses(lenses, radius_arcmin=3)
mass_results.to_csv('lens_stellar_mass_ned.csv', index=False)
print("Done. Results saved to lens_stellar_mass_ned.csv")
print(mass_results)

import numpy as np
import pandas as pd
from astropy.cosmology import Planck18 as cosmo
from astroquery.ipac.ned import Ned
from astropy.coordinates import SkyCoord
import astropy.units as u

def kmag_to_mass(kmag, redshift):
    if np.isnan(kmag) or np.isnan(redshift):
        return np.nan
    d_l = cosmo.luminosity_distance(redshift).to('pc').value
    M_K_sun = 3.28
    M_K = kmag - 5 * (np.log10(d_l) - 1)
    L_K = 10**(-0.4 * (M_K - M_K_sun))
    M_L_ratio = 0.8
    return L_K * M_L_ratio

def type_to_mass(gal_type):
    if gal_type is None:
        return np.nan
    gal_type = gal_type.lower()
    if 'elliptical' in gal_type or gal_type == 'e':
        return 1e11
    elif 'spiral' in gal_type or gal_type.startswith('s'):
        return 5e10
    elif 'lenticular' in gal_type or 's0' in gal_type:
        return 7e10
    else:
        return np.nan

def surface_mass_density(total_mass, redshift, radius_arcmin=3):
    if redshift is None or np.isnan(redshift):
        return np.nan
    theta_rad = radius_arcmin * (np.pi / 180) / 60
    d_a = cosmo.angular_diameter_distance(redshift).to(u.Mpc).value
    radius_mpc = theta_rad * d_a
    area = np.pi * radius_mpc**2
    return total_mass / area

def query_ned_galaxies(lens_coord, radius_arcmin=3):
    try:
        result_table = Ned.query_region(lens_coord, radius=radius_arcmin * u.arcmin)
    except Exception as e:
        print(f"NED query failed at {lens_coord.to_string('hmsdms')}: {e}")
        return None
    return result_table

def process_lenses(lenses_df, radius_arcmin=3):
    results = []
    for idx, lens in lenses_df.iterrows():
        print(f"Querying lens {lens['lens_id']} at RA={lens['ra']:.4f}, DEC={lens['dec']:.4f}")
        center = SkyCoord(ra=lens['ra'], dec=lens['dec'], unit='deg')
        catalog = query_ned_galaxies(center, radius_arcmin=radius_arcmin)
        if catalog is None or len(catalog) == 0:
            print(f"  No galaxies found for lens {lens['lens_id']}")
            results.append({'lens_id': lens['lens_id'], 'total_mass_Msun': 0.0, 'mass_surface_density_Msun_per_Mpc2': 0.0})
            continue

        total_mass = 0.0
        for obj in catalog:
            mass = np.nan
            if 'Kmag' in catalog.colnames and obj['Kmag'] is not None and not np.isnan(obj['Kmag']):
                mass = kmag_to_mass(obj['Kmag'], lens['z'])
            elif 'Type' in catalog.colnames and obj['Type'] is not None:
                mass = type_to_mass(obj['Type'])
            if not np.isnan(mass):
                total_mass += mass

        sigma = surface_mass_density(total_mass, lens['z'], radius_arcmin)
        print(f"  Total stellar mass: {total_mass:.2e} M☉, Surface density: {sigma:.2e} M☉/Mpc²")
        results.append({'lens_id': lens['lens_id'], 'total_mass_Msun': total_mass, 'mass_surface_density_Msun_per_Mpc2': sigma})
    return pd.DataFrame(results)

# Replace with your actual lenses (lens_id, ra, dec, z)
lenses = pd.DataFrame({
    'lens_id': ['L1', 'L2'],
    'ra': [150.1, 149.9],
    'dec': [2.3, 2.5],
    'z': [0.3, 0.45]
})

mass_results = process_lenses(lenses, radius_arcmin=3)
mass_results.to_csv('lens_stellar_mass_ned_fallback.csv', index=False)
print("Done. Results saved to lens_stellar_mass_ned_fallback.csv")
print(mass_results)

import numpy as np
import pandas as pd
from astropy.cosmology import Planck18 as cosmo
from astropy.coordinates import SkyCoord
import astropy.units as u

# Typical mass fallback if no photometry (in solar masses)
TYPICAL_MASS = 5e10

def rmag_to_mass(rmag, gmag, redshift):
    if np.isnan(rmag) or np.isnan(gmag) or np.isnan(redshift):
        return np.nan
    color = gmag - rmag
    M_L_r = 10**(-0.306 + 1.097 * color)
    d_l = cosmo.luminosity_distance(redshift).to('pc').value
    M_r_sun = 4.65
    M_r = rmag - 5 * (np.log10(d_l) - 1)
    L_r = 10**(-0.4 * (M_r - M_r_sun))
    return L_r * M_L_r

def surface_mass_density(total_mass, redshift, radius_arcmin=3):
    if redshift is None or np.isnan(redshift):
        return np.nan
    theta_rad = radius_arcmin * (np.pi / 180) / 60
    d_a = cosmo.angular_diameter_distance(redshift).to(u.Mpc).value
    radius_mpc = theta_rad * d_a
    area = np.pi * radius_mpc**2
    return total_mass / area

# Example: load your lens catalog with RA, DEC, redshift
# Must have columns: lens_id, ra, dec, z
lenses = pd.read_csv('your_lens_catalog.csv')

# Load your raw SDSS galaxy catalog from your previous queries
# Must have columns at least: ra, dec, type, modelMag_g, modelMag_r
galaxies = pd.read_csv('your_galaxies_raw.csv')

# Convert lens and galaxy coords to SkyCoord
lens_coords = SkyCoord(ra=lenses['ra'].values, dec=lenses['dec'].values, unit='deg')
gal_coords = SkyCoord(ra=galaxies['ra'].values, dec=galaxies['dec'].values, unit='deg')

# For each galaxy, find nearest lens and separation
idx, sep2d, _ = gal_coords.match_to_catalog_sky(lens_coords)

# Assign galaxy to lens only if within 3 arcmin (0.05 deg)
max_sep = 3 * u.arcmin
assigned_lens_ids = []
for i, s in enumerate(sep2d):
    if s <= max_sep:
        assigned_lens_ids.append(lenses.iloc[idx[i]]['lens_id'])
    else:
        assigned_lens_ids.append(None)
galaxies['lens_id'] = assigned_lens_ids

# Filter galaxies that belong to any lens
galaxies = galaxies[galaxies['lens_id'].notnull()]

# Merge to get lens redshift per galaxy
galaxies = galaxies.merge(lenses[['lens_id', 'z']], on='lens_id', how='left')

# Compute mass per galaxy
def compute_mass(row):
    if row['type'] != 3:  # SDSS type 3 = galaxy
        return 0
    if not np.isnan(row['modelMag_r']) and not np.isnan(row['modelMag_g']) and not np.isnan(row['z']):
        mass = rmag_to_mass(row['modelMag_r'], row['modelMag_g'], row['z'])
        if not np.isnan(mass):
            return mass
    # fallback typical mass
    return TYPICAL_MASS

galaxies['stellar_mass'] = galaxies.apply(compute_mass, axis=1)

# Sum masses per lens
mass_sum = galaxies.groupby('lens_id')['stellar_mass'].sum().reset_index()

# Add redshift back
mass_sum = mass_sum.merge(lenses[['lens_id', 'z']], on='lens_id', how='left')

# Compute surface density
mass_sum['surface_mass_density_Msun_per_Mpc2'] = mass_sum.apply(
    lambda row: surface_mass_density(row['stellar_mass'], row['z']), axis=1)

# Save results
mass_sum.to_csv('lens_total_stellar_mass.csv', index=False)

print("Done! Summary saved to lens_total_stellar_mass.csv")
print(mass_sum)

!pip install astroquery
