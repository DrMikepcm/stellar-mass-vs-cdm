# -*- coding: utf-8 -*-
"""notebooks/lens_stellarmass_distribution_analysis.ipyn

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1paIZY-L8CiorSXirfPqeptaSTwMFuVUg
"""

import pandas as pd

# Load the data being generated by the main script
df = pd.read_csv('lens_stellar_mass_lenscat_5_batches_100.csv')

# Define surface density categories (adjust thresholds as needed)
def classify_density(row):
    if row['stellar_mass'] == 0 or pd.isna(row['stellar_mass']):
        return 'zero'
    elif row['stellar_mass'] < 1e9:
        return 'low'
    elif row['stellar_mass'] < 1e10:
        return 'medium'
    else:
        return 'high'

df['density_class'] = df.apply(classify_density, axis=1)

# Count how many lenses fall into each class
class_counts = df['density_class'].value_counts()

print("Density classification counts:")
print(class_counts)

# Optional: save to file
class_counts.to_csv('lens_density_class_counts.csv')

import pandas as pd

# Load the file created by your main code
df = pd.read_csv('lens_stellar_mass_lenscat_5_batches_100.csv')

# Slice just the first 100 entries
df_first_100 = df.head(100)

# Show a preview
df_first_100.head()

import os

# List all CSV files you've written to the current directory
for file in os.listdir():
    if file.endswith('.csv'):
        print(file)

import os

# List all CSV files you've written to the current directory
for file in os.listdir():
    if file.endswith('.csv'):
        print(file)

import os
print(os.getcwd())

from google.colab import drive
drive.mount('/content/drive')

# Then check inside your Drive folder:
import os
for root, dirs, files in os.walk('/content/drive/MyDrive'):
    for file in files:
        if file.endswith('.csv'):
            print(os.path.join(root, file))

/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_progress.csv

import pandas as pd

# Load your saved progress file
file_path = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_progress.csv'
df = pd.read_csv(file_path)

# Preview the data
df.head()

import os

# Show all files in your Drive folder
for root, dirs, files in os.walk('/content/drive/MyDrive'):
    for name in files:
        if 'stellar_mass_progress' in name:
            print(os.path.join(root, name))

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

file_path = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_progress.csv'
df = pd.read_csv(file_path)

df.head()

import pandas as pd

# Load the full progress file with all processed results
file_path = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_progress.csv'
df = pd.read_csv(file_path)

# Optional: check how many total lenses were processed
print(f"Total lenses processed: {len(df)}")

# Define the stellar mass classification logic
def classify_mass(m):
    if pd.isna(m) or m == 0:
        return 'zero'
    elif m < 1e9:
        return 'low'
    elif m < 1e10:
        return 'medium'
    else:
        return 'high'

# Apply the classification
df['mass_class'] = df['stellar_mass'].apply(classify_mass)

# Count the number of lenses in each class
mass_counts = df['mass_class'].value_counts()

print("Stellar mass classification counts:")
print(mass_counts)

import pandas as pd
import matplotlib.pyplot as plt

# Load your saved results CSV (update path if needed)
file_path = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_progress.csv'
df = pd.read_csv(file_path)

print(f"Loaded {len(df)} lenses.")

# Show basic statistics of raw total stellar mass values
print("Stellar mass (Msun) stats:")
print(df['total_mass_Msun'].describe())

# Optional: plot histogram of total stellar mass
plt.hist(df['total_mass_Msun'], bins=30, color='skyblue', edgecolor='black')
plt.xlabel('Total Stellar Mass (Msun)')
plt.ylabel('Number of Lenses')
plt.title('Distribution of Raw Total Stellar Mass around Lenses')
plt.yscale('log')  # Because mass can vary widely, log scale helps visualization
plt.show()

# Similarly for surface mass density if you want:
print("\nSurface Mass Density (Msun/Mpc^2) stats:")
print(df['mass_surface_density_Msun_per_Mpc2'].describe())

plt.hist(df['mass_surface_density_Msun_per_Mpc2'], bins=30, color='salmon', edgecolor='black')
plt.xlabel('Surface Mass Density (Msun/Mpc^2)')
plt.ylabel('Number of Lenses')
plt.title('Distribution of Raw Surface Mass Density around Lenses')
plt.yscale('log')
plt.show()

import pandas as pd
import numpy as np
from astropy.cosmology import Planck18 as cosmo
import astropy.units as u
import matplotlib.pyplot as plt

# 1. Load your saved results CSV (update path if needed)
file_path = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_progress.csv'
df = pd.read_csv(file_path)
print(f"Loaded {len(df)} lenses.")

# 2. Define field radius in arcmin and convert to radians
radius_arcmin = 20
radius_deg = radius_arcmin / 60
radius_rad = np.deg2rad(radius_deg)

# 3. Function to compute surface mass density (Msun/kpc^2) for each lens
def compute_surface_density(row):
    z = row['redshift']
    total_mass = row['total_mass_Msun']
    if pd.isna(z) or total_mass == 0:
        return np.nan
    # Angular diameter distance in kpc
    d_a = cosmo.angular_diameter_distance(z).to(u.kpc).value
    # Physical radius in kpc
    R_kpc = radius_rad * d_a
    area_kpc2 = np.pi * R_kpc**2
    sigma = total_mass / area_kpc2
    return sigma

df['surface_density_Msun_per_kpc2'] = df.apply(compute_surface_density, axis=1)

# 4. Classify lenses by environment based on surface density thresholds
def classify_environment(sigma):
    if pd.isna(sigma) or sigma == 0:
        return 'zero_galaxies'
    elif sigma < 1e3:
        return 'field (low density)'
    elif sigma < 1e4:
        return 'group / cluster outskirts (medium density)'
    else:
        return 'cluster core (high density)'

df['environment_class'] = df['surface_density_Msun_per_kpc2'].apply(classify_environment)

# 5. Print counts of lenses in each environment
env_counts = df['environment_class'].value_counts()
print("\nLens counts by environment:")
print(env_counts)

# 6. Save enriched dataframe back to Drive
output_path = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_with_env.csv'
df.to_csv(output_path, index=False)
print(f"\nSaved updated results with surface densities and classifications to:\n{output_path}")

# 7. Plot distribution of total stellar mass
plt.figure(figsize=(10,4))
plt.hist(df['total_mass_Msun'].dropna(), bins=30, color='skyblue', edgecolor='black')
plt.xlabel('Total Stellar Mass (Msun)')
plt.ylabel('Number of Lenses')
plt.title('Distribution of Raw Total Stellar Mass around Lenses')
plt.yscale('log')
plt.grid(True, which='both', linestyle='--', alpha=0.5)
plt.show()

# 8. Plot distribution of surface mass density
plt.figure(figsize=(10,4))
plt.hist(df['surface_density_Msun_per_kpc2'].dropna(), bins=30, color='salmon', edgecolor='black')
plt.xlabel('Surface Mass Density (Msun/kpcÂ²)')
plt.ylabel('Number of Lenses')
plt.title('Distribution of Surface Mass Density around Lenses')
plt.yscale('log')
plt.grid(True, which='both', linestyle='--', alpha=0.5)
plt.show()

import pandas as pd

# Load your saved results CSV
file_path = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_progress.csv'
df = pd.read_csv(file_path)

print(f"Loaded {len(df)} lenses.")

# Define function to classify environment by astrophysical surface density cutoffs
def classify_environment(sigma):
    if pd.isna(sigma) or sigma == 0:
        return 'zero_galaxies'
    elif sigma < 1e3:
        return 'low_density'
    elif sigma < 1e4:
        return 'medium_density'
    else:
        return 'high_density'

# Apply classification
df['environment_class'] = df['mass_surface_density_Msun_per_Mpc2'].apply(classify_environment)

# Count number of lenses in each environment
counts = df['environment_class'].value_counts()
print("\nLens counts by environment:")
print(counts)

# Optional: save the classified data
df.to_csv('/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_classified.csv', index=False)
print("\nClassified data saved to 'lens_stellar_mass_classified.csv'")

df['surface_density_kpc2'] = df['mass_surface_density_Msun_per_Mpc2'] / 1e6
print(df['surface_density_kpc2'].describe())

import os
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Mount Google Drive if not already mounted
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Search for CSV files with "stellar_mass" in the filename inside your Drive
drive_root = '/content/drive/MyDrive/'

csv_files = []
for root, dirs, files in os.walk(drive_root):
    for f in files:
        if f.endswith('.csv') and 'stellar_mass' in f:
            full_path = os.path.join(root, f)
            csv_files.append((full_path, os.path.getmtime(full_path)))

# Sort files by modified time descending (newest first)
csv_files.sort(key=lambda x: x[1], reverse=True)

if len(csv_files) < 2:
    print("Found fewer than 2 'stellar_mass' CSV files in your Drive folder.")
else:
    print("Found these 'stellar_mass' CSV files (most recent 2):")
    for i, (path, mtime) in enumerate(csv_files[:2], 1):
        print(f"{i}: {path}")

    # Load the two most recent files
    old_file = csv_files[1][0]  # second newest
    new_file = csv_files[0][0]  # newest

    print(f"\nLoading old results from:\n{old_file}")
    df_old = pd.read_csv(old_file)
    print(f"Loaded {len(df_old)} entries")

    print(f"\nLoading new results from:\n{new_file}")
    df_new = pd.read_csv(new_file)
    print(f"Loaded {len(df_new)} entries")

    # Convert surface density to M_sun / kpc^2 (from M_sun / Mpc^2)
    df_old['surface_density_kpc2'] = df_old['mass_surface_density_Msun_per_Mpc2'] / 1e6
    df_new['surface_density_kpc2'] = df_new['mass_surface_density_Msun_per_Mpc2'] / 1e6

    # Classification functions
    def classify_tertiles(df):
        tertiles = np.percentile(df['surface_density_kpc2'].dropna(), [33.33, 66.67])
        def classify(x):
            if x <= tertiles[0]:
                return 'low'
            elif x <= tertiles[1]:
                return 'medium'
            else:
                return 'high'
        return df['surface_density_kpc2'].apply(classify)

    def classify_fixed(df):
        def classify(x):
            if pd.isna(x) or x == 0:
                return 'zero_galaxies'
            elif x < 1e3:
                return 'low_density'
            elif x < 1e4:
                return 'medium_density'
            else:
                return 'high_density'
        return df['surface_density_kpc2'].apply(classify)

    # Apply classifications
    df_old['class_tertiles'] = classify_tertiles(df_old)
    df_new['class_tertiles'] = classify_tertiles(df_new)

    df_old['class_fixed'] = classify_fixed(df_old)
    df_new['class_fixed'] = classify_fixed(df_new)

    # Print counts
    print("\nOld results classification counts (tertiles):")
    print(df_old['class_tertiles'].value_counts())

    print("\nOld results classification counts (fixed cutoffs):")
    print(df_old['class_fixed'].value_counts())

    print("\nNew results classification counts (tertiles):")
    print(df_new['class_tertiles'].value_counts())

    print("\nNew results classification counts (fixed cutoffs):")
    print(df_new['class_fixed'].value_counts())

    # Plot histograms side-by-side
    plt.figure(figsize=(14, 6))

    plt.subplot(1,2,1)
    plt.hist(df_old['surface_density_kpc2'].dropna(), bins=30, color='blue', alpha=0.7, label='Old results')
    plt.axvline(1e3, color='green', linestyle='--', label='Low/Medium cutoff (1e3)')
    plt.axvline(1e4, color='red', linestyle='--', label='Medium/High cutoff (1e4)')
    plt.xlabel('Surface Density (Mâ / kpcÂ²)')
    plt.ylabel('Number of lenses')
    plt.title('Old Results Surface Density')
    plt.legend()
    plt.yscale('log')

    plt.subplot(1,2,2)
    plt.hist(df_new['surface_density_kpc2'].dropna(), bins=30, color='orange', alpha=0.7, label='New results')
    plt.axvline(1e3, color='green', linestyle='--', label='Low/Medium cutoff (1e3)')
    plt.axvline(1e4, color='red', linestyle='--', label='Medium/High cutoff (1e4)')
    plt.xlabel('Surface Density (Mâ / kpcÂ²)')
    plt.ylabel('Number of lenses')
    plt.title('New Results Surface Density')
    plt.legend()
    plt.yscale('log')

    plt.tight_layout()
    plt.show()

import pandas as pd

# Load your data file (adjust path as needed)
file_path = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_progress.csv'
df = pd.read_csv(file_path)

# Define the fixed cutoffs in surface density (M_sun/kpc^2)
low_threshold = 1e3      # 1,000 Mâ/kpcÂ²
medium_threshold = 1e4   # 10,000 Mâ/kpcÂ²

# Filter out zero galaxies first (total_mass or surface_density == 0)
df_nonzero = df[df['mass_surface_density_Msun_per_kpc2'] > 0]

# Count how many fall into each fixed category
low_count = (df_nonzero['mass_surface_density_Msun_per_kpc2'] < low_threshold).sum()
medium_count = ((df_nonzero['mass_surface_density_Msun_per_kpc2'] >= low_threshold) &
                (df_nonzero['mass_surface_density_Msun_per_kpc2'] < medium_threshold)).sum()
high_count = (df_nonzero['mass_surface_density_Msun_per_kpc2'] >= medium_threshold).sum()

print(f"Low density lenses: {low_count}")
print(f"Medium density lenses: {medium_count}")
print(f"High density lenses: {high_count}")
print(f"Lenses with zero galaxies found: {(df['mass_surface_density_Msun_per_kpc2'] == 0).sum()}")

import pandas as pd

# Path to your saved results CSV (update if needed)
file_path = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_progress.csv'

# Load the data
df = pd.read_csv(file_path)
print(f"Loaded {len(df)} lenses.")

# Check the columns to verify surface density column name
print("Columns in data:", df.columns)

# Convert surface density from Msun/Mpc^2 to Msun/kpc^2
# 1 Mpc^2 = 1,000,000 kpc^2, so divide by 1e6
df['surface_density_kpc2'] = df['mass_surface_density_Msun_per_Mpc2'] / 1e6

# Define thresholds in Msun/kpc^2
low_threshold = 1e3      # 1,000 Msun/kpc^2 (low density)
medium_threshold = 1e4   # 10,000 Msun/kpc^2 (medium density)

# Classify lenses by surface density
def classify_density(sd):
    if sd == 0:
        return 'zero_galaxies'
    elif sd < low_threshold:
        return 'low_density'
    elif sd < medium_threshold:
        return 'medium_density'
    else:
        return 'high_density'

df['environment_class'] = df['surface_density_kpc2'].apply(classify_density)

# Count how many lenses fall into each category
counts = df['environment_class'].value_counts()
print("\nLens counts by environment:")
print(counts)

# Optional: save the classified data for your reference
output_csv = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_classified.csv'
df.to_csv(output_csv, index=False)
print(f"\nClassified data saved to '{output_csv}'")

print(df['surface_density_kpc2'].describe())

def classify_density(value):
    if value == 0:
        return 'zero'
    elif value <= 1e6:
        return 'low'
    elif value <= 1e7:
        return 'medium'
    else:
        return 'high'

df['environment_class'] = df['surface_density_kpc2'].apply(classify_density)

counts = df['environment_class'].value_counts()
print("Lens counts by environment:")
print(counts)

import pandas as pd
from scipy.stats import skew

# Load your data
file_path = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_classified.csv'
df = pd.read_csv(file_path)

# Extract the surface density column
surface_density = df['surface_density_kpc2']

# Calculate skewness (ignore zeros if you want)
skewness_all = skew(surface_density)
skewness_nonzero = skew(surface_density[surface_density > 0])

# Calculate percentiles
percentiles = surface_density.quantile([0.1, 0.25, 0.5, 0.75, 0.9])

print(f"Skewness (all data): {skewness_all:.3f}")
print(f"Skewness (non-zero data): {skewness_nonzero:.3f}")
print("\nPercentiles:")
print(percentiles)

import pandas as pd
from scipy.stats import skew

# Load your data (update file path as needed)
file_path = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_classified.csv'
df = pd.read_csv(file_path)

# Filter out zero surface densities
non_zero = df[df['surface_density_kpc2'] > 0]['surface_density_kpc2']

# Calculate skewness on non-zero data
skewness_non_zero = skew(non_zero)

# Calculate percentiles on non-zero data
percentiles_non_zero = non_zero.quantile([0.10, 0.25, 0.50, 0.75, 0.90])

print(f"Skewness (non-zero data): {skewness_non_zero:.3f}\n")
print("Percentiles (non-zero data):")
print(percentiles_non_zero)

import pandas as pd
import matplotlib.pyplot as plt

# Load your data (adjust path as needed)
file_path = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_classified.csv'
df = pd.read_csv(file_path)

# Filter out zero surface densities
non_zero_df = df[df['surface_density_kpc2'] > 0]

# Plot histogram with log scale (better for skewed data)
plt.figure(figsize=(10,5))
plt.hist(non_zero_df['surface_density_kpc2'], bins=30, color='skyblue', edgecolor='black')
plt.xlabel('Surface Density (Mâ/kpcÂ²)')
plt.ylabel('Number of Lenses')
plt.title('Histogram of Non-zero Surface Densities (log scale)')
plt.yscale('log')
plt.grid(True, which='both

import pandas as pd
import matplotlib.pyplot as plt

# Load your data (adjust path as needed)
file_path = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_classified.csv'
df = pd.read_csv(file_path)

# Filter out zero surface densities
non_zero_df = df[df['surface_density_kpc2'] > 0]

# Plot histogram with log scale (better for skewed data)
plt.figure(figsize=(10,5))
plt.hist(non_zero_df['surface_density_kpc2'], bins=30, color='skyblue', edgecolor='black')
plt.xlabel('Surface Density (Mâ/kpcÂ²)')
plt.ylabel('Number of Lenses')
plt.title('Histogram of Non-zero Surface Densities (log scale)')
plt.yscale('log')
plt.grid(True, which='both', linestyle='--', alpha=0.5)
plt.show()

# Boxplot to show spread and outliers
plt.figure(figsize=(6,4))
plt.boxplot(non_zero_df['surface_density_kpc2'], vert=False, patch_artist=True, boxprops=dict(facecolor='lightgreen'))
plt.xlabel('Surface Density (Mâ/kpcÂ²)')
plt.title('Boxplot of Non-zero Surface Densities')
plt.grid(True, axis='x', linestyle='--', alpha=0.5)
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import spearmanr, pearsonr
from sklearn.linear_model import LinearRegression

# Filter data to non-zero surface densities
df_nz = non_zero_df.copy()

# Log-transform the surface density
df_nz['log_density'] = np.log10(df_nz['surface_density_kpc2'])

# Compute correlation coefficients
pearson_corr, p_pearson = pearsonr(df_nz['redshift'], df_nz['log_density'])
spearman_corr, p_spearman = spearmanr(df_nz['redshift'], df_nz['log_density'])

# Fit linear regression model on redshift vs. log(surface_density)
X = df_nz['redshift'].values.reshape(-1, 1)
y = df_nz['log_density'].values
model = LinearRegression().fit(X, y)
trendline = model.predict(X)

# Plot
plt.figure(figsize=(8, 6))
plt.scatter(df_nz['redshift'], df_nz['surface_density_kpc2'], alpha=0.5, label='Data')
plt.plot(df_nz['redshift'], 10**trendline, color='red', label='Trendline (log fit)')
plt.yscale('log')
plt.xlabel('Redshift')
plt.ylabel('Stellar Mass Surface Density [Mâ/kpcÂ²]')
plt.title('Surface Density vs. Redshift (log scale)')
plt.grid(True)
plt.legend()

# Show correlation in title
plt.suptitle(f'Pearson r = {pearson_corr:.2f}, Spearman Ï = {spearman_corr:.2f}', y=0.94, fontsize=10)

plt.tight_layout()
plt.show()



import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from scipy.stats import pearsonr, spearmanr

# Assuming your DataFrame is called df
# Filter out zero surface density values
df_nonzero = df[df['surface_density_kpc2'] > 0].copy()

# Extract x and y
x = df_nonzero = df_nonzero.rename(columns={'redshift': 'z_lens'})  # if your column is 'redshift'

y = df_nonzero['surface_density_kpc2']

# Compute correlation coefficients
pearson_r, pearson_p = pearsonr(x, y)
spearman_r, spearman_p = spearmanr(x, y)

# Plot setup
plt.figure(figsize=(10, 6))
sns.scatterplot(x=x, y=y, s=50, color='dodgerblue', alpha=0.6, label='Lenses')

# Regression line (linear)
sns.regplot(x=x, y=y, scatter=False, color='red', label='Linear Fit')

# LOWESS smoothing
lowess = sm.nonparametric.lowess
lowess_smoothed = lowess(y, x, frac=0.4)
plt.plot(lowess_smoothed[:, 0], lowess_smoothed[:, 1], color='green', lw=2, label='LOWESS Curve')

# Plot labels and legend
plt.xlabel('Lens Redshift (z)', fontsize=14)
plt.ylabel('Surface Density (kpcâ»Â²)', fontsize=14)
plt.title('Surface Density vs. Lens Redshift', fontsize=16)
plt.grid(True, which='both', ls='--', alpha=0.5)
plt.legend()
plt.tight_layout()

# Print correlation results
print(f"Pearson r = {pearson_r:.3f}, p = {pearson_p:.3e}")
print(f"Spearman Ï = {spearman_r:.3f}, p = {spearman_p:.3e}")

# Show the plot
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from scipy.stats import pearsonr, spearmanr
import numpy as np

# Load your data (update the path and filename)
file_path = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_classified.csv'
df = pd.read_csv(file_path)

# Make sure column names match your data; adjust if needed
# For example: 'redshift' or 'zlens', 'surface_density_kpc2' or similar
redshift_col = 'redshift'
surface_density_col = 'surface_density_kpc2'

# Filter to keep only rows with valid (nonzero, non-null) surface density and redshift
df_filtered = df[
    (df[surface_density_col] > 0) &
    (~df[surface_density_col].isna()) &
    (~df[redshift_col].isna())
].copy()

# Extract Series for correlation
x = df_filtered[redshift_col]
y = df_filtered[surface_density_col]

# Defensive check: ensure both are 1D and same length
assert x.ndim == 1 and y.ndim == 1 and len(x) == len(y), "x and y must be 1D and equal length"

# Compute correlations
pearson_r, pearson_p = pearsonr(x, y)
spearman_r, spearman_p = spearmanr(x, y)

# Plot
plt.figure(figsize=(10, 6))
sns.scatterplot(x=x, y=y, s=50, color='dodgerblue', alpha=0.6, label='Lenses')

# Linear regression line
sns.regplot(x=x, y=y, scatter=False, color='red', label='Linear Fit')

# LOWESS smoothing
lowess = sm.nonparametric.lowess
lowess_smoothed = lowess(y, x, frac=0.4)
plt.plot(lowess_smoothed[:, 0], lowess_smoothed[:, 1], color='green', lw=2, label='LOWESS Curve')

plt.xlabel('Lens Redshift (z)', fontsize=14)
plt.ylabel('Surface Density (kpc$^{-2}$)', fontsize=14)
plt.title('Surface Density vs. Lens Redshift', fontsize=16)
plt.grid(True, which='both', ls='--', alpha=0.5)
plt.legend()
plt.tight_layout()

print(f"Pearson r = {pearson_r:.3f}, p = {pearson_p:.3e}")
print(f"Spearman Ï = {spearman_r:.3f}, p = {spearman_p:.3e}")

plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Load your lens data
df = pd.read_csv("lens_stellar_mass_lenscat.csv")

# Assume there's a column like 'stellar_density' or 'local_mass_density'
density_col = 'stellar_density'  # or whatever you named it

# Drop missing or non-numeric density rows
df = df[pd.to_numeric(df[density_col], errors='coerce').notnull()]
df[density_col] = df[density_col].astype(float)

# Rank lenses by density into percentiles
df['density_percentile'] = pd.qcut(df[density_col], 100, labels=False)  # 0 = lowest, 99 = highest

# Tag lenses in the lowest 10% and 25%
df['low_density_10pct'] = df['density_percentile'] <= 9
df['low_density_25pct'] = df['density_percentile'] <= 24

# Count how many fall in low-density bins
n_total = len(df)
n_low10 = df['low_density_10pct'].sum()
n_low25 = df['low_density_25pct'].sum()

print(f"Total lenses: {n_total}")
print(f"Lenses in bottom 10% of density: {n_low10} ({n_low10/n_total:.1%})")
print(f"Lenses in bottom 25% of density: {n_low25} ({n_low25/n_total:.1%})")

# OPTIONAL: Save these subsets for inspection
df[df['low_density_10pct']].to_csv("low_density_lenses_10pct.csv", index=False)
df[df['low_density_25pct']].to_csv("low_density_lenses_25pct.csv", index=False)

# Plot histogram to show density distribution
plt.figure(figsize=(8, 5))
plt.hist(df[density_col], bins=30, alpha=0.7)
plt.axvline(df[density_col].quantile(0.10), color='r', linestyle='--', label='10th percentile')
plt.axvline(df[density_col].quantile(0.25), color='orange', linestyle='--', label='25th percentile')
plt.xlabel("Stellar Mass Density (Î£*)")
plt.ylabel("Number of Lenses")
plt.title("Distribution of Local Stellar Mass Density")
plt.legend()
plt.tight_layout()
plt.show()



from google.colab import drive
drive.mount('/content/drive')

# Change path to your folder if different
!ls /content/drive/MyDrive/

# Step 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Step 2: Define your path (edit if needed)
import os
drive_path = '/content/drive/MyDrive/lens_stellar_mass_lenscat.csv'

# Step 3: Load CSV using pandas
import pandas as pd

# Confirm the file exists first
if os.path.exists(drive_path):
    df = pd.read_csv(drive_path)
    print("CSV successfully loaded. Preview:")
    print(df.head())
else:
    print("â File not found at:", drive_path)

# Search your Drive for the filename
import os

search_name = "lens_stellar_mass_lenscat.csv"
for root, dirs, files in os.walk("/content/drive/MyDrive"):
    if search_name in files:
        print("â Found at:", os.path.join(root, search_name))

import os

target = "lens_stellar_mass_lenscat.csv"
matches = []

for root, dirs, files in os.walk("/content/drive/MyDrive"):
    if target in files:
        matches.append(os.path.join(root, target))

if matches:
    print("â Found at:", matches[0])
else:
    print("â Not found.")

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Install lenscat if needed
!pip install -q lenscat

# Imports
import pandas as pd
from lenscat import get_lenses
import numpy as np

# Step 1: Load lens catalog
lenses = get_lenses()
df = lenses.copy()
print(f"â Loaded {len(df)} total lenses.")

# Step 2: Filter lenses with redshift and stellar mass
df = df[df['z'] > 0]
df = df[df['stellar_mass_logMsun'].notna()]
print(f"â Filtered to {len(df)} lenses with redshift and stellar mass.")

# Step 3: (Optional) Compute stellar mass in Msun
df['stellar_mass'] = 10 ** df['stellar_mass_logMsun']

# Optional: compute "surface density" proxy
# If effective radius (Re) in kpc is available
if 'Re_kpc' in df.columns and df['Re_kpc'].notna().sum() > 0:
    df = df[df['Re_kpc'].notna()]
    df['surface_density'] = df['stellar_mass'] / (np.pi * df['Re_kpc']**2)
    print("â Computed surface_density [Msun/kpcÂ²]")

# Step 4: Save to Google Drive
save_path = "/content/drive/MyDrive/lens_stellar_mass_lenscat.csv"
df.to_csv(save_path, index=False)
print(f"â Saved to: {save_path}")

import lenscat
print(lenscat.__version__)

from lenscat import catalog

# catalog is a global variable representing the full lens catalog
df = catalog.to_pandas()

print(df.head())

from google.colab import drive
import pandas as pd
import os

# Mount Drive if not done yet
drive.mount('/content/drive')

# Set your file path â update this to your actual CSV file location
file_path = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_progress.csv'

# Check if file exists
if os.path.isfile(file_path):
    print(f"Loading file from: {file_path}")
    df = pd.read_csv(file_path)
    print(f"Loaded {len(df)} lenses")
    print(df.head())
else:
    print(f"File NOT found at: {file_path}")

import os

folder = '/content/drive/MyDrive/lens_stellar_mass_results/'

print("CSV files in lens_stellar_mass_results:")
for filename in os

import os

folder = '/content/drive/MyDrive/lens_stellar_mass_results/'

print("CSV files in lens_stellar_mass_results:")
for filename in os

import os

folder = '/content/drive/MyDrive/lens_stellar_mass_results/'

print("CSV files in lens_stellar_mass_results:")
for filename in os.listdir(folder):
    if filename.endswith('.csv'):
        print(filename)

import pandas as pd

# Define paths to the 3 candidate files
files = {
    "progress": "lens_stellar_mass_results/lens_stellar_mass_progress.csv",
    "with_env": "lens_stellar_mass_results/lens_stellar_mass_with_env.csv",
    "classified": "lens_stellar_mass_results/lens_stellar_mass_classified.csv"
}

# Function to preview numeric columns from each file
for label, path in files.items():
    print(f"\n=== Preview: {label} ===")
    try:
        df = pd.read_csv(path)
        numeric_cols = df.select_dtypes(include='number').columns
        print(f"Numeric columns: {list(numeric_cols)}")
        print(df[numeric_cols].head(3))
    except Exception as e:
        print(f"Error reading {path}: {e}")

import os

directory = "lens_stellar_mass_results"
for file in os.listdir(directory):
    if file.endswith(".csv"):
        print(file)

import os

# Show your current working directory
print("Current working directory:", os.getcwd())

# List all files and folders here
print("\nFiles and directories in current location:")
print(os.listdir("."))

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Load your classified data (update path if needed)
file_path = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_classified.csv'
df = pd.read_csv(file_path)

# Use the non-zero surface densities only
surface_densities = df.loc[df['surface_density_kpc2'] > 0, 'surface_density_kpc2']

# Calculate percentiles
percentiles = np.percentile(surface_densities, [10, 25, 50, 75, 90])

print("Percentiles of non-zero surface densities (M_sun/kpc^2):")
for p, val in zip([10,25,50,75,90], percentiles):
    print(f"  {p}th percentile: {val:.2e}")

# CDM Environment Density Ranges in M_sun/kpc^2 (approximate)
cdm_envs = {
    'Field galaxy': (1e2, 1e3),
    'Group': (1e3, 1e4),
    'Cluster outskirts': (1e4, 1e5),
    'Cluster core': (1e5, 1e6),
}

# Plot histogram of surface densities
plt.figure(figsize=(10,6))
plt.hist(surface_densities, bins=40, color='skyblue', edgecolor='black', log=True)

# Plot percentile lines
for val, p in zip(percentiles, [10,25,50,75,90]):
    plt.axvline(val, color='red', linestyle='--', alpha=0.7)
    plt.text(val, plt.ylim()[1]*0.5, f'{p}th', rotation=90, color='red', va='center')

# Plot CDM environment bands (log scale)
for env, (low, high) in cdm_envs.items():
    plt.fill_betweenx([1, 1e3], low, high, alpha=0.15, label=env)

plt.xscale('log')
plt.xlabel('Surface Mass Density (M$_\odot$/kpc$^2$)')
plt.ylabel('Number of Lenses (log scale)')
plt.title('Lens Surface Densities vs. CDM Environment Density Ranges')
plt.legend(loc='upper right')
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np

# Load your data
file_path = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_classified.csv'
df = pd.read_csv(file_path)

# Filter positive stellar surface mass densities
surface_densities = df.loc[df['surface_density_kpc2'] > 0, 'surface_density_kpc2']

# Calculate the 10th and 20th percentiles
percentiles = np.percentile(surface_densities, [10, 20])
print(f"Observed 10th percentile (stellar): {percentiles[0]:.2e} M_sun/kpc^2")
print(f"Observed 20th percentile (stellar): {percentiles[1]:.2e} M_sun/kpc^2")

# Assume a range of stellar-to-total mass fractions f*
f_star_values = [0.01, 0.03, 0.05, 0.1, 0.2]

print("\nInferred total surface mass density Î£_total = Î£_* / f_*:")

for f_star in f_star_values:
    total_10 = percentiles[0] / f_star
    total_20 = percentiles[1] / f_star
    print(f"f* = {f_star:.2f} | 10th percentile Î£_total = {total_10:.2e} | 20th percentile Î£_total = {total_20:.2e}")

# CDM strong lensing threshold
cdm_threshold = 1e8

print("\nComparison to CDM strong lensing threshold (1e8 M_sun/kpc^2):")

for f_star in f_star_values:
    total_10 = percentiles[0] / f_star
    total_20 = percentiles[1] / f_star
    below_10 = total_10 < cdm_threshold
    below_20 = total_20 < cdm_threshold
    print(f"f*={f_star:.2f}: 10th percentile below threshold? {below_10}, 20th percentile below threshold? {below_20}")

import pandas as pd
import numpy as np

file_path = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_classified.csv'
df = pd.read_csv(file_path)

surface_densities = df.loc[df['surface_density_kpc2'] > 0, 'surface_density_kpc2']

# Get 10th percentile cutoff
cutoff_10 = np.percentile(surface_densities, 10)

# Select lenses in lowest 10%
lowest_10 = surface_densities[surface_densities <= cutoff_10]

cdm_threshold = 1e8  # strong lensing threshold in M_sun/kpc^2

f_star_values = [0.01, 0.03, 0.05, 0.1, 0.2]

print(f"Lenses in lowest 10% stellar surface density: {len(lowest_10)}\n")

for f_star in f_star_values:
    inferred_total = lowest_10 / f_star
    n_below = (inferred_total < cdm_threshold).sum()
    n_total = len(lowest_10)
    frac_below = n_below / n_total * 100
    print(f"f* = {f_star:.2f} | {n_below}/{n_total} lenses ({frac_below:.1f}%) below CDM lensing threshold")

print(f"Total lenses with positive surface density: {len(surface_densities)}")

import pandas as pd

progress_file = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_progress.csv'

# Load the progress file
df_progress = pd.read_csv(progress_file)

# See columns to confirm what data is available
print(df_progress.columns)

# Assuming it has columns like 'density_category' or similar,
# summarize counts per density category:
counts = df_progress['density_category'].value_counts()

print("Lens counts by density category so far:")
print(counts)

# Total lenses processed so far:
total_lenses = len(df_progress)
print(f"\nTotal lenses processed so far: {total_lenses}")

import pandas as pd
import matplotlib.pyplot as plt

progress_file = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_progress.csv'

df_progress = pd.read_csv(progress_file)

# Count lenses per density class
counts = df_progress['density_class'].value_counts()
total_lenses = len(df_progress)

print("Lens counts by density class:")
print(counts)

# Calculate percentage per class
percentages = counts / total_lenses * 100
print("\nPercentages by density class:")
print(percentages.round(2))

# Bar plot of density class distribution
plt.figure(figsize=(8,5))
counts.plot(kind='bar', color=['steelblue', 'orange', 'green'])
plt.title('Lens Counts by Density Class (Progress so far)')
plt.ylabel('Number of Lenses')
plt.xlabel('Density Class')
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

# OPTIONAL: If you want to explicitly calculate lowest 10% by mass_surface_density_Msun_per_Mpc2,
# convert Mpc^2 to kpc^2 (1 Mpc^2 = 1e6 kpc^2) first:

df_progress['mass_surface_density_Msun_per_kpc2'] = (
    df_progress['mass_surface_density_Msun_per_Mpc2'] / 1e6
)

cutoff_10 = df_progress['mass_surface_density_Msun_per_kpc2'].quantile(0.10)
print(f"\nLowest 10% surface density cutoff (M_sun/kpc^2): {cutoff_10:.2e}")

lowest_10 = df_progress[df_progress['mass_surface_density_Msun_per_kpc2'] <= cutoff_10]
print(f"Lenses in lowest 10% by surface density: {len(lowest_10)}")

import pandas as pd
import numpy as np

progress_file = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_progress.csv'
df = pd.read_csv(progress_file)

# Convert surface density to kpc^2 units if needed
df['mass_surface_density_kpc2'] = df['mass_surface_density_Msun_per_Mpc2'] / 1e6

# Filter out zero surface density lenses
non_zero_df = df[df['mass_surface_density_kpc2'] > 0].copy()

print(f"Total lenses: {len(df)}")
print(f"Lenses with non-zero surface density: {len(non_zero_df)}")

# Calculate 10th percentile cutoff of non-zero surface densities
cutoff_10 = np.percentile(non_zero_df['mass_surface_density_kpc2'], 10)
print(f"10th percentile (non-zero) surface density cutoff: {cutoff_10:.2e} M_sun/kpc^2")

# Select lenses below or equal to this 10th percentile cutoff
lowest_10 = non_zero_df[non_zero_df['mass_surface_density_kpc2'] <= cutoff_10]

print(f"Number of lenses in lowest 10% (non-zero): {len(lowest_10)}")

# Now, for each baryon fraction, compute total surface density and compare to CDM threshold
cdm_threshold = 1e8  # M_sun/kpc^2
f_star_values = [0.01, 0.03, 0.05, 0.1, 0.2]

for f_star in f_star_values:
    inferred_total = lowest_10['mass_surface_density_kpc2'] / f_star
    n_below = (inferred_total < cdm_threshold).sum()
    n_total = len(lowest_10)
    frac_below = n_below / n_total * 100 if n_total > 0 else 0
    print(f"f* = {f_star:.2f} | {n_below}/{n_total} lenses ({frac_below:.1f}%) below CDM threshold")

import pandas as pd

progress_file = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_progress.csv'
df = pd.read_csv(progress_file)

# Convert surface density units: M_sun/Mpc^2 to M_sun/kpc^2
df['mass_surface_density_kpc2'] = df['mass_surface_density_Msun_per_Mpc2'] / 1e6

# Filter out zero surface density lenses
non_zero_df = df[df['mass_surface_density_kpc2'] > 0].copy()

cdm_threshold = 1e8  # M_sun/kpc^2

f_star_values = [0.01, 0.03, 0.05, 0.1, 0.2]

total_lenses = len(non_zero_df)
print(f"Total lenses with non-zero stellar surface density: {total_lenses}\n")

for f_star in f_star_values:
    inferred_total = non_zero_df['mass_surface_density_kpc2'] / f_star
    below_threshold = inferred_total < cdm_threshold
    n_below = below_threshold.sum()
    frac_below = n_below / total_lenses * 100
    print(f"f* = {f_star:.2f} | {n_below}/{total_lenses} lenses ({frac_below:.1f}%) below CDM threshold")

import numpy as np
import pandas as pd

progress_file = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_progress.csv'
df = pd.read_csv(progress_file)

# Convert surface density units: M_sun/Mpc^2 to M_sun/kpc^2
df['mass_surface_density_kpc2'] = df['mass_surface_density_Msun_per_Mpc2'] / 1e6

# Filter out zero surface density lenses
non_zero_df = df[df['mass_surface_density_kpc2'] > 0].copy()

cdm_threshold = 1e8  # M_sun/kpc^2

# Create fine grid for f_star in [0.05, 0.1] at 0.01 steps
f_star_fine = np.arange(0.05, 0.11, 0.01)

total_lenses = len(non_zero_df)
print(f"Total lenses with non-zero stellar surface density: {total_lenses}\n")

for f_star in f_star_fine:
    inferred_total = non_zero_df['mass_surface_density_kpc2'] / f_star
    n_below = (inferred_total < cdm_threshold).sum()
    frac_below = n_below / total_lenses * 100
    print(f"f* = {f_star:.2f} | {n_below}/{total_lenses} lenses ({frac_below:.1f}%) below CDM threshold")

import pandas as pd

# Load directly from the GitHub CSV
url = "https://raw.githubusercontent.com/coljac/lenscat/main/catalogue.csv"
df = pd.read_csv(url)

# Filter to confident lenses with valid redshift
df = df[(df['grade'] == 'confident') & df['z'].notna()]

print(f"Loaded {len(df)} confident lenses with redshift.")
df.head()

import pandas as pd

url = "https://raw.githubusercontent.com/coljac/lenscat/main/lenscat/catalogue.csv"
df = pd.read_csv(url)

# Filter to confident lenses with valid redshift
df = df[(df['grade'] == 'confident') & df['z'].notna()]

print(f"Loaded {len(df)} confident lenses with redshift.")
df.head()

import pandas as pd

# Directly input a few lens entries (add more if needed)
lens_data = [
    {"ra": 10.684, "dec": 41.269, "grade": "confident", "z": 0.05},
    {"ra": 150.116, "dec": 2.2058, "grade": "confident", "z": 0.21},
    {"ra": 53.123, "dec": -27.808, "grade": "probable", "z": 0.33},
    {"ra": 180.0, "dec": 30.0, "grade": "confident", "z": None},
    # add your full dataset here
]

df = pd.DataFrame(lens_data)

# Filter to confident lenses with valid redshift
df = df[(df["grade"] == "confident") & df["z"].notna()]

print(f"Loaded {len(df)} confident lenses with redshift.")
df.head()

# Install if needed (uncomment in Colab)
# !pip install lenscat astroquery astropy pandas numpy scipy --quiet

import os
import time
import random
import numpy as np
import pandas as pd
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.simbad import Simbad
from lenscat import catalog

# Setup SIMBAD query
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
# Correct way to clear votable fields:
custom_simbad.votable_fields.clear()
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

def query_bh_counts(coord, radius_arcmin=10, max_retries=3):
    radius = radius_arcmin * u.arcmin
    retries = 0
    while retries < max_retries:
        try:
            result = custom_simbad.query_region(coord, radius=radius)
            if result is None or 'OTYPE' not in result.colnames:
                return 0, []
            mask = [any(bh in otype.upper() for bh in bh_types) for otype in result['OTYPE']]
            filtered = result[mask]
            count = len(filtered)
            coords_bh = SkyCoord(ra=filtered['RA'], dec=filtered['DEC'], unit=(u.hourangle, u.deg)) if count > 0 else []
            return count, coords_bh
        except Exception as e:
            retries += 1
            print(f"SIMBAD query failed at {coord.to_string('hmsdms')} attempt {retries}: {e}")
            time.sleep(5)
    print(f"SIMBAD query failed at {coord.to_string('hmsdms')} after {max_retries} attempts.")
    return 0, []

print("Loading lens catalog from lenscat...")
df = catalog.to_pandas()

df_confident = df[df['grading'] == 'confident'].copy()
df_confident['z'] = pd.to_numeric(df_confident['zlens'], errors='coerce')
df_confident = df_confident.dropna(subset=['z']).reset_index(drop=True)

print(f"Number of confident lenses with redshift: {len(df_confident)}")

batch_size = 25
radii = [10, 15, 20]  # arcminutes
all_results = []

for batch_i in range(0, len(df_confident), batch_size):
    batch_df = df_confident.iloc[batch_i:batch_i + batch_size]
    print(f"\nProcessing batch {(batch_i // batch_size) + 1}, lenses {batch_i} to {batch_i + len(batch_df)}")

    for radius in radii:
        print(f" Radius = {radius} arcmin")
        total_bh = 0
        zero_bh_lenses = 0

        for _, lens in batch_df.iterrows():
            coord = SkyCoord(ra=lens['RA']*u.deg, dec=lens['DEC']*u.deg)
            count, coords_bh = query_bh_counts(coord, radius)
            if count == 0:
                zero_bh_lenses += 1
            total_bh += count
            all_results.append({
                'lens_name': lens['name'],
                'radius_arcmin': radius,
                'bh_count': count
            })

        n_lenses = len(batch_df)
        avg_bh = total_bh / n_lenses if n_lenses else 0
        zero_frac = zero_bh_lenses / n_lenses if n_lenses else 0

        print(f"  Total BH found: {total_bh}")
        print(f"  Average BH per lens: {avg_bh:.2f}")
        print(f"  Lenses with zero BH: {zero_bh_lenses} ({zero_frac:.1%})")

print("Done.")

# Install if needed (uncomment in Colab)
# !pip install lenscat astroquery astropy pandas numpy scipy --quiet

import os
import time
import random
import numpy as np
import pandas as pd
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.simbad import Simbad
from lenscat import catalog

# Setup SIMBAD query
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60

# Clear all default fields robustly
try:
    custom_simbad.remove_votable_fields('all')
except Exception:
    for field in custom_simbad.get_votable_fields():
        custom_simbad.remove_votable_fields(field)

# Add only fields we want
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

def query_bh_counts(coord, radius_arcmin=10, max_retries=3):
    radius = radius_arcmin * u.arcmin
    retries = 0
    while retries < max_retries:
        try:
            result = custom_simbad.query_region(coord, radius=radius)
            if result is None or 'OTYPE' not in result.colnames:
                return 0, None
            mask = [any(bh in otype.upper() for bh in bh_types) for otype in result['OTYPE']]
            filtered = result[mask]
            count = len(filtered)
            if count > 0:
                coords_bh = SkyCoord(ra=filtered['RA'], dec=filtered['DEC'], unit=(u.hourangle, u.deg))
            else:
                coords_bh = None
            return count, coords_bh
        except Exception as e:
            retries += 1
            print(f"SIMBAD query failed at {coord.to_string('hmsdms')} attempt {retries}: {e}")
            time.sleep(5)
    print(f"SIMBAD query failed at {coord.to_string('hmsdms')} after {max_retries} attempts.")
    return 0, None

print("Loading lens catalog from lenscat...")
df = catalog.to_pandas()

df_confident = df[df['grading'] == 'confident'].copy()
df_confident['z'] = pd.to_numeric(df_confident['zlens'], errors='coerce')
df_confident = df_confident.dropna(subset=['z']).reset_index(drop=True)

print(f"Number of confident lenses with redshift: {len(df_confident)}")

batch_size = 25
radii = [10, 15, 20]  # arcminutes
all_results = []

for batch_i in range(0, len(df_confident), batch_size):
    batch_df = df_confident.iloc[batch_i:batch_i + batch_size]
    print(f"\nProcessing batch {(batch_i // batch_size) + 1}, lenses {batch_i} to {batch_i + len(batch_df) - 1}")

    for radius in radii:
        print(f" Radius = {radius} arcmin")
        total_bh = 0
        zero_bh_lenses = 0

        for _, lens in batch_df.iterrows():
            coord = SkyCoord(ra=lens['RA']*u.deg, dec=lens['DEC']*u.deg)
            count, coords_bh = query_bh_counts(coord, radius)
            if count == 0:
                zero_bh_lenses += 1
            total_bh += count
            all_results.append({
                'lens_name': lens['name'],
                'radius_arcmin': radius,
                'bh_count': count
            })
            time.sleep(1)  # Sleep 1s to be polite to SIMBAD server

        n_lenses = len(batch_df)
        avg_bh = total_bh / n_lenses if n_lenses else 0
        zero_frac = zero_bh_lenses / n_lenses if n_lenses else 0

        print(f"  Total BH found: {total_bh}")
        print(f"  Average BH per lens: {avg_bh:.2f}")
        print(f"  Lenses with zero BH: {zero_bh_lenses} ({zero_frac:.1%})")

print("Done.")



from astropy.coordinates import SkyCoord
import astropy.units as u

coord = SkyCoord(ra=10.684*u.deg, dec=41.269*u.deg)  # Example coords (Andromeda)
result = custom_simbad.query_region(coord, radius=10 * u.arcmin)
print(result)

# If needed, uncomment this line in Colab or your environment to install dependencies
# !pip install astroquery astropy pandas lenscat --quiet

import time
import pandas as pd
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.simbad import Simbad
from lenscat import catalog

# Setup Simbad with timeout and add desired fields (no clearing of defaults)
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

def query_bh_count(coord, radius_arcmin=10):
    radius = radius_arcmin * u.arcmin
    try:
        result = custom_simbad.query_region(coord, radius=radius)
        if result is None or 'OTYPE' not in result.colnames:
            return 0
        mask = [any(bh in otype.upper() for bh in bh_types) for otype in result['OTYPE']]
        filtered = result[mask]
        return len(filtered)
    except Exception as e:
        print(f"Query failed at {coord.to_string('hmsdms')}: {e}")
        return 0

print("Loading lens catalog...")
df = catalog.to_pandas()
df_confident = df[df['grading'] == 'confident'].copy()
df_confident['z'] = pd.to_numeric(df_confident['zlens'], errors='coerce')
df_confident = df_confident.dropna(subset=['z']).reset_index(drop=True)

print(f"Total confident lenses with redshift: {len(df_confident)}")

# Only process first 10 lenses for quick test â remove or increase for full run
test_lenses = df_confident.head(10)

for idx, lens in test_lenses.iterrows():
    coord = SkyCoord(ra=lens['RA']*u.deg, dec=lens['DEC']*u.deg)
    count = query_bh_count(coord, radius_arcmin=10)
    print(f"Lens {lens['name']} at RA={lens['RA']}, DEC={lens['DEC']} has {count} BH-type objects within 10 arcmin.")
    time.sleep(1)  # Be polite to SIMBAD servers

print("Done.")

test_coord = SkyCoord(ra=187.2779*u.deg, dec=2.0524*u.deg)
print(query_bh_count(test_coord, 10))

from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u
from lenscat import catalog

custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

# Load lenses
df = catalog.to_pandas()
df_confident = df[df['grading'] == 'confident'].copy()
df_confident['z'] = pd.to_numeric(df_confident['zlens'], errors='coerce')
df_confident = df_confident.dropna(subset=['z']).reset_index(drop=True)

# Pick one lens to test
lens = df_confident.iloc[0]
coord = SkyCoord(ra=lens['RA']*u.deg, dec=lens['DEC']*u.deg)

print(f"Testing SIMBAD query around lens {lens['name']} at RA={lens['RA']}, DEC={lens['DEC']}")

result = custom_simbad.query_region(coord, radius=20*u.arcmin)

if result is None:
    print("No objects found in this region.")
else:
    print(f"Total objects found: {len(result)}")
    print("Distinct SIMBAD object types (OTYPE) in this region:")
    print(set(result['OTYPE']))

    # Now filter BH-type objects
    mask = [any(bh in otype.upper() for bh in bh_types) for otype in result['OTYPE']]
    filtered = result[mask]

    print(f"BH-type objects found: {len(filtered)}")
    if len(filtered) > 0:
        print(filtered[['MAIN_ID', 'OTYPE', 'RA', 'DEC']])

from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u
from lenscat import catalog
import pandas as pd

# Setup Simbad with timeout and desired fields
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

# Load lenses
df = catalog.to_pandas()
df_confident = df[df['grading'] == 'confident'].copy()
df_confident['z'] = pd.to_numeric(df_confident['zlens'], errors='coerce')
df_confident = df_confident.dropna(subset=['z']).reset_index(drop=True)

# Select one lens to test
lens = df_confident.iloc[0]
coord = SkyCoord(ra=lens['RA']*u.deg, dec=lens['DEC']*u.deg)

print(f"Testing SIMBAD query around lens {lens['name']} at RA={lens['RA']}, DEC={lens['DEC']}")

result = custom_simbad.query_region(coord, radius=20*u.arcmin)

if result is None:
    print("No objects found in this region.")
else:
    print(f"Total objects found: {len(result)}")
    print("Columns in SIMBAD result:", result.colnames)

    # Find otype column ignoring case
    otype_col = None
    for col in result.colnames:
        if 'otype' in col.lower():
            otype_col = col
            break

    if otype_col is None:
        print("No OTYPE column found in result!")
    else:
        print(f"Distinct SIMBAD object types in column '{otype_col}':")
        print(set(result[otype_col]))

        # Case-insensitive filtering for BH types
        mask = [any(bh.lower() in str(otype).lower() for bh in bh_types) for otype in result[otype_col]]
        filtered = result[mask]

        print(f"BH-type objects found: {len(filtered)}")
        if len(filtered) > 0:
            print(filtered[['main_id', otype_col, 'ra', 'dec']])

import time
import pandas as pd
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.simbad import Simbad
from lenscat import catalog

# Setup Simbad
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

# Load lens catalog
df = catalog.to_pandas()
df_confident = df[df['grading'] == 'confident'].copy()
df_confident['z'] = pd.to_numeric(df_confident['zlens'], errors='coerce')
df_confident = df_confident.dropna(subset=['z']).reset_index(drop=True)

print(f"Total confident lenses with redshift: {len(df_confident)}")

results = []

for idx, lens in df_confident.iterrows():
    coord = SkyCoord(ra=lens['RA']*u.deg, dec=lens['DEC']*u.deg)
    try:
        result = custom_simbad.query_region(coord, radius=20*u.arcmin)
    except Exception as e:
        print(f"Query failed for lens {lens['name']} at index {idx}: {e}")
        results.append({'lens_name': lens['name'], 'bh_count': None})
        time.sleep(1)
        continue

    if result is None:
        bh_count = 0
    else:
        # Find otype column ignoring case
        otype_col = None
        for col in result.colnames:
            if 'otype' in col.lower():
                otype_col = col
                break

        if otype_col is None:
            bh_count = 0
        else:
            # Case-insensitive filter for BH types
            mask = [any(bh.lower() in str(otype).lower() for bh in bh_types) for otype in result[otype_col]]
            filtered = result[mask]
            bh_count = len(filtered)

    results.append({'lens_name': lens['name'], 'bh_count': bh_count})

    print(f"{idx+1}/{len(df_confident)}: Lens {lens['name']} - BH count: {bh_count}")
    time.sleep(1)  # Respect SIMBAD rate limits

# Save results
df_results = pd.DataFrame(results)
df_results.to_csv('lens_bh_counts.csv', index=False)
print("Saved results to lens_bh_counts.csv")

from google.colab import drive
drive.mount('/content/drive')

import time
import random
import pandas as pd
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.simbad import Simbad
from lenscat import catalog

# Change this path to your Google Drive folder where you want to save
drive_folder = '/content/drive/MyDrive/lens_bh_analysis/'

custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

df = catalog.to_pandas()
df_confident = df[df['grading'] == 'confident'].copy()
df_confident['z'] = pd.to_numeric(df_confident['zlens'], errors='coerce')
df_confident = df_confident.dropna(subset=['z']).reset_index(drop=True)

batch_size = 25
results_lenses = []
results_randoms = []

# Helper: generate a random sky position avoiding lenses Â±0.5 deg buffer
def random_sky_coord(exclude_coords, buffer_deg=0.5):
    while True:
        ra_rand = random.uniform(0, 360)
        dec_rand = random.uniform(-90, 90)
        coord_rand = SkyCoord(ra=ra_rand*u.deg, dec=dec_rand*u.deg)
        # Check separation from all lenses
        sep = coord_rand.separation(exclude_coords)
        if all(sep.degree > buffer_deg):
            return coord_rand

total_lenses = len(df_confident)
lens_coords = SkyCoord(ra=df_confident['RA'].values*u.deg, dec=df_confident['DEC'].values*u.deg)

print(f"Total confident lenses with redshift: {total_lenses}")

for batch_start in range(0, total_lenses, batch_size):
    batch_end = min(batch_start + batch_size, total_lenses)
    batch_df = df_confident.iloc[batch_start:batch_end]
    print(f"\nProcessing lenses {batch_start + 1} to {batch_end}")

    # Query BH counts for lenses
    for idx, lens in batch_df.iterrows():
        coord = SkyCoord(ra=lens['RA']*u.deg, dec=lens['DEC']*u.deg)
        try:
            result = custom_simbad.query_region(coord, radius=20*u.arcmin)
        except Exception as e:
            print(f"Query failed for lens {lens['name']} at index {idx}: {e}")
            results_lenses.append({'lens_name': lens['name'], 'bh_count': None})
            time.sleep(1)
            continue

        if result is None:
            bh_count = 0
        else:
            otype_col = None
            for col in result.colnames:
                if 'otype' in col.lower():
                    otype_col = col
                    break
            if otype_col is None:
                bh_count = 0
            else:
                mask = [any(bh.lower() in str(otype).lower() for bh in bh_types) for otype in result[otype_col]]
                filtered = result[mask]
                bh_count = len(filtered)

        results_lenses.append({'lens_name': lens['name'], 'bh_count': bh_count})
        print(f"  Lens {lens['name']} - BH count: {bh_count}")
        time.sleep(1)

    # Query BH counts for random sky positions (same number as batch size)
    for _ in range(len(batch_df)):
        coord_rand = random_sky_coord(lens_coords, buffer_deg=0.5)
        try:
            result_rand = custom_simbad.query_region(coord_rand, radius=20*u.arcmin)
        except Exception as e:
            print(f"Query failed for random coord {coord_rand.to_string('hmsdms')}: {e}")
            results_randoms.append({'random_coord': coord_rand.to_string('hmsdms'), 'bh_count': None})
            time.sleep(1)
            continue

        if result_rand is None:
            bh_count_rand = 0
        else:
            otype_col = None
            for col in result_rand.colnames:
                if 'otype' in col.lower():
                    otype_col = col
                    break
            if otype_col is None:
                bh_count_rand = 0
            else:
                mask = [any(bh.lower() in str(otype).lower() for bh in bh_types) for otype in result_rand[otype_col]]
                filtered = result_rand[mask]
                bh_count_rand = len(filtered)

        results_randoms.append({'random_coord': coord_rand.to_string('hmsdms'), 'bh_count': bh_count_rand})
        print(f"  Random sky pos {coord_rand.to_string('hmsdms')} - BH count: {bh_count_rand}")
        time.sleep(1)

    # Summarize batch
    lens_counts = [r['bh_count'] for r in results_lenses[batch_start:batch_end] if r['bh_count'] is not None]
    random_counts = [r['bh_count'] for r in results_randoms[-len(batch_df):] if r['bh_count'] is not None]

    print(f"Batch {batch_start // batch_size + 1} summary:")
    print(f"  Lenses mean BH count: {sum(lens_counts)/len(lens_counts):.2f}")
    print(f"  Random mean BH count: {sum(random_counts)/len(random_counts):.2f}")

    # Save batch results
    pd.DataFrame(results_lenses).to_csv(drive_folder + 'lens_bh_counts_partial.csv', index=False)
    pd.DataFrame(results_randoms).to_csv(drive_folder + 'random_bh_counts_partial.csv', index=False)
    print("Saved partial results to Google Drive.")

print("\nAll batches processed.")
pd.DataFrame(results_lenses).to_csv(drive_folder + 'lens_bh_counts_final.csv', index=False)
pd.DataFrame(results_randoms).to_csv(drive_folder + 'random_bh_counts_final.csv', index=False)
print("Saved final results to Google Drive.")

import os

drive_folder = '/content/drive/MyDrive/lens_bh_analysis/'

if not os.path.exists(drive_folder):
    os.makedirs(drive_folder)
    print(f"Created folder: {drive_folder}")
else:
    print(f"Folder already exists: {drive_folder}")

import os
import time
import random
import numpy as np
import pandas as pd
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.simbad import Simbad
from lenscat import catalog
from scipy.stats import ks_2samp, poisson

# Setup Google Drive folder
drive_folder = '/content/drive/MyDrive/lens_bh_analysis/'
if not os.path.exists(drive_folder):
    os.makedirs(drive_folder)

custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

df = catalog.to_pandas()
df_confident = df[df['grading'] == 'confident'].copy()
df_confident['z'] = pd.to_numeric(df_confident['zlens'], errors='coerce')
df_confident = df_confident.dropna(subset=['z']).reset_index(drop=True)

# Limit to 500 lenses (or change as needed)
max_lenses = 500
df_confident = df_confident.iloc[:max_lenses]

batch_size = 25
results_lenses = []
results_randoms = []

lens_coords = SkyCoord(ra=df_confident['RA'].values*u.deg, dec=df_confident['DEC'].values*u.deg)

def random_sky_coord(exclude_coords, buffer_deg=0.5):
    while True:
        ra_rand = random.uniform(0, 360)
        dec_rand = random.uniform(-90, 90)
        coord_rand = SkyCoord(ra=ra_rand*u.deg, dec=dec_rand*u.deg)
        sep = coord_rand.separation(exclude_coords)
        if all(sep.degree > buffer_deg):
            return coord_rand

total_lenses = len(df_confident)
print(f"Running on {total_lenses} confident lenses with redshift.")

for batch_start in range(0, total_lenses, batch_size):
    batch_end = min(batch_start + batch_size, total_lenses)
    batch_df = df_confident.iloc[batch_start:batch_end]
    print(f"\nProcessing lenses {batch_start + 1} to {batch_end}")

    for idx, lens in batch_df.iterrows():
        coord = SkyCoord(ra=lens['RA']*u.deg, dec=lens['DEC']*u.deg)
        try:
            result = custom_simbad.query_region(coord, radius=20*u.arcmin)
        except Exception as e:
            print(f"Query failed for lens {lens['name']} at index {idx}: {e}")
            results_lenses.append({'lens_name': lens['name'], 'ra': lens['RA'], 'dec': lens['DEC'], 'bh_count': None})
            time.sleep(1)
            continue

        if result is None:
            bh_count = 0
        else:
            otype_col = None
            for col in result.colnames:
                if 'otype' in col.lower():
                    otype_col = col
                    break
            if otype_col is None:
                bh_count = 0
            else:
                mask = [any(bh.lower() in str(otype).lower() for bh in bh_types) for otype in result[otype_col]]
                filtered = result[mask]
                bh_count = len(filtered)

        results_lenses.append({'lens_name': lens['name'], 'ra': lens['RA'], 'dec': lens['DEC'], 'bh_count': bh_count})
        print(f"  Lens {lens['name']} - BH count: {bh_count}")
        time.sleep(1)

    for _ in range(len(batch_df)):
        coord_rand = random_sky_coord(lens_coords, buffer_deg=0.5)
        try:
            result_rand = custom_simbad.query_region(coord_rand, radius=20*u.arcmin)
        except Exception as e:
            print(f"Query failed for random coord {coord_rand.to_string('hmsdms')}: {e}")
            results_randoms.append({'random_ra': coord_rand.ra.deg, 'random_dec': coord_rand.dec.deg, 'bh_count': None})
            time.sleep(1)
            continue

        if result_rand is None:
            bh_count_rand = 0
        else:
            otype_col = None
            for col in result_rand.colnames:
                if 'otype' in col.lower():
                    otype_col = col
                    break
            if otype_col is None:
                bh_count_rand = 0
            else:
                mask = [any(bh.lower() in str(otype).lower() for bh in bh_types) for otype in result_rand[otype_col]]
                filtered = result_rand[mask]
                bh_count_rand = len(filtered)

        results_randoms.append({'random_ra': coord_rand.ra.deg, 'random_dec': coord_rand.dec.deg, 'bh_count': bh_count_rand})
        print(f"  Random sky pos RA={coord_rand.ra.deg:.3f} DEC={coord_rand.dec.deg:.3f} - BH count: {bh_count_rand}")
        time.sleep(1)

    # Save intermediate results
    pd.DataFrame(results_lenses).to_csv(drive_folder + 'lens_bh_counts_partial.csv', index=False)
    pd.DataFrame(results_randoms).to_csv(drive_folder + 'random_bh_counts_partial.csv', index=False)
    print("Saved partial results to Google Drive.")

# Convert to DataFrame for analysis
df_lenses = pd.DataFrame(results_lenses).dropna(subset=['bh_count'])
df_randoms = pd.DataFrame(results_randoms).dropna(subset=['bh_count'])

# KS test
ks_stat, ks_p = ks_2samp(df_lenses['bh_count'], df_randoms['bh_count'])
print(f"\nKS test comparing BH counts lens vs random:")
print(f"  KS statistic = {ks_stat:.4f}, p-value = {ks_p:.4e}")

# Poisson test (compare mean counts and zeros)
mean_lens = df_lenses['bh_count'].mean()
mean_random = df_randoms['bh_count'].mean()

zero_lens = (df_lenses['bh_count'] == 0).sum()
zero_random = (df_randoms['bh_count'] == 0).sum()

print(f"\nPoisson stats:")
print(f"  Mean BH count near lenses = {mean_lens:.2f}")
print(f"  Mean BH count near random = {mean_random:.2f}")
print(f"  Lenses with zero BH = {zero_lens} / {len(df_lenses)} ({zero_lens/len(df_lenses):.1%})")
print(f"  Randoms with zero BH = {zero_random} / {len(df_randoms)} ({zero_random/len(df_randoms):.1%})")

import os
import time
import random
import numpy as np
import pandas as pd
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.simbad import Simbad
from lenscat import catalog
from scipy.stats import ks_2samp

# Setup Google Drive folder
drive_folder = '/content/drive/MyDrive/lens_bh_analysis/'
if not os.path.exists(drive_folder):
    os.makedirs(drive_folder)

custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.add_votable_fields('otype', 'ra', 'dec', 'z_value')

bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

df = catalog.to_pandas()
df_confident = df[df['grading'] == 'confident'].copy()
df_confident['z'] = pd.to_numeric(df_confident['zlens'], errors='coerce')
df_confident = df_confident.dropna(subset=['z']).reset_index(drop=True)

max_lenses = 500
df_confident = df_confident.iloc[:max_lenses]

batch_size = 25
results_lenses = []
results_randoms = []

lens_coords = SkyCoord(ra=df_confident['RA'].values*u.deg, dec=df_confident['DEC'].values*u.deg)
lens_decs = df_confident['DEC'].values

def random_sky_coord_matched_dec(exclude_coords, lens_decs, buffer_deg=0.5):
    while True:
        ra_rand = random.uniform(0, 360)
        dec_rand = np.random.choice(lens_decs)
        coord_rand = SkyCoord(ra=ra_rand*u.deg, dec=dec_rand*u.deg)
        sep = coord_rand.separation(exclude_coords)
        if all(sep.degree > buffer_deg):
            return coord_rand

def filter_result_by_redshift(result, z_lens, delta_z=0.05):
    if result is None or 'z_value' not in result.colnames:
        return None
    mask = []
    for z in result['z_value']:
        try:
            if z is not None and abs(z - z_lens) <= delta_z:
                mask.append(True)
            else:
                mask.append(False)
        except Exception:
            mask.append(False)
    filtered = result[mask]
    return filtered

total_lenses = len(df_confident)
print(f"Running on {total_lenses} confident lenses with redshift.")

for batch_start in range(0, total_lenses, batch_size):
    batch_end = min(batch_start + batch_size, total_lenses)
    batch_df = df_confident.iloc[batch_start:batch_end]
    print(f"\nProcessing lenses {batch_start + 1} to {batch_end}")

    # Query around lenses
    for idx, lens in batch_df.iterrows():
        coord = SkyCoord(ra=lens['RA']*u.deg, dec=lens['DEC']*u.deg)
        z_lens = lens['z']
        try:
            result = custom_simbad.query_region(coord, radius=20*u.arcmin)
        except Exception as e:
            print(f"Query failed for lens {lens['name']} at index {idx}: {e}")
            results_lenses.append({'lens_name': lens['name'], 'ra': lens['RA'], 'dec': lens['DEC'], 'z': z_lens, 'bh_count': None})
            time.sleep(1)
            continue

        filtered_result = filter_result_by_redshift(result, z_lens, delta_z=0.05)

        if filtered_result is None or len(filtered_result) == 0:
            bh_count = 0
        else:
            otype_col = None
            for col in filtered_result.colnames:
                if 'otype' in col.lower():
                    otype_col = col
                    break
            if otype_col is None:
                bh_count = 0
            else:
                mask = [any(bh.lower() in str(otype).lower() for bh in bh_types) for otype in filtered_result[otype_col]]
                filtered_bh = filtered_result[mask]
                bh_count = len(filtered_bh)

        results_lenses.append({'lens_name': lens['name'], 'ra': lens['RA'], 'dec': lens['DEC'], 'z': z_lens, 'bh_count': bh_count})
        print(f"  Lens {lens['name']} - BH count (z filtered): {bh_count}")
        time.sleep(1)

    # Query around random coords (matched Dec)
    for _ in range(len(batch_df)):
        coord_rand = random_sky_coord_matched_dec(lens_coords, lens_decs, buffer_deg=0.5)
        try:
            result_rand = custom_simbad.query_region(coord_rand, radius=20*u.arcmin)
        except Exception as e:
            print(f"Query failed for random coord {coord_rand.to_string('hmsdms')}: {e}")
            results_randoms.append({'random_ra': coord_rand.ra.deg, 'random_dec': coord_rand.dec.deg, 'bh_count': None})
            time.sleep(1)
            continue

        if result_rand is None:
            bh_count_rand = 0
        else:
            otype_col = None
            for col in result_rand.colnames:
                if 'otype' in col.lower():
                    otype_col = col
                    break
            if otype_col is None:
                bh_count_rand = 0
            else:
                mask = [any(bh.lower() in str(otype).lower() for bh in bh_types) for otype in result_rand[otype_col]]
                filtered_bh = result_rand[mask]
                bh_count_rand = len(filtered_bh)

        results_randoms.append({'random_ra': coord_rand.ra.deg, 'random_dec': coord_rand.dec.deg, 'bh_count': bh_count_rand})
        print(f"  Random sky pos RA={coord_rand.ra.deg:.3f} DEC={coord_rand.dec.deg:.3f} - BH count: {bh_count_rand}")
        time.sleep(1)

    # Print batch stats:
    df_batch_lenses = pd.DataFrame(results_lenses).iloc[batch_start:batch_end]
    df_batch_randoms = pd.DataFrame(results_randoms).iloc[batch_start:batch_end]

    mean_lens_batch = df_batch_lenses['bh_count'].dropna().mean()
    mean_random_batch = df_batch_randoms['bh_count'].dropna().mean()

    zero_lens_batch = (df_batch_lenses['bh_count'] == 0).sum()
    zero_random_batch = (df_batch_randoms['bh_count'] == 0).sum()

    print(f"Batch BH stats (lenses vs random):")
    print(f"  Mean BH per lens: {mean_lens_batch:.2f}")
    print(f"  Mean BH per random field: {mean_random_batch:.2f}")
    print(f"  Lenses with zero BH: {zero_lens_batch} / {len(df_batch_lenses)}")
    print(f"  Randoms with zero BH: {zero_random_batch} / {len(df_batch_randoms)}")

    # Save partial results after each batch
    pd.DataFrame(results_lenses).to_csv(drive_folder + 'lens_bh_counts_partial.csv', index=False)
    pd.DataFrame(results_randoms).to_csv(drive_folder + 'random_bh_counts_partial.csv', index=False)
    print("Saved partial results to Google Drive.")

# After all batches: final stats and save

df_lenses = pd.DataFrame(results_lenses).dropna(subset=['bh_count'])
df_randoms = pd.DataFrame(results_randoms).dropna(subset=['bh_count'])

ks_stat, ks_p = ks_2samp(df_lenses['bh_count'], df_randoms['bh_count'])
print(f"\nKS test comparing BH counts lens vs random:")
print(f"  KS statistic = {ks_stat:.4f}, p-value = {ks_p:.4e}")

mean_lens = df_lenses['bh_count'].mean()
mean_random = df_randoms['bh_count'].mean()
zero_lens = (df_lenses['bh_count'] == 0).sum()
zero_random = (df_randoms['bh_count'] == 0).sum()

print(f"\nPoisson stats:")
print(f"  Mean BH count near lenses = {mean_lens:.2f}")
print(f"  Mean BH count near random = {mean_random:.2f}")
print(f"  Lenses with zero BH = {zero_lens} / {len(df_lenses)} ({zero_lens/len(df_lenses):.1%})")
print(f"  Randoms with zero BH = {zero_random} / {len(df_randoms)} ({zero_random/len(df_randoms):.1%})")

pd.DataFrame(results_lenses).to_csv(drive_folder + 'lens_bh_counts_final.csv', index=False)
pd.DataFrame(results_randoms).to_csv(drive_folder + 'random_bh_counts_final.csv', index=False)
print("Saved final results to Google Drive.")

print("Redshifts found around lens:", [z for z in result['z_value'] if z is not None])

from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u

# Setup SIMBAD with custom fields
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60

# Clear default fields safely
for f in custom_simbad.get_votable_fields():
    custom_simbad.remove_votable_fields(f)

# Add only fields you want
custom_simbad.add_votable_fields('otype', 'ra', 'dec', 'z_value')

# Example coordinate
coord = SkyCoord(ra=12.6158*u.deg, dec=-17.6693*u.deg)

result = custom_simbad.query_region(coord, radius=20*u.arcmin)

print("Columns returned by SIMBAD:", result.colnames)

if 'z_value' in result.colnames:
    zvals = [z for z in result['z_value'] if z is not None]
    print("Redshifts found:", zvals)
else:
    print("No redshift data found in SIMBAD results for this region.")

# Example redshift filtering function
def filter_result_by_redshift(result, z_lens, delta_z=0.05):
    if 'z_value' not in result.colnames:
        # No redshift info, skip filtering
        return result
    mask = []
    for z in result['z_value']:
        try:
            if z is not None and abs(z - z_lens) <= delta_z:
                mask.append(True)
            else:
                mask.append(False)
        except Exception:
            mask.append(False)
    return result[mask]

# Use as:
# filtered_result = filter_result_by_redshift(result, lens_redshift, delta_z=0.05)

pip install astropy astroquery lenscat tqdm

# Imports
import pandas as pd
import numpy as np
from lenscat import load_lenscat
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord, Angle
from astropy import units as u
from astropy.table import Table
import random, os
from tqdm import tqdm
from scipy.stats import ks_2samp, poisson
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')
drive_folder = '/content/drive/MyDrive/lens_bh_analysis/'
os.makedirs(drive_folder, exist_ok=True)

# SIMBAD setup
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.remove_votable_fields(*custom_simbad.get_votable_fields())
custom_simbad.add_votable_fields('otype', 'ra', 'dec', 'z_value')

# BH-related object types
bh_types = {"BH", "BHXRB", "XRB", "BLLac", "Blazar", "AGN", "QSO"}

# Load lens data
df = load_lenscat()
df = df[df['grade'].isin(['confident', 'probable']) & df['z'] > 0].reset_index(drop=True)
print(f"Loaded {len(df)} lenses with redshift.")

# Parameters
radius = 20 * u.arcmin
N_lenses = 500
lens_sample = df.sample(N_lenses, random_state=42)

# Results
results_lenses = []
results_randoms = []

# Query function
def query_bhs_near(coord):
    try:
        result = custom_simbad.query_region(coord, radius=radius)
        if result is None:
            return []
        result = result.to_pandas()
        result = result[result['OTYPE'].str.upper().isin(bh_types)]
        result = result[result['Z_VALUE'].notnull()]
        return list(SkyCoord(result['RA'], result['DEC'], unit=(u.hourangle, u.deg)))
    except Exception as e:
        print("SIMBAD error:", e)
        return []

# Main loop
for i, row in tqdm(lens_sample.iterrows(), total=len(lens_sample)):
    lens_coord = SkyCoord(row['ra'], row['dec'], unit='deg')
    gal_lat = lens_coord.galactic.b.deg

    # Lens field
    bh_coords = query_bhs_near(lens_coord)
    results_lenses.append({
        'lens_id': row['id'],
        'ra': lens_coord.ra.deg,
        'dec': lens_coord.dec.deg,
        'z': row['z'],
        'bh_count': len(bh_coords),
        'bh_coords': bh_coords
    })

    # Random field
    while True:
        rand_ra = random.uniform(0, 360)
        rand_dec = random.uniform(-90, 90)
        rand_coord = SkyCoord(ra=rand_ra*u.deg, dec=rand_dec*u.deg)
        if abs(rand_coord.galactic.b.deg - gal_lat) < 2:
            break
    rand_bh_coords = query_bhs_near(rand_coord)
    results_randoms.append({
        'rand_ra': rand_coord.ra.deg,
        'rand_dec': rand_coord.dec.deg,
        'bh_count': len(rand_bh_coords),
        'bh_coords': rand_bh_coords
    })

    # Print interim stats every 25
    if (i+1) % 25 == 0:
        lens_counts = [r['bh_count'] for r in results_lenses]
        rand_counts = [r['bh_count'] for r in results_randoms]
        print(f"\n--- Batch {i+1} ---")
        print(f"Lenses mean BH count: {np.mean(lens_counts):.2f}")
        print(f"Randoms mean BH count: {np.mean(rand_counts):.2f}")
        ks = ks_2samp(lens_counts, rand_counts)
        poisson_p = poisson.sf(np.mean(lens_counts)-1, mu=np.mean(rand_counts))
        print(f"KS test p-value: {ks.pvalue:.4f}")
        print(f"Poisson p-value: {poisson_p:.4e}")

        # Save partial
        pd.DataFrame(results_lenses).to_pickle(drive_folder + 'lens_bh_counts_partial.pkl')
        pd.DataFrame(results_randoms).to_pickle(drive_folder + 'random_bh_counts_partial.pkl')

# Final stats
lens_counts = [r['bh_count'] for r in results_lenses]
rand_counts = [r['bh_count'] for r in results_randoms]
print("\n=== FINAL RESULTS ===")
print(f"Lenses mean BH count: {np.mean(lens_counts):.2f}")
print(f"Randoms mean BH count: {np.mean(rand_counts):.2f}")
ks = ks_2samp(lens_counts, rand_counts)
poisson_p = poisson.sf(np.mean(lens_counts)-1, mu=np.mean(rand_counts))
print(f"KS test p-value: {ks.pvalue:.4f}")
print(f"Poisson p-value: {poisson_p:.4e}")

# Save final results
pd.DataFrame(results_lenses).to_pickle(drive_folder + 'lens_bh_counts.pkl')
pd.DataFrame(results_randoms).to_pickle(drive_folder + 'random_bh_counts.pkl')

!pip install git+https://github.com/mikeastro/lenscat.git --upgrade

# Configure SIMBAD
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.remove_votable_fields()
custom_simbad.add_votable_fields("otype", "ra", "dec", "z_value")

# Load lens catalog
df = load_lenscat()
df = df[df['grade'].isin(['confident']) & df['z'].notnull()].reset_index(drop=True)
df_sample = df.sample(500, random_state=42)

lens_coords = SkyCoord(ra=df_sample['ra'].values * u.deg, dec=df_sample['dec'].values * u.deg)
lens_redshifts = df_sample['z'].values
radius = 20 * u.arcmin

bh_types = {'BH', 'BH?','BHXRB','XRB','BLAZAR','AGN','QSO'}

results_lenses = []
results_randoms = []

for i, (lens, z_lens) in enumerate(zip(lens_coords, lens_redshifts)):
    print(f"[{i+1}/500] Lens at RA: {lens.ra.deg:.4f}, Dec: {lens.dec.deg:.4f}, z: {z_lens}")

    # --- Lens Field ---
    result = custom_simbad.query_region(lens, radius=radius)
    bh_count = 0
    coords = []
    zs = []
    if result:
        for row in result:
            if row['OTYPE'] in bh_types:
                ra = row['RA']
                dec = row['DEC']
                coord = SkyCoord(ra + ' ' + dec, unit=(u.hourangle, u.deg))
                coords.append(coord)
                zs.append(row['Z_VALUE'] if row['Z_VALUE'] else None)
                bh_count += 1
    results_lenses.append({
        'lens_ra': lens.ra.deg,
        'lens_dec': lens.dec.deg,
        'z_lens': z_lens,
        'bh_count': bh_count,
        'bh_coords': coords,
        'bh_z': zs
    })

    # --- Random Field at Same Latitude, Different Longitude ---
    rand_lon = random.uniform(0, 360)
    rand_coord = SkyCoord(ra=rand_lon * u.deg, dec=lens.dec, frame='icrs')
    result_r = custom_simbad.query_region(rand_coord, radius=radius)
    bh_count_r = 0
    coords_r = []
    zs_r = []
    if result_r:
        for row in result_r:
            if row['OTYPE'] in bh_types:
                ra = row['RA']
                dec = row['DEC']
                coord = SkyCoord(ra + ' ' + dec, unit=(u.hourangle, u.deg))
                coords_r.append(coord)
                zs_r.append(row['Z_VALUE'] if row['Z_VALUE'] else None)
                bh_count_r += 1
    results_randoms.append({
        'rand_ra': rand_coord.ra.deg,
        'rand_dec': rand_coord.dec.deg,
        'bh_count': bh_count_r,
        'bh_coords': coords_r,
        'bh_z': zs_r
    })

    if i % 50 == 0:
        pd.DataFrame(results_lenses).to_pickle(drive_folder + 'lens_results_partial.pkl')
        pd.DataFrame(results_randoms).to_pickle(drive_folder + 'random_results_partial.pkl')
        print("Saved intermediate results to Google Drive.")

# Final save
pd.DataFrame(results_lenses).to_pickle(drive_folder + 'lens_results_500.pkl')
pd.DataFrame(results_randoms).to_pickle(drive_folder + 'random_results_500.pkl')
print("â Done. Results saved.")

# Summary Stats
lens_mean = np.mean([r['bh_count'] for r in results_lenses])
rand_mean = np.mean([r['bh_count'] for r in results_randoms])
print(f"\nð Lens mean BH count: {lens_mean:.2f}")
print(f"ð Random mean BH count: {rand_mean:.2f}")

from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u
import pandas as pd

# Configure SIMBAD with redshift + position + object type
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.reset_votable_fields()
custom_simbad.add_votable_fields("ra", "dec", "otype", "z_value")

# Example lens coordinate (replace with your own RA/DEC if needed)
lens_coord = SkyCoord(ra=150.0625, dec=2.1794, unit="deg", frame="icrs")

# Search within 10 arcmin
result = custom_simbad.query_region(lens_coord, radius=10 * u.arcmin)

# Convert to DataFrame and show result
if result is not None:
    df = result.to_pandas()
    print("Objects found:", len(df))
    print(df[["MAIN_ID", "RA", "DEC", "OTYPE", "Z_VALUE"]].head())
else:
    print("No objects found near the lens.")

import pandas as pd
import numpy as np
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u
import time
import os

# 1. Load your lens catalog (example here â replace with lenscat if needed)
lens_df = pd.read_csv("your_lens_catalog.csv")  # Must have 'ra', 'dec' columns

# 2. Set SIMBAD with correct fields
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.reset_votable_fields()
custom_simbad.add_votable_fields("ra", "dec", "otype", "rvz_redshift")

# 3. Set output directory
output_dir = "simbad_lens_batches"
os.makedirs(output_dir, exist_ok=True)

# 4. Filter BH-type object names
bh_types = ["BH", "BH?","BLAZAR", "AGN", "QSO", "XRB", "BHXRB"]

# 5. Process in batches
batch_size = 50
num_batches = int(np.ceil(len(lens_df) / batch_size))

for batch_num in range(num_batches):
    start = batch_num * batch_size
    end = min((batch_num + 1) * batch_size, len(lens_df))
    print(f"\nð Processing batch {batch_num + 1}/{num_batches} (lenses {start}â{end})")

    lens_batch = lens_df.iloc[start:end]
    results = []

    for i, row in lens_batch.iterrows():
        try:
            coord = SkyCoord(ra=row["ra"], dec=row["dec"], unit="deg", frame="icrs")
            simbad_result = custom_simbad.query_region(coord, radius=10 * u.arcmin)

            if simbad_result is not None:
                df = simbad_result.to_pandas()
                df = df.rename(columns={
                    'RA': 'RA',
                    'DEC': 'DEC',
                    'MAIN_ID': 'MAIN_ID',
                    'OTYPE': 'OTYPE',
                    'RVZ_REDSHIFT': 'REDSHIFT'
                })
                df["lens_ra"] = row["ra"]
                df["lens_dec"] = row["dec"]
                df["lens_index"] = i

                df_filtered = df[df["OTYPE"].isin(bh_types)].copy()
                results.append(df_filtered)
            else:
                print(f"  â No SIMBAD objects near lens {i}")

        except Exception as e:
            print(f"  â ï¸ Error at lens {i}: {e}")
        time.sleep(1)  # Be gentle with SIMBAD

    # Save this batch
    if results:
        full_df = pd.concat(results, ignore_index=True)
        out_path = os.path.join(output_dir, f"simbad_batch_{batch_num+1:02}.csv")
        full_df.to_csv(out_path, index=False)
        print(f"â Saved batch {batch_num+1} to {out_path}")
    else:
        print("â ï¸ No BH-type objects found in this batch.")

print("\nð All batches complete.")

import pandas as pd
import numpy as np
from lenscat import lenscat
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u
import os
import time

# Set up SIMBAD with safe fields only
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.remove_votable_fields()
custom_simbad.add_votable_fields("otype", "ra", "dec")  # exclude z_value for now

# Load lens catalog: confident + probable
cat = lenscat.load()
lens_df = cat[(cat['grade'] == 'confident') | (cat['grade'] == 'probable')].copy()
lens_df = lens_df.reset_index(drop=True)
print(f"Loaded {len(lens_df)} strong lenses.")

# Output dir
output_dir = "/content/drive/MyDrive/lens_bh_results_clean"
os.makedirs(output_dir, exist_ok=True)

# Settings
radius_arcmin = 20
batch_size = 50

# Query function
def query_simbad_objects(ra, dec, radius_arcmin=20):
    coord = SkyCoord(ra=ra*u.deg, dec=dec*u.deg)
    try:
        result = custom_simbad.query_region(coord, radius=radius_arcmin * u.arcmin)
        if result is not None:
            df = result.to_pandas()
            df['lens_ra'] = ra
            df['lens_dec'] = dec
            return df
    except Exception as e:
        print(f"Query failed at RA={ra}, DEC={dec}: {e}")
    return pd.DataFrame()

# Run batches
for i in range(0, len(lens_df), batch_size):
    batch = lens_df.iloc[i:i+batch_size]
    all_results = []

    print(f"\nâ³ Processing batch {i // batch_size + 1} / {len(lens_df) // batch_size + 1}")
    for idx, row in batch.iterrows():
        ra, dec = row['ra'], row['dec']
        result_df = query_simbad_objects(ra, dec, radius_arcmin)
        if not result_df.empty:
            all_results.append(result_df)
        time.sleep(1.5)  # throttle to avoid SIMBAD ban

    # Save
    if all_results:
        full_df = pd.concat(all_results, ignore_index=True)
        outpath = f"{output_dir}/batch_{i//batch_size+1:03}.csv"
        full_df.to_csv(outpath, index=False)
        print(f"â Saved {len(full_df)} rows to {outpath}")
    else:
        print("â ï¸ No objects found in this batch.")

!pip install -q lenscat astroquery astropy pandas

import lenscat
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u
import pandas as pd

cat = lenscat.load()

print(f"Total lenses loaded: {len(cat)}")

# Filter confident lenses
confident = cat[cat['grade'] == 'confident']

print(f"Confident lenses count: {len(confident)}")

# Pick one lens coordinate
ra = confident.iloc[0]['ra']
dec = confident.iloc[0]['dec']

print(f"Querying SIMBAD around RA={ra}, DEC={dec}")

custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.remove_votable_fields()
custom_simbad.add_votable_fields('ra', 'dec', 'otype', 'rvz_redshift')

coord = SkyCoord(ra=ra*u.deg, dec=dec*u.deg)
result = custom_simbad.query_region(coord, radius=10*u.arcmin)

if result is not None:
    df = result.to_pandas()
    df.columns = [c.upper() for c in df.columns]
    print(df[['MAIN_ID', 'RA', 'DEC', 'OTYPE', 'RVZ_REDSHIFT']].head())
else:
    print("No SIMBAD objects found near lens.")

!pip install -q lenscat astroquery astropy pandas

from lenscat import catalog
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u
import pandas as pd

cat = catalog.to_pandas()

print(f"Total lenses loaded: {len(cat)}")

confident = cat[cat['grading'] == 'confident']

print(f"Confident lenses count: {len(confident)}")

ra = confident.iloc[0]['ra']
dec = confident.iloc[0]['dec']

print(f"Querying SIMBAD around RA={ra}, DEC={dec}")

custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.remove_votable_fields()
custom_simbad.add_votable_fields('ra', 'dec', 'otype', 'rvz_redshift')

coord = SkyCoord(ra=ra*u.deg, dec=dec*u.deg)
result = custom_simbad.query_region(coord, radius=10*u.arcmin)

if result is not None:
    df = result.to_pandas()
    df.columns = [c.upper() for c in df.columns]
    print(df[['MAIN_ID', 'RA', 'DEC', 'OTYPE', 'RVZ_REDSHIFT']].head())
else:
    print("No SIMBAD objects found near lens.")



# Install dependencies (uncomment if needed)
# !pip install -q lenscat astroquery astropy pandas

import lenscat
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u
import pandas as pd
import time
import os

# Load catalog and lowercase columns for easy access
cat = lenscat.catalog.to_pandas()
cat.columns = [c.lower() for c in cat.columns]

# Filter confident lenses with valid redshift
confident = cat[(cat['grading'] == 'confident')].copy()
confident['zlens'] = pd.to_numeric(confident['zlens'], errors='coerce')
confident = confident.dropna(subset=['zlens']).reset_index(drop=True)

print(f"Total lenses loaded: {len(cat)}")
print(f"Confident lenses with redshift: {len(confident)}")

# Setup SIMBAD query
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.remove_votable_fields()
custom_simbad.add_votable_fields('ra', 'dec', 'otype', 'rvz_redshift')

# BH-like types to filter on (case-insensitive)
bh_types = ['BH', 'QSO', 'AGN', 'BLAZAR', 'XRB', 'LMXB', 'HMXB']

# Output folder (adjust your path accordingly)
output_dir = "/content/drive/MyDrive/lens_bh_results"
os.makedirs(output_dir, exist_ok=True)

batch_size = 25
radius_arcmin = 20

results = []

for start in range(0, len(confident), batch_size):
    batch = confident.iloc[start:start+batch_size]
    print(f"\nProcessing batch {start//batch_size + 1} with {len(batch)} lenses")

    for idx, row in batch.iterrows():
        ra = row['ra']
        dec = row['dec']
        coord = SkyCoord(ra=ra*u.deg, dec=dec*u.deg)

        try:
            result = custom_simbad.query_region(coord, radius=radius_arcmin*u.arcmin)
        except Exception as e:
            print(f"SIMBAD query failed for lens {row['name']} at RA={ra}, DEC={dec}: {e}")
            continue

        if result is None:
            count = 0
        else:
            df = result.to_pandas()
            # SIMBAD columns are uppercase by default; normalize
            df.columns = [c.lower() for c in df.columns]
            # Filter BH types case-insensitive
            mask = df['otype'].str.upper().isin(bh_types)
            count = mask.sum()

        results.append({
            'lens_name': row['name'],
            'ra': ra,
            'dec': dec,
            'zlens': row['zlens'],
            'bh_count': count
        })

        print(f"Lens {row['name']}: found {count} BH-like objects")

        time.sleep(1.5)  # prevent hitting SIMBAD too fast

    # Save batch results
    batch_num = start // batch_size + 1
    batch_df = pd.DataFrame(results)
    batch_df.to_csv(f"{output_dir}/bh_counts_batch_{batch_num:03}.csv", index=False)
    print(f"Saved batch {batch_num} results.")

print("\nAll done.")

# Install dependencies (uncomment if needed)
# !pip install -q lenscat astroquery astropy pandas

import lenscat
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u
import pandas as pd
import time
import os

# Load catalog and lowercase columns for easy access
cat = lenscat.catalog.to_pandas()
cat.columns = [c.lower() for c in cat.columns]

# Filter confident lenses with valid redshift
confident = cat[(cat['grading'] == 'confident')].copy()
confident['zlens'] = pd.to_numeric(confident['zlens'], errors='coerce')
confident = confident.dropna(subset=['zlens']).reset_index(drop=True)

print(f"Total lenses loaded: {len(cat)}")
print(f"Confident lenses with redshift: {len(confident)}")

# Setup SIMBAD query
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.remove_votable_fields()
custom_simbad.add_votable_fields('ra', 'dec', 'otype', 'rvz_redshift')

# BH-like types to filter on (case-insensitive)
bh_types = ['BH', 'QSO', 'AGN', 'BLAZAR', 'XRB', 'LMXB', 'HMXB']

# Output folder (adjust your path accordingly)
output_dir = "/content/drive/MyDrive/lens_bh_results"
os.makedirs(output_dir, exist_ok=True)

batch_size = 25
radius_arcmin = 20

results = []

for start in range(0, len(confident), batch_size):
    batch = confident.iloc[start:start+batch_size]
    print(f"\nProcessing batch {start//batch_size + 1} with {len(batch)} lenses")

    for idx, row in batch.iterrows():
        ra = row['ra']
        dec = row['dec']
        coord = SkyCoord(ra=ra*u.deg, dec=dec*u.deg)

        try:
            result = custom_simbad.query_region(coord, radius=radius_arcmin*u.arcmin)
        except Exception as e:
            print(f"SIMBAD query failed for lens {row['name']} at RA={ra}, DEC={dec}: {e}")
            continue

        if result is None:
            count = 0
        else:
            df = result.to_pandas()
            # SIMBAD columns are uppercase by default; normalize
            df.columns = [c.lower() for c in df.columns]
            # Filter BH types case-insensitive
            mask = df['otype'].str.upper().isin(bh_types)
            count = mask.sum()

        results.append({
            'lens_name': row['name'],
            'ra': ra,
            'dec': dec,
            'zlens': row['zlens'],
            'bh_count': count
        })

        print(f"Lens {row['name']}: found {count} BH-like objects")

        time.sleep(1.5)  # prevent hitting SIMBAD too fast

    # Save batch results
    batch_num = start // batch_size + 1
    batch_df = pd.DataFrame(results)
    batch_df.to_csv(f"{output_dir}/bh_counts_batch_{batch_num:03}.csv", index=False)
    print(f"Saved batch {batch_num} results.")

print("\nAll done.")

import pandas as pd


progress_file = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_progress.csv'
df = pd.read_csv(progress_file)


# Convert surface density units: M_sun/Mpc^2 to M_sun/kpc^2
df['mass_surface_density_kpc2'] = df['mass_surface_density_Msun_per_Mpc2'] / 1e6


# Filter out zero surface density lenses
non_zero_df = df[df['mass_surface_density_kpc2'] > 0].copy()


cdm_threshold = 1e8  # M_sun/kpc^2


f_star_values = [0.01, 0.03, 0.05, 0.1, 0.2]


total_lenses = len(non_zero_df)
print(f"Total lenses with non-zero stellar surface density: {total_lenses}\n")


for f_star in f_star_values:
   inferred_total = non_zero_df['mass_surface_density_kpc2'] / f_star
   below_threshold = inferred_total < cdm_threshold
   n_below = below_threshold.sum()
   frac_below = n_below / total_lenses * 100
   print(f"f* = {f_star:.2f} | {n_below}/{total_lenses} lenses ({frac_below:.1f}%) below CDM threshold")



print(cat.columns)

import pandas as pd


progress_file = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_progress.csv'
df = pd.read_csv(progress_file)


# Convert surface density units: M_sun/Mpc^2 to M_sun/kpc^2
df['mass_surface_density_kpc2'] = df['mass_surface_density_Msun_per_Mpc2'] / 1e6


# Filter out zero surface density lenses
non_zero_df = df[df['mass_surface_density_kpc2'] > 0].copy()


cdm_threshold = 1e8  # M_sun/kpc^2


f_star_values = [0.01, 0.03, 0.05, 0.1, 0.2]


total_lenses = len(non_zero_df)
print(f"Total lenses with non-zero stellar surface density: {total_lenses}\n")


for f_star in f_star_values:
   inferred_total = non_zero_df['mass_surface_density_kpc2'] / f_star
   below_threshold = inferred_total < cdm_threshold
   n_below = below_threshold.sum()
   frac_below = n_below / total_lenses * 100
   print(f"f* = {f_star:.2f} | {n_below}/{total_lenses} lenses ({frac_below:.1f}%) below CDM threshold")

import pandas as pd

progress_file = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_progress.csv'
df = pd.read_csv(progress_file)

# Check the first few rows and columns for sanity
print(df.head())
print("Columns:", df.columns)

# Convert surface density units: M_sun/Mpc^2 to M_sun/kpc^2
df['mass_surface_density_kpc2'] = df['mass_surface_density_Msun_per_Mpc2'] / 1e6

# Filter out zero or negative surface density lenses
non_zero_df = df[df['mass_surface_density_kpc2'] > 0].copy()

cdm_threshold = 1e8  # Threshold in M_sun/kpc^2

f_star_values = [0.01, 0.03, 0.05, 0.1, 0.2]

total_lenses = len(non_zero_df)
print(f"Total lenses with non-zero stellar surface density: {total_lenses}\n")

for f_star in f_star_values:
    inferred_total = non_zero_df['mass_surface_density_kpc2'] / f_star
    below_threshold = inferred_total < cdm_threshold
    n_below = below_threshold.sum()
    frac_below = (n_below / total_lenses) * 100
    print(f"f* = {f_star:.2f} | {n_below}/{total_lenses} lenses ({frac_below:.1f}%) below CDM threshold")

import os
import pandas as pd

folder = '/content/drive/MyDrive/lens_stellar_mass_results/'
all_files = [folder + f for f in os.listdir(folder) if f.endswith('.csv')]

df_list = [pd.read_csv(f) for f in all_files]
full_df = pd.concat(df_list, ignore_index=True)

print(f"Combined total rows: {len(full_df)}")
print(full_df.head())

# Then run your analysis on full_df instead of just the small partial file

import lenscat

pip install lenscat

# Install needed packages (only run once per Colab session)
!pip install astroquery astropy pandas

import os
import pandas as pd

# 1. Folder where your partial CSV files are saved
folder = '/content/drive/MyDrive/lens_stellar_mass_results/'

# 2. Find all CSV files in that folder
all_files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.csv')]
print(f"Found {len(all_files)} CSV files.")

# 3. Load and combine all CSVs into one DataFrame
df_list = [pd.read_csv(f) for f in all_files]
full_df = pd.concat(df_list, ignore_index=True)
print(f"Combined total rows: {len(full_df)}")

# 4. Convert surface density units: M_sun/Mpc^2 to M_sun/kpc^2
full_df['surface_density_kpc2'] = full_df['mass_surface_density_Msun_per_Mpc2'] / 1e6

# 5. Filter out lenses with zero or negative stellar surface density
non_zero_df = full_df[full_df['surface_density_kpc2'] > 0].copy()
total_lenses = len(non_zero_df)
print(f"Total lenses with non-zero stellar surface density: {total_lenses}\n")

# 6. Define your CDM threshold and stellar mass fractions to test
cdm_threshold = 1e8  # M_sun/kpc^2
f_star_values = [0.01, 0.03, 0.05, 0.1, 0.2]

# 7. Run the threshold analysis
for f_star in f_star_values:
    inferred_total = non_zero_df['surface_density_kpc2'] / f_star
    below_threshold = inferred_total < cdm_threshold
    n_below = below_threshold.sum()
    frac_below = (n_below / total_lenses) * 100
    print(f"f* = {f_star:.2f} | {n_below}/{total_lenses} lenses ({frac_below:.1f}%) below CDM threshold")

import numpy as np
import pandas as pd

# Use finer steps in f_star between 0.05 and 0.1
f_star_values = [0.01, 0.03] + list(np.arange(0.05, 0.11, 0.01)) + [0.2]

results = []

for f_star in f_star_values:
    inferred_total = non_zero_df['surface_density_kpc2'] / f_star
    below_threshold = inferred_total < cdm_threshold
    n_below = below_threshold.sum()
    frac_below = (n_below / total_lenses) * 100
    results.append({
        'f_star': f_star,
        'Lenses_Below_Threshold': n_below,
        'Total_Lenses': total_lenses,
        'Percent_Below_Threshold': frac_below
    })

summary_df = pd.DataFrame(results)
print(summary_df)

import pandas as pd
import numpy as np

# Assuming you already have your dataframe 'non_zero_df' with column 'mass_surface_density_kpc2' and total_lenses = 146

cdm_threshold = 1e8  # Msun/kpc^2

# Create finer f_star range between 0.05 and 0.1 with steps of 0.01 plus existing values
f_star_values = np.concatenate((
    np.array([0.01, 0.03, 0.05]),
    np.arange(0.06, 0.11, 0.01),
    np.array([0.20])
))

results = []

total_lenses = len(non_zero_df)

for f_star in f_star_values:
    inferred_total = non_zero_df['mass_surface_density_kpc2'] / f_star
    below_threshold = inferred_total < cdm_threshold
    n_below = below_threshold.sum()
    pct_below = (n_below / total_lenses) * 100
    results.append({
        'f_star': round(f_star, 2),
        'Lenses_Below_Threshold': n_below,
        'Total_Lenses': total_lenses,
        'Percent_Below_Threshold': round(pct_below, 1)
    })

df_results = pd.DataFrame(results)
print(df_results.to_string(index=False))

import pandas as pd
import numpy as np

# Load your combined data CSV (adjust path as needed)
df = pd.read_csv('/content/drive/MyDrive/lens_stellar_mass_results/combined_lens_data.csv')

# Convert units M_sun/Mpc^2 to M_sun/kpc^2
df['mass_surface_density_kpc2'] = df['mass_surface_density_Msun_per_Mpc2'] / 1e6

# Filter out zero or NaN surface densities
non_zero_df = df[df['mass_surface_density_kpc2'] > 0].copy()

cdm_threshold = 1e8  # Msun/kpc^2

# Create f_star values including 0.06 to 0.10 steps
f_star_values = np.concatenate((
    np.array([0.01, 0.03, 0.05]),
    np.arange(0.06, 0.11, 0.01),
    np.array([0.20])
))

total_lenses = len(non_zero_df)

results = []
for f_star in f_star_values:
    inferred_total = non_zero_df['mass_surface_density_kpc2'] / f_star
    below_threshold = inferred_total < cdm_threshold
    n_below = below_threshold.sum()
    pct_below = (n_below / total_lenses) * 100
    results.append({
        'f_star': round(f_star, 2),
        'Lenses_Below_Threshold': n_below,
        'Total_Lenses': total_lenses,
        'Percent_Below_Threshold': round(pct_below, 1)
    })

df_results = pd.DataFrame(results)
print(df_results.to_string(index=False))





!pip install --upgrade astroquery



import os
import time
import random
import pandas as pd
import numpy as np
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.simbad import Simbad
from lenscat import catalog

# Mount Google Drive if needed (run once in Colab)
from google.colab import drive
drive.mount('/content/drive')

SAVE_DIR = '/content/drive/MyDrive/lens_bh_clustering_results'
os.makedirs(SAVE_DIR, exist_ok=True)
print(f"Saving results to: {SAVE_DIR}")

# Setup SIMBAD query object with BH types of interest
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.reset_votable_fields()
custom_simbad.add_votable_fields('otype', 'ra', 'dec')


bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

def query_bh_objects(coord, radius_arcmin=20, max_retries=3):
    """Query SIMBAD for BH-type objects within radius of coord.
    Returns a list of dicts with RA, DEC, OTYPE, or empty if none found."""
    radius = radius_arcmin * u.arcmin
    retries = 0
    while retries < max_retries:
        try:
            result = custom_simbad.query_region(coord, radius=radius)
            if result is None:
                return []  # no objects found
            # Filter results to BH types (case-insensitive)
            bh_mask = [any(bh in str(otype).upper() for bh in bh_types) for otype in result['OTYPE']]
            bh_results = result[bh_mask]
            # Extract RA, DEC, OTYPE for BH objects
            bh_objects = []
            for row in bh_results:
                bh_objects.append({
                    'ra_deg': row['RA'],
                    'dec_deg': row['DEC'],
                    'otype': row['OTYPE']
                })
            return bh_objects
        except Exception as e:
            retries += 1
            print(f"SIMBAD query error at {coord.to_string('hmsdms')} attempt {retries}: {e}")
            time.sleep(5)
    print(f"Failed SIMBAD query at {coord.to_string('hmsdms')} after {max_retries} retries. Returning empty list.")
    return []

# Load lens catalog and filter confident lenses with redshift
df = catalog.to_pandas()
df_confident = df[df['grading'] == 'confident'].copy()
df_confident['z'] = pd.to_numeric(df_confident['zlens'], errors='coerce')
df_confident = df_confident.dropna(subset=['z']).reset_index(drop=True)
print(f"Number of confident lenses with redshift: {len(df_confident)}")

# Parameters
radius_arcmin = 20
batch_size = 25

# Track stats
zero_count_lenses = 0
total_bh_count = 0

# Prepare to save summary stats per batch
summary_stats = []

# Process lenses in batches
for batch_start in range(0, len(df_confident), batch_size):
    batch_df = df_confident.iloc[batch_start:batch_start+batch_size]
    print(f"\nProcessing batch {batch_start // batch_size + 1} lenses ({len(batch_df)} lenses)")

    for idx, lens in batch_df.iterrows():
        lens_id = lens['name']
        ra = lens['RA']
        dec = lens['DEC']
        z = lens['z']
        coord = SkyCoord(ra=ra * u.deg, dec=dec * u.deg)

        bh_objects = query_bh_objects(coord, radius_arcmin=radius_arcmin)

        # Count BH objects
        n_bh = len(bh_objects)
        total_bh_count += n_bh
        if n_bh == 0:
            zero_count_lenses += 1

        # Save results for this lens to a CSV file
        lens_results_df = pd.DataFrame(bh_objects)
        lens_results_df['lens_id'] = lens_id
        lens_results_df['lens_ra'] = ra
        lens_results_df['lens_dec'] = dec
        lens_results_df['lens_z'] = z

        save_path = os.path.join(SAVE_DIR, f"{lens_id}_bh_objects.csv")
        lens_results_df.to_csv(save_path, index=False)

        print(f"Lens {lens_id}: Found {n_bh} BH objects; saved to {save_path}")

    # Batch summary printout
    avg_bh_per_lens = total_bh_count / (batch_start + len(batch_df))
    print(f"\n-- Batch {batch_start // batch_size + 1} Summary --")
    print(f"Total lenses processed: {batch_start + len(batch_df)}")
    print(f"Lenses with zero BHs: {zero_count_lenses}")
    print(f"Average BH objects per lens so far: {avg_bh_per_lens:.3f}")

    # Save batch summary stats (optional)
    summary_stats.append({
        'batch': batch_start // batch_size + 1,
        'lenses_processed': batch_start + len(batch_df),
        'zero_bh_lenses': zero_count_lenses,
        'avg_bh_per_lens': avg_bh_per_lens
    })

# Save summary stats
summary_df = pd.DataFrame(summary_stats)
summary_save_path = os.path.join(SAVE_DIR, 'bh_query_summary_stats.csv')
summary_df.to_csv(summary_save_path, index=False)
print(f"\nAll done! Summary stats saved to {summary_save_path}")

import os
import time
import pandas as pd
import numpy as np
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.simbad import Simbad
from lenscat import catalog

# Mount your Google Drive if needed (only once)
from google.colab import drive
drive.mount('/content/drive')

SAVE_DIR = '/content/drive/MyDrive/lens_bh_clustering_results'
os.makedirs(SAVE_DIR, exist_ok=True)
print(f"Saving results to: {SAVE_DIR}")

# Setup SIMBAD query
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.reset_votable_fields()

custom_simbad.add_votable_fields('otype', 'ra', 'dec')

bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

def query_bh_objects(coord, radius_arcmin=10, max_retries=3):
    radius = radius_arcmin * u.arcmin
    retries = 0
    while retries < max_retries:
        try:
            result = custom_simbad.query_region(coord, radius=radius)
            if result is None:
                return []
            if 'OTYPE' not in result.colnames:
                print(f"Warning: 'OTYPE' missing in SIMBAD response at {coord.to_string('hmsdms')}")
                return []
            # Filter BH-related objects case-insensitively
            bh_rows = [row for row in result if any(bh in row['OTYPE'].upper() for bh in bh_types)]
            return bh_rows
        except Exception as e:
            retries += 1
            print(f"SIMBAD query failed at {coord.to_string('hmsdms')} attempt {retries}: {e}")
            time.sleep(5)
    print(f"SIMBAD query failed at {coord.to_string('hmsdms')} after {max_retries} retries.")
    return []

# Load lens catalog and filter confident lenses with redshift
df = catalog.to_pandas()
df['z'] = pd.to_numeric(df['zlens'], errors='coerce')
confident_lenses = df[(df['grading'] == 'confident') & (~df['z'].isna())].reset_index(drop=True)
print(f"Number of confident lenses with redshift: {len(confident_lenses)}")

BATCH_SIZE = 25
total_lenses = len(confident_lenses)

for i in range(0, total_lenses, BATCH_SIZE):
    batch = confident_lenses.iloc[i:i + BATCH_SIZE]
    print(f"\nProcessing batch {i//BATCH_SIZE + 1} lenses ({len(batch)} lenses)")

    for idx, lens in batch.iterrows():
        coord = SkyCoord(ra=lens['RA'] * u.deg, dec=lens['DEC'] * u.deg)
        bh_objects = query_bh_objects(coord, radius_arcmin=20)  # Use 20' radius as you wanted
        n_bh = len(bh_objects)

        # Convert bh_objects to DataFrame for saving
        if n_bh > 0:
            bh_df = pd.DataFrame(bh_objects)
        else:
            # Empty dataframe with columns matching SIMBAD response structure
            bh_df = pd.DataFrame(columns=custom_simbad.get_votable_fields())

        filename = os.path.join(SAVE_DIR, f"{lens['name']}_bh_objects.csv")
        bh_df.to_csv(filename, index=False)

        print(f"Lens {lens['name']}: Found {n_bh} BH objects; saved to {filename}")

    # Every batch print summary stats
    total_bhs = sum(len(query_bh_objects(SkyCoord(ra=l['RA'] * u.deg, dec=l['DEC'] * u.deg), radius_arcmin=20)) for idx, l in batch.iterrows())
    zero_count = sum(len(query_bh_objects(SkyCoord(ra=l['RA'] * u.deg, dec=l['DEC'] * u.deg), radius_arcmin=20)) == 0 for idx, l in batch.iterrows())
    avg_bhs = total_bhs / len(batch)
    print(f"\nBatch {i//BATCH_SIZE + 1} summary: Total BHs found = {total_bhs}, Zero BH lenses = {zero_count}, Avg BHs per lens = {avg_bhs:.2f}")





# Install dependencies if needed (uncomment in Colab)
# !pip install lenscat astroquery astropy pandas numpy scipy --quiet

import os
import time
import random
import numpy as np
import pandas as pd
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.simbad import Simbad
from lenscat import catalog

# Mount Google Drive (uncomment if running in Colab)
# from google.colab import drive
# drive.mount('/content/drive')

SAVE_DIR = '/content/drive/MyDrive/lens_bh_clustering_results'
os.makedirs(SAVE_DIR, exist_ok=True)
print(f"Saving results to: {SAVE_DIR}")

# Setup SIMBAD query object
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
# Clear all votable fields, then add only the ones we need
custom_simbad.votable_fields.clear()
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

def query_bh_counts(coord, radius_arcmin=10, max_retries=3):
    radius = radius_arcmin * u.arcmin
    retries = 0

    while retries < max_retries:
        try:
            result = custom_simbad.query_region(coord, radius=radius)
            if result is None:
                return 0, []
            if 'OTYPE' not in result.colnames:
                print(f"Warning: 'OTYPE' missing in SIMBAD response at {coord.to_string('hmsdms')}")
                return 0, []
            # Case-insensitive filter for BH types
            mask = [any(bh in otype.upper() for bh in bh_types) for otype in result['OTYPE']]
            filtered = result[mask]

            count = len(filtered)
            # Extract ra, dec of BH objects for clustering analysis
            coords_bh = SkyCoord(ra=filtered['RA'], dec=filtered['DEC'], unit=(u.hourangle, u.deg))

            return count, coords_bh
        except Exception as e:
            retries += 1
            print(f"SIMBAD query failed at {coord.to_string('hmsdms')} (attempt {retries}): {e}")
            time.sleep(5)
    print(f"SIMBAD query failed at {coord.to_string('hmsdms')} after {max_retries} retries, skipping.")
    return 0, []

print("Loading lens catalog...")
df = catalog.to_pandas()

# Filter confident lenses with valid redshift
df_confident = df[(df['grading'] == 'confident')].copy()
df_confident['z'] = pd.to_numeric(df_confident['zlens'], errors='coerce')
df_confident = df_confident.dropna(subset=['z']).reset_index(drop=True)

print(f"Number of confident lenses with redshift: {len(df_confident)}")

batch_size = 25
batches = [df_confident.iloc[i:i + batch_size] for i in range(0, len(df_confident), batch_size)]

radii = [10, 15, 20]  # arcminutes

all_results = []

for batch_i, batch_df in enumerate(batches, 1):
    print(f"\nProcessing batch {batch_i} lenses ({len(batch_df)} lenses)")

    for radius in radii:
        print(f" Radius = {radius} arcmin")
        total_bh_counts = 0
        zero_count_lenses = 0

        for idx, lens in batch_df.iterrows():
            lens_id = lens['name']
            coord = SkyCoord(ra=lens['RA'] * u.deg, dec=lens['DEC'] * u.deg)
            count, coords_bh = query_bh_counts(coord, radius_arcmin=radius)

            # Save BH object coordinates per lens
            coords_df = pd.DataFrame({
                'ra': coords_bh.ra.deg if count > 0 else [],
                'dec': coords_bh.dec.deg if count > 0 else []
            })
            lens_save_path = os.path.join(SAVE_DIR, f"{lens_id}_bh_objects_radius{radius}arcmin.csv")
            coords_df.to_csv(lens_save_path, index=False)

            if count == 0:
                zero_count_lenses += 1
            total_bh_counts += count

            all_results.append({
                'batch': batch_i,
                'lens_id': lens_id,
                'radius_arcmin': radius,
                'bh_count': count,
                'bh_coords_file': lens_save_path
            })

        n_lenses = len(batch_df)
        avg_bh_per_lens = total_bh_counts / n_lenses if n_lenses > 0 else 0
        zero_frac = zero_count_lenses / n_lenses if n_lenses > 0 else 0

        print(f"  Total BH objects found in batch: {total_bh_counts}")
        print(f"  Average BH per lens: {avg_bh_per_lens:.2f}")
        print(f"  Lenses with zero BH objects: {zero_count_lenses} ({zero_frac:.1%})")

    # Save cumulative batch results every batch
    results_df = pd.DataFrame(all_results)
    save_path = os.path.join(SAVE_DIR, 'lens_bh_clustering_results.csv')
    results_df.to_csv(save_path, index=False)
    print(f"  Batch {batch_i} results saved to {save_path}")

print("All batches processed! Final results saved.")

# Install dependencies (run only once in Colab)
# !pip install lenscat astroquery astropy pandas numpy scipy --quiet

import os
import time
import random
import numpy as np
import pandas as pd
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.simbad import Simbad
from lenscat import catalog

# Mount Google Drive (this prompts you once in Colab)
from google.colab import drive
drive.mount('/content/drive')

SAVE_DIR = '/content/drive/MyDrive/lens_bh_clustering_results'
os.makedirs(SAVE_DIR, exist_ok=True)
print(f"Saving results to: {SAVE_DIR}")

# Setup SIMBAD query object

from astroquery.simbad import Simbad
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.remove_votable_fields('*')
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

custom_simbad.add_votable_fields('otype', 'ra', 'dec')

bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

def query_bh_counts(coord, radius_arcmin=10, max_retries=3):
    radius = radius_arcmin * u.arcmin
    retries = 0
    warned_missing_otype = False

    while retries < max_retries:
        try:
            result = custom_simbad.query_region(coord, radius=radius)
            if result is None:
                return 0, []
            if 'OTYPE' not in result.colnames:
                if not warned_missing_otype:
                    print(f"Warning: 'OTYPE' missing in SIMBAD response at {coord.to_string('hmsdms')}")
                    warned_missing_otype = True
                return 0, []
            mask = [any(bh in otype.upper() for bh in bh_types) for otype in result['OTYPE']]
            filtered = result[mask]
            count = len(filtered)
            coords_bh = SkyCoord(ra=filtered['RA'], dec=filtered['DEC'], unit=(u.hourangle, u.deg))
            return count, coords_bh
        except Exception as e:
            if retries == 0:
                print(f"SIMBAD query failed at {coord.to_string('hmsdms')} (will retry up to {max_retries} times): {e}")
            retries += 1
            time.sleep(5)
    print(f"SIMBAD query failed at {coord.to_string('hmsdms')} after {max_retries} retries, skipping.")
    return 0, []

print("Loading lens catalog...")
df = catalog.to_pandas()

df_confident = df[(df['grading'] == 'confident')].copy()
df_confident['z'] = pd.to_numeric(df_confident['zlens'], errors='coerce')
df_confident = df_confident.dropna(subset=['z']).reset_index(drop=True)

print(f"Number of confident lenses with redshift: {len(df_confident)}")

batch_size = 25
batches = [df_confident.iloc[i:i + batch_size] for i in range(0, len(df_confident), batch_size)]

radii = [10, 15, 20]  # arcminutes

all_results = []

for batch_i, batch_df in enumerate(batches, 1):
    print(f"\nProcessing batch {batch_i} lenses ({len(batch_df)} lenses)")

    for radius in radii:
        print(f" Radius = {radius} arcmin")
        total_bh_counts = 0
        zero_count_lenses = 0

        for idx, lens in batch_df.iterrows():
            lens_id = lens['name']
            coord = SkyCoord(ra=lens['RA'] * u.deg, dec=lens['DEC'] * u.deg)
            count, coords_bh = query_bh_counts(coord, radius_arcmin=radius)

            coords_df = pd.DataFrame({
                'ra': coords_bh.ra.deg if count > 0 else [],
                'dec': coords_bh.dec.deg if count > 0 else []
            })
            lens_save_path = os.path.join(SAVE_DIR, f"{lens_id}_bh_objects_radius{radius}arcmin.csv")
            coords_df.to_csv(lens_save_path, index=False)

            if count == 0:
                zero_count_lenses += 1
            total_bh_counts += count

            all_results.append({
                'batch': batch_i,
                'lens_id': lens_id,
                'radius_arcmin': radius,
                'bh_count': count,
                'bh_coords_file': lens_save_path
            })

            time.sleep(0.5)  # polite delay

        n_lenses = len(batch_df)
        avg_bh_per_lens = total_bh_counts / n_lenses if n_lenses > 0 else 0
        zero_frac = zero_count_lenses / n_lenses if n_lenses > 0 else 0

        print(f"  Total BH objects found: {total_bh_counts}")
        print(f"  Average BH per lens: {avg_bh_per_lens:.2f}")
        print(f"  Lenses with zero BH objects: {zero_count_lenses} ({zero_frac:.1%})")

    results_df = pd.DataFrame(all_results)
    save_path = os.path.join(SAVE_DIR, 'lens_bh_clustering_results.csv')
    results_df.to_csv(save_path, index=False)
    print(f"  Batch {batch_i} results saved to {save_path}")

print("All batches processed! Final results saved.")

# Install dependencies if not installed (uncomment if needed)
# !pip install lenscat astroquery astropy pandas numpy --quiet

import os
import time
import random
import numpy as np
import pandas as pd
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.simbad import Simbad
from lenscat import catalog

# Mount Google Drive (run only once)
from google.colab import drive
drive.mount('/content/drive', force_remount=False)

# Directory to save results
SAVE_DIR = '/content/drive/MyDrive/lens_bh_clustering_results'
os.makedirs(SAVE_DIR, exist_ok=True)
print(f"Saving results to: {SAVE_DIR}")

# Setup SIMBAD query object
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
# Add needed fields to default query (do NOT remove defaults to avoid errors)
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

def query_bh_counts(coord, radius_arcmin=10, max_retries=3):
    radius = radius_arcmin * u.arcmin
    retries = 0

    while retries < max_retries:
        try:
            result = custom_simbad.query_region(coord, radius=radius)
            if result is None:
                return 0, []
            if 'OTYPE' not in result.colnames:
                print(f"Warning: 'OTYPE' missing in SIMBAD response at {coord.to_string('hmsdms')}")
                return 0, []
            # Case-insensitive filter for BH types
            mask = [any(bh in otype.upper() for bh in bh_types) for otype in result['OTYPE']]
            filtered = result[mask]

            count = len(filtered)
            # Extract ra, dec of BH objects for clustering analysis
            coords_bh = SkyCoord(ra=filtered['RA'], dec=filtered['DEC'], unit=(u.hourangle, u.deg))

            return count, coords_bh
        except Exception as e:
            retries += 1
            print(f"SIMBAD query failed at {coord.to_string('hmsdms')} (attempt {retries}): {e}")
            time.sleep(5)
    print(f"SIMBAD query failed at {coord.to_string('hmsdms')} after {max_retries} retries, skipping.")
    return 0, []

print("Loading lens catalog...")
df = catalog.to_pandas()

# Filter confident lenses with valid redshift
df_confident = df[(df['grading'] == 'confident')].copy()
df_confident['z'] = pd.to_numeric(df_confident['zlens'], errors='coerce')
df_confident = df_confident.dropna(subset=['z']).reset_index(drop=True)

print(f"Number of confident lenses with redshift: {len(df_confident)}")

batch_size = 25
batches = [df_confident.iloc[i:i + batch_size] for i in range(0, len(df_confident), batch_size)]

radii = [10, 15, 20]  # arcminutes

all_results = []

for batch_i, batch_df in enumerate(batches, 1):
    print(f"\nProcessing batch {batch_i} lenses ({len(batch_df)} lenses)")

    for radius in radii:
        print(f" Radius = {radius} arcmin")
        total_bh_counts = 0
        zero_count_lenses = 0

        for idx, lens in batch_df.iterrows():
            lens_id = lens['name']
            coord = SkyCoord(ra=lens['RA'] * u.deg, dec=lens['DEC'] * u.deg)
            count, coords_bh = query_bh_counts(coord, radius_arcmin=radius)

            # Save BH object coordinates per lens
            coords_df = pd.DataFrame({
                'ra': coords_bh.ra.deg if count > 0 else [],
                'dec': coords_bh.dec.deg if count > 0 else []
            })
            lens_save_path = os.path.join(SAVE_DIR, f"{lens_id}_bh_objects_radius{radius}arcmin.csv")
            coords_df.to_csv(lens_save_path, index=False)

            if count == 0:
                zero_count_lenses += 1
            total_bh_counts += count

            all_results.append({
                'batch': batch_i,
                'lens_id': lens_id,
                'radius_arcmin': radius,
                'bh_count': count,
                'bh_coords_file': lens_save_path
            })

        n_lenses = len(batch_df)
        avg_bh_per_lens = total_bh_counts / n_lenses if n_lenses > 0 else 0
        zero_frac = zero_count_lenses / n_lenses if n_lenses > 0 else 0

        print(f"  Total BH objects found in batch: {total_bh_counts}")
        print(f"  Average BH per lens: {avg_bh_per_lens:.2f}")
        print(f"  Lenses with zero BH objects: {zero_count_lenses} ({zero_frac:.1%})")

    # Save cumulative batch results every batch
    results_df = pd.DataFrame(all_results)
    save_path = os.path.join(SAVE_DIR, 'lens_bh_clustering_results.csv')
    results_df.to_csv(save_path, index=False)
    print(f"  Batch {batch_i} results saved to {save_path}")

print("All batches processed! Final results saved.")

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Install dependencies if not already installed (uncomment if needed)
# !pip install lenscat astroquery astropy pandas numpy --quiet

import os
import time
import random
import numpy as np
import pandas as pd
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.simbad import Simbad
from lenscat import catalog

SAVE_DIR = '/content/drive/MyDrive/lens_bh_clustering_results'
os.makedirs(SAVE_DIR, exist_ok=True)
print(f"Saving results to: {SAVE_DIR}")

custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

def query_bh_counts(coord, radius_arcmin=10, max_retries=3):
    radius = radius_arcmin * u.arcmin
    retries = 0
    while retries < max_retries:
        try:
            result = custom_simbad.query_region(coord, radius=radius)
            if result is None:
                return 0, []
            if 'OTYPE' not in result.colnames:
                print(f"Warning: 'OTYPE' missing in SIMBAD response at {coord.to_string('hmsdms')}")
                return 0, []
            mask = [any(bh in otype.upper() for bh in bh_types) for otype in result['OTYPE']]
            filtered = result[mask]
            count = len(filtered)
            coords_bh = SkyCoord(ra=filtered['RA'], dec=filtered['DEC'], unit=(u.hourangle, u.deg))
            return count, coords_bh
        except Exception as e:
            retries += 1
            print(f"SIMBAD query failed at {coord.to_string('hmsdms')} (attempt {retries}): {e}")
            time.sleep(5)
    print(f"SIMBAD query failed at {coord.to_string('hmsdms')} after {max_retries} retries, skipping.")
    return 0, []

print("Loading lens catalog...")
df = catalog.to_pandas()

df_confident = df[(df['grading'] == 'confident')].copy()
df_confident['z'] = pd.to_numeric(df_confident['zlens'], errors='coerce')
df_confident = df_confident.dropna(subset=['z']).reset_index(drop=True)

print(f"Number of confident lenses with redshift: {len(df_confident)}")

batch_size = 25
batches = [df_confident.iloc[i:i + batch_size] for i in range(0, len(df_confident), batch_size)]

radii = [10, 15, 20]

all_results = []

for batch_i, batch_df in enumerate(batches, 1):
    print(f"\nProcessing batch {batch_i} lenses ({len(batch_df)} lenses)")
    for radius in radii:
        print(f" Radius = {radius} arcmin")
        total_bh_counts = 0
        zero_count_lenses = 0

        for idx, lens in batch_df.iterrows():
            lens_id = lens['name']
            coord = SkyCoord(ra=lens['RA'] * u.deg, dec=lens['DEC'] * u.deg)
            count, coords_bh = query_bh_counts(coord, radius_arcmin=radius)

            coords_df = pd.DataFrame({
                'ra': coords_bh.ra.deg if count > 0 else [],
                'dec': coords_bh.dec.deg if count > 0 else []
            })
            lens_save_path = os.path.join(SAVE_DIR, f"{lens_id}_bh_objects_radius{radius}arcmin.csv")
            coords_df.to_csv(lens_save_path, index=False)

            if count == 0:
                zero_count_lenses += 1
            total_bh_counts += count

            all_results.append({
                'batch': batch_i,
                'lens_id': lens_id,
                'radius_arcmin': radius,
                'bh_count': count,
                'bh_coords_file': lens_save_path
            })

        n_lenses = len(batch_df)
        avg_bh_per_lens = total_bh_counts / n_lenses if n_lenses > 0 else 0
        zero_frac = zero_count_lenses / n_lenses if n_lenses > 0 else 0

        print(f"  Total BH objects found in batch: {total_bh_counts}")
        print(f"  Average BH per lens: {avg_bh_per_lens:.2f}")
        print(f"  Lenses with zero BH objects: {zero_count_lenses} ({zero_frac:.1%})")

    results_df = pd.DataFrame(all_results)
    save_path = os.path.join(SAVE_DIR, 'lens_bh_clustering_results.csv')
    results_df.to_csv(save_path, index=False)
    print(f"  Batch {batch_i} results saved to {save_path}")

print("All batches processed! Final results saved.")

from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.simbad import Simbad

custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

test_coord = SkyCoord(ra=180.0*u.deg, dec=0.0*u.deg)  # example coordinate
result = custom_simbad.query_region(test_coord, radius=10*u.arcmin)

if result:
    print(result)
else:
    print("No objects found near test coord")

# Uncomment and run once in Colab to mount Google Drive:
# from google.colab import drive
# drive.mount('/content/drive')

import os
import time
import pandas as pd
import numpy as np
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.simbad import Simbad
from lenscat import catalog

# Directory to save results on Google Drive
SAVE_DIR = '/content/drive/MyDrive/lens_bh_clustering_results'
os.makedirs(SAVE_DIR, exist_ok=True)
print(f"Saving results to: {SAVE_DIR}")

# Setup SIMBAD query object
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
# Clear default votable fields and add needed ones
from astroquery.simbad import Simbad

custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.add_votable_fields('otype', 'ra', 'dec')



# BH-related SIMBAD object types (case-insensitive substring match)
bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO', 'BLL']

def query_bh_counts(coord, radius_arcmin=10, max_retries=3):
    radius = radius_arcmin * u.arcmin
    retries = 0
    while retries < max_retries:
        try:
            result = custom_simbad.query_region(coord, radius=radius)
            if result is None:
                return 0, [], []
            if 'OTYPE' not in result.colnames:
                print(f"Warning: 'OTYPE' missing in SIMBAD response at {coord.to_string('hmsdms')}")
                return 0, [], []
            # Print unique otypes found for diagnostics
            unique_otypes = set(result['OTYPE'])
            print(f"Unique OTYPEs near {coord.to_string('hmsdms')}: {unique_otypes}")

            # Case-insensitive substring filter for BH types
            mask = [any(bh in otype.upper() for bh in bh_types) for otype in result['OTYPE']]
            filtered = result[mask]

            count = len(filtered)
            # BH object coords (for clustering)
            coords_bh = SkyCoord(ra=filtered['RA'], dec=filtered['DEC'], unit=(u.hourangle, u.deg))

            return count, coords_bh.ra.deg.tolist(), coords_bh.dec.deg.tolist()
        except Exception as e:
            retries += 1
            print(f"SIMBAD query failed at {coord.to_string('hmsdms')} (attempt {retries}): {e}")
            time.sleep(5)
    print(f"SIMBAD query failed at {coord.to_string('hmsdms')} after {max_retries} retries. Skipping.")
    return 0, [], []

print("Loading lens catalog...")
df = catalog.to_pandas()

# Filter confident lenses with valid redshift
df_confident = df[df['grading'] == 'confident'].copy()
df_confident['z'] = pd.to_numeric(df_confident['zlens'], errors='coerce')
df_confident = df_confident.dropna(subset=['z']).reset_index(drop=True)
print(f"Number of confident lenses with redshift: {len(df_confident)}")

batch_size = 25
batches = [df_confident.iloc[i:i + batch_size] for i in range(0, len(df_confident), batch_size)]
radii = [10, 15, 20]  # arcminutes

all_results = []

for batch_i, batch_df in enumerate(batches, 1):
    print(f"\nProcessing batch {batch_i} lenses ({len(batch_df)} lenses)")
    for radius in radii:
        print(f" Radius = {radius} arcmin")
        total_bh = 0
        zero_count = 0
        for idx, lens in batch_df.iterrows():
            lens_id = lens['name']
            coord = SkyCoord(ra=lens['RA'] * u.deg, dec=lens['DEC'] * u.deg)
            count, ra_list, dec_list = query_bh_counts(coord, radius_arcmin=radius)

            # Save BH coords per lens and radius
            coords_df = pd.DataFrame({'ra_deg': ra_list, 'dec_deg': dec_list})
            save_path = os.path.join(SAVE_DIR, f"{lens_id}_bh_radius{radius}arcmin.csv")
            coords_df.to_csv(save_path, index=False)

            if count == 0:
                zero_count += 1
            total_bh += count

            all_results.append({
                'batch': batch_i,
                'lens_id': lens_id,
                'radius_arcmin': radius,
                'bh_count': count,
                'bh_coords_file': save_path
            })

        n_lenses = len(batch_df)
        avg_bh_per_lens = total_bh / n_lenses if n_lenses else 0
        zero_frac = zero_count / n_lenses if n_lenses else 0

        print(f"  Total BH found: {total_bh}")
        print(f"  Average BH per lens: {avg_bh_per_lens:.2f}")
        print(f"  Lenses with zero BH objects: {zero_count} ({zero_frac:.1%})")

    # Save cumulative results after each batch
    results_df = pd.DataFrame(all_results)
    results_path = os.path.join(SAVE_DIR, 'lens_bh_clustering_results.csv')
    results_df.to_csv(results_path, index=False)
    print(f"  Batch {batch_i} results saved to {results_path}")

print("All batches processed. Final results saved.")

# ð¦ Install dependencies
!pip install -q lenscat astroquery astropy pandas numpy

# ð Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# ð Create results folder in Drive
import os
save_dir = "/content/drive/MyDrive/lens_bh_clustering_results"
os.makedirs(save_dir, exist_ok=True)
print(f"Saving results to: {save_dir}")

# ð Imports
import pandas as pd
import numpy as np
from lenscat import load_lenscat
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord, Angle
import astropy.units as u
import random
import time

# ð¯ Define BH-type object categories (SIMBAD otypes)
target_types = {"BH", "BHXRB", "XRB", "QSO", "AGN", "BLAZAR"}

# âï¸ Configure SIMBAD
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.remove_votable_fields('coordinates')
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

# ð¯ Load confident lenses with redshift
all_lenses = load_lenscat()
lenses = all_lenses.query('grade == "confident" and z_lens > 0')
print(f"Loaded {len(lenses)} confident lenses with redshift")

# ð§ª Select a smaller subset for testing
n_lenses = 100  # You can raise this to 1000+
sample = lenses.sample(n=n_lenses, random_state=42)

# ð Utility to query SIMBAD
def query_bh_objects(ra, dec, radius_arcmin=20):
    coord = SkyCoord(ra, dec, unit="deg")
    radius = Angle(radius_arcmin, unit=u.arcmin)
    try:
        result = custom_simbad.query_region(coord, radius=radius)
        if result is None:
            return 0
        types = result['OTYPE']
        return sum(1 for t in types if t and str(t).upper() in target_types)
    except Exception as e:
        print(f"Query error at RA={ra}, Dec={dec}: {e}")
        return 0

# ð Batch run
lens_results = []
random_results = []

used_positions = set()

for i, row in sample.iterrows():
    ra, dec = row['ra_lens'], row['dec_lens']

    # Avoid duplicates
    used_positions.add((round(ra, 5), round(dec, 5)))

    # SIMBAD query around lens
    lens_count = query_bh_objects(ra, dec)
    lens_results.append({
        "ra": ra,
        "dec": dec,
        "bh_count": lens_count,
        "type": "lens"
    })

    # Find a random, non-overlapping position
    while True:
        rand_ra = random.uniform(0, 360)
        rand_dec = random.uniform(-70, 70)
        key = (round(rand_ra, 5), round(rand_dec, 5))
        if key not in used_positions:
            used_positions.add(key)
            break

    rand_count = query_bh_objects(rand_ra, rand_dec)
    random_results.append({
        "ra": rand_ra,
        "dec": rand_dec,
        "bh_count": rand_count,
        "type": "random"
    })

    print(f"[{i+1}/{n_lenses}] Lens BHs: {lens_count}, Random BHs: {rand_count}")
    time.sleep(0.5)  # Avoid SIMBAD rate limit

# ð¾ Save results
df = pd.DataFrame(lens_results + random_results)
save_path = os.path.join(save_dir, f"bh_clustering_batch_{n_lenses}.csv")
df.to_csv(save_path, index=False)
print(f"â Results saved to {save_path}")

# ð Summary
lens_mean = df[df['type'] == 'lens']['bh_count'].mean()
rand_mean = df[df['type'] == 'random']['bh_count'].mean()
print(f"Mean BHs near lenses: {lens_mean:.2f}")
print(f"Mean BHs near randoms: {rand_mean:.2f}")

# -------------------- 1. Install dependencies --------------------
!pip install lenscat astroquery astropy --quiet

# -------------------- 2. Mount Google Drive --------------------
from google.colab import drive
drive.mount('/content/drive')

# -------------------- 3. Import packages --------------------
import pandas as pd
import numpy as np
from astropy.coordinates import SkyCoord, Angle
import astropy.units as u
from astroquery.simbad import Simbad
from lenscat import load_lenscat
import random
import os
import time

# -------------------- 4. Define constants --------------------
BATCH_SIZE = 50
RADIUS_ARCMIN = 20
MAX_BATCHES = 2   # Adjust as needed (2Ã50 = 100 lenses)
TARGET_OTYPES = {"BH", "BHXRB", "XRB", "QSO", "AGN", "BLAZAR"}

save_dir = "/content/drive/MyDrive/lens_bh_clustering_results"
os.makedirs(save_dir, exist_ok=True)

# -------------------- 5. Load lens catalog --------------------
lens_df = load_lenscat()
lens_df = lens_df[lens_df['grade'].isin(["confident", "probable"])]
lens_df = lens_df.dropna(subset=['ra', 'dec', 'z'])

lens_df = lens_df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle
lens_df = lens_df.iloc[:BATCH_SIZE * MAX_BATCHES]  # Limit for testing

used_coords = set()

# -------------------- 6. Define SIMBAD query --------------------
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.remove_votable_fields('coordinates')
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

def query_bh_objects(ra, dec, radius_arcmin):
    coord = SkyCoord(ra, dec, unit="deg")
    radius = Angle(radius_arcmin, unit=u.arcmin)
    try:
        result = custom_simbad.query_region(coord, radius=radius)
        if result is None or len(result) == 0:
            return []
        bh_coords = []
        for row in result:
            obj_type = row['OTYPE']
            if obj_type and obj_type.upper() in TARGET_OTYPES:
                obj_coord = SkyCoord(f"{row['RA']} {row['DEC']}", unit=(u.hourangle, u.deg))
                bh_coords.append(obj_coord)
        return bh_coords
    except Exception as e:
        print(f"SIMBAD query failed at RA={ra}, DEC={dec}: {e}")
        return []

# -------------------- 7. Define random coord generator --------------------
def generate_random_coord(avoid_coords, min_sep_deg=0.5):
    while True:
        ra_rand = random.uniform(0, 360)
        dec_rand = random.uniform(-90, 90)
        candidate = SkyCoord(ra_rand, dec_rand, unit="deg")
        too_close = any(candidate.separation(c).deg < min_sep_deg for c in avoid_coords)
        if not too_close:
            avoid_coords.add(candidate)
            return candidate

# -------------------- 8. Run batch processing --------------------
all_results = []

for batch_num in range(MAX_BATCHES):
    print(f"\n--- Processing batch {batch_num+1} of {MAX_BATCHES} ---")
    batch_lenses = lens_df.iloc[batch_num*BATCH_SIZE : (batch_num+1)*BATCH_SIZE]
    batch_results = []

    for idx, row in batch_lenses.iterrows():
        ra, dec = row["ra"], row["dec"]
        lens_coord = SkyCoord(ra, dec, unit="deg")
        used_coords.add(lens_coord)

        # Query lens position
        lens_bhs = query_bh_objects(ra, dec, RADIUS_ARCMIN)
        lens_bh_count = len(lens_bhs)

        # Query random control
        rand_coord = generate_random_coord(used_coords)
        rand_bhs = query_bh_objects(rand_coord.ra.deg, rand_coord.dec.deg, RADIUS_ARCMIN)
        rand_bh_count = len(rand_bhs)

        # Save
        batch_results.append({
            "lens_id": row["lens_name"],
            "lens_ra": ra,
            "lens_dec": dec,
            "lens_bh_count": lens_bh_count,
            "rand_ra": rand_coord.ra.deg,
            "rand_dec": rand_coord.dec.deg,
            "rand_bh_count": rand_bh_count,
            "lens_bh_coords": lens_bhs,
            "rand_bh_coords": rand_bhs
        })

        print(f"{row['lens_name']} | Lens BHs: {lens_bh_count}, Random BHs: {rand_bh_count}")
        time.sleep(1)  # Avoid overloading SIMBAD

    # Save batch
    save_path = os.path.join(save_dir, f"batch_{batch_num+1}_results.pkl")
    pd.to_pickle(batch_results, save_path)
    print(f"â Saved batch {batch_num+1} to: {save_path}")
    all_results.extend(batch_results)

print("\nð All batches complete. You can now load the .pkl files for analysis.")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from astropy.coordinates import SkyCoord
from astropy import units as u
from astroquery.simbad import Simbad
from tqdm import tqdm
import random
import os

from lenscat import load_lenscat

!pip install -q lenscat astroquery astropy tqdm



from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from astropy.coordinates import SkyCoord
from astropy import units as u
from astroquery.simbad import Simbad
from tqdm import tqdm
import random
import os

from lenscat import load_lenscat

from google.colab import drive
drive.mount('/content/drive')

!pip install astroquery lenscat astropy

import os
import pandas as pd
import numpy as np
from astropy.coordinates import SkyCoord
from astroquery.simbad import Simbad
import astropy.units as u
from lenscat import load_lenscat
import random
import time

# â Set output path
output_dir = "/content/drive/MyDrive/lens_bh_clustering_results"
os.makedirs(output_dir, exist_ok=True)
print("Saving results to:", output_dir)

# â Load lenses (only confident lenses with z)
lenses = load_lenscat()
lenses = lenses[(lenses['grade'].isin(['confident'])) & (lenses['z'] > 0)].reset_index(drop=True)

# â Sample lenses
np.random.seed(42)
lenses_sample = lenses.sample(n=100, random_state=42).reset_index(drop=True)

# â Define BH-like SIMBAD types (case insensitive)
bh_types = {'BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO'}
def is_bh_type(val):
    if isinstance(val, str):
        return val.upper() in bh_types
    return False

# â Setup SIMBAD with correct fields
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

# â Function: SIMBAD query
def query_simbad(ra_deg, dec_deg, radius_arcmin=20):
    coord = SkyCoord(ra=ra_deg*u.deg, dec=dec_deg*u.deg, frame='icrs')
    result = custom_simbad.query_region(coord, radius=radius_arcmin*u.arcmin)
    if result is None:
        return pd.DataFrame(columns=['RA', 'DEC', 'OTYPE'])
    df = result.to_pandas()
    df = df.rename(columns={'RA': 'RA', 'DEC': 'DEC', 'OTYPE': 'otype'})
    df['otype'] = df['otype'].str.upper()
    return df[df['otype'].isin(bh_types)]

# â Function: Generate random point far from known lenses
lens_coords = SkyCoord(ra=lenses['ra'].values*u.deg, dec=lenses['dec'].values*u.deg)
def generate_random_point():
    while True:
        ra_rand = random.uniform(0, 360)
        dec_rand = random.uniform(-90, 90)
        rand_coord = SkyCoord(ra_rand*u.deg, dec_rand*u.deg)
        if rand_coord.separation(lens_coords).min() > 0.5*u.deg:
            return ra_rand, dec_rand

# â Run analysis
lens_results, random_results = [], []

for i, row in lenses_sample.iterrows():
    ra_lens, dec_lens = row['ra'], row['dec']

    # Query real lens field
    lens_bh = query_simbad(ra_lens, dec_lens)
    lens_results.append({
        'index': i,
        'lens_ra': ra_lens,
        'lens_dec': dec_lens,
        'num_bhs': len(lens_bh),
        'bh_coords': list(zip(lens_bh['RA'], lens_bh['DEC']))
    })

    # Query random control field
    ra_rand, dec_rand = generate_random_point()
    rand_bh = query_simbad(ra_rand, dec_rand)
    random_results.append({
        'index': i,
        'rand_ra': ra_rand,
        'rand_dec': dec_rand,
        'num_bhs': len(rand_bh),
        'bh_coords': list(zip(rand_bh['RA'], rand_bh['DEC']))
    })

    print(f"Processed lens {i+1}/{len(lenses_sample)} | BHs: {len(lens_bh)} | Rand BHs: {len(rand_bh)}")

    # Optional: Save intermediate results every 10 lenses
    if (i+1) % 10 == 0:
        pd.DataFrame(lens_results).to_pickle(os.path.join(output_dir, f"lens_results_batch{i+1}.pkl"))
        pd.DataFrame(random_results).to_pickle(os.path.join(output_dir, f"random_results_batch{i+1}.pkl"))

# â Final save
pd.DataFrame(lens_results).to_pickle(os.path.join(output_dir, "lens_results_final.pkl"))
pd.DataFrame(random_results).to_pickle(os.path.join(output_dir, "random_results_final.pkl"))

print("â All done!")

!pip install astroquery lenscat astropy

!pip install astroquery --quiet

!pip install -q lenscat astroquery astropy tqdm

from astropy.cosmology import Planck15 as cosmo
from astropy.constants import G, c
import numpy as np

# Assume you have columns: 'ra', 'dec', 'redshift' (z_lens), 'mass_surface_density_kpc2' (stellar surface density)

# If you don't have source redshift, approximate:
non_zero_df['z_source'] = non_zero_df['redshift'] + 0.5

# Function to compute Sigma_crit in M_sun/kpc^2
def sigma_crit_kpc2(z_lens, z_source):
    if z_source <= z_lens:
        return np.nan
    Dl = cosmo.angular_diameter_distance(z_lens).to('kpc').value
    Ds = cosmo.angular_diameter_distance(z_source).to('kpc').value
    Dls = cosmo.angular_diameter_distance_z1z2(z_lens, z_source).to('kpc').value

    sigma_crit = (c ** 2) / (4 * np.pi * G) * (Ds / (Dl * Dls))  # units: kg/m^2
    # Convert SI to M_sun/kpc^2:
    sigma_crit = sigma_crit.to('M_sun/kpc^2').value
    return sigma_crit

# Vectorize the function for performance
sigma_crit_vec = np.vectorize(sigma_crit_kpc2)

# Compute Sigma_crit for each lens
non_zero_df['Sigma_crit_kpc2'] = sigma_crit_vec(non_zero_df['redshift'], non_zero_df['z_source'])

# Compute f_star = Sigma_star / Sigma_crit
non_zero_df['f_star'] = non_zero_df['mass_surface_density_kpc2'] / non_zero_df['Sigma_crit_kpc2']

# Flag lenses with high stellar fraction (e.g. f_star > 0.2)
high_fraction = non_zero_df[non_zero_df['f_star'] > 0.2]

print(f"Number of lenses where stellar mass surface density > 20% of critical density: {len(high_fraction)}")

# Optionally, show the high fraction lenses
print(high_fraction[['ra', 'dec', 'redshift', 'mass_surface_density_kpc2', 'Sigma_crit_kpc2', 'f_star']])

import pandas as pd
import numpy as np
from astropy.constants import G, c
from astropy import units as u
from astropy.cosmology import Planck15 as cosmo

# Load your data (update path as needed)
progress_file = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_progress.csv'
df = pd.read_csv(progress_file)

# Convert surface density units from M_sun/Mpc^2 to M_sun/kpc^2
df['mass_surface_density_kpc2'] = df['mass_surface_density_Msun_per_Mpc2'] / 1e6

# Filter out zero surface density lenses
non_zero_df = df[df['mass_surface_density_kpc2'] > 0].copy()

# Approximate source redshift if missing
if 'z_source' not in non_zero_df.columns:
    non_zero_df['z_source'] = non_zero_df['redshift'] + 0.5

# Function to compute critical surface density Î£_crit in M_sun/kpc^2
def sigma_crit_kpc2(z_lens, z_source):
    if z_source <= z_lens:
        return np.nan
    Dl = cosmo.angular_diameter_distance(z_lens).to(u.m)
    Ds = cosmo.angular_diameter_distance(z_source).to(u.m)
    Dls = cosmo.angular_diameter_distan

import pandas as pd
import numpy as np
from astropy.constants import G, c
from astropy import units as u
from astropy.cosmology import Planck15 as cosmo

# Load your data (update path as needed)
progress_file = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_progress.csv'
df = pd.read_csv(progress_file)

# Convert surface density units from M_sun/Mpc^2 to M_sun/kpc^2
df['mass_surface_density_kpc2'] = df['mass_surface_density_Msun_per_Mpc2'] / 1e6

# Filter out zero surface density lenses
non_zero_df = df[df['mass_surface_density_kpc2'] > 0].copy()

# Approximate source redshift if missing
if 'z_source' not in non_zero_df.columns:
    non_zero_df['z_source'] = non_zero_df['redshift'] + 0.5

# Function to compute critical surface density Î£_crit in M_sun/kpc^2
def sigma_crit_kpc2(z_lens, z_source):
    if z_source <= z_lens:
        return np.nan
    Dl = cosmo.angular_diameter_distance(z_lens).to(u.m)
    Ds = cosmo.angular_diameter_distance(z_source).to(u.m)
    Dls = cosmo.angular_diameter_distance_z1z2(z_lens, z_source).to(u.m)

    sigma_crit = (c**2 / (4 * np.pi * G)) * (Ds / (Dl * Dls))  # kg/m^2
    sigma_crit = sigma_crit.to(u.M_sun / u.kpc**2)
    return sigma_crit.value

# Vectorize for faster computation
sigma_crit_vec = np.vectorize(sigma_crit_kpc2)

# Compute Î£_crit for each lens
non_zero_df['Sigma_crit_kpc2'] = sigma_crit_vec(non_zero_df['redshift'], non_zero_df['z_source'])

# Compute stellar fraction f_star = Î£_star / Î£_crit
non_zero_df['f_star'] = non_zero_df['mass_surface_density_kpc2'] / non_zero_df['Sigma_crit_kpc2']

# Summary printout
print(f"Total lenses with non-zero stellar surface density: {len(non_zero_df)}")

# Show how many lenses have stellar fraction above 0.2 (20%)
high_fraction = non_zero_df[non_zero_df['f_star'] > 0.2]
print(f"Lenses with stellar fraction f* > 0.2: {len(high_fraction)}")

# Show first few entries with key columns
print(non_zero_df[['redshift', 'mass_surface_density_kpc2', 'Sigma_crit_kpc2', 'f_star']].head())

import pandas as pd
import numpy as np
from astropy.cosmology import Planck18 as cosmo
import astropy.units as u

# Load your data
progress_file = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_progress.csv'
df = pd.read_csv(progress_file)

# Assuming df has columns:
# 'redshift' - lens redshift
# 'total_stellar_mass_20arcmin' - total stellar mass within 20' radius in solar masses

# Check column names and units
print(df.columns)

# Convert 20 arcminutes to kpc physical scale at each lens redshift
theta = 20 * u.arcmin

# Compute angular diameter distance in kpc
d_A = cosmo.angular_diameter_distance(df['redshift']).to(u.kpc)

# Physical radius corresponding to 20'
R_phys = (theta.to(u.radian).value * d_A).to(u.kpc)

# Calculate area in kpc^2
area_kpc2 = np.pi * (R_phys.value)**2

# Calculate stellar surface mass density: M_sun / kpc^2
df['stellar_surface_density_20arcmin'] = df['total_stellar_mass_20arcmin'] / area_kpc2

# CDM threshold for strong lensing (M_sun / kpc^2)
cdm_threshold = 1e8

f_star_values = [0.01, 0.03, 0.05, 0.1, 0.2]

total_lenses = len(df)
print(f"Total lenses: {total_lenses}")

for f_star in f_star_values:
    inferred_total = df['stellar_surface_density_20arcmin'] / f_star
    below_threshold = inferred_total < cdm_threshold
    n_below = below_threshold.sum()
    frac_below = n_below / total_lenses * 100
    print(f"f* = {f_star:.2f} | {n_below}/{total_lenses} lenses ({frac_below:.1f}%) below CDM threshold")

import pandas as pd
import numpy as np
from astropy.cosmology import Planck18 as cosmo
import astropy.units as u

# Load data
progress_file = '/content/drive/MyDrive/lens_stellar_mass_results/lens_stellar_mass_progress.csv'
df = pd.read_csv(progress_file)

print("Columns in dataframe:", df.columns)

# Convert surface density to M_sun/kpc^2
df['mass_surface_density_kpc2'] = df['mass_surface_density_Msun_per_Mpc2'] / 1e6

# Calculate physical radius for 20 arcmin at each redshift
theta = 20 * u.arcmin
d_A = cosmo.angular_diameter_distance(df['redshift']).to(u.kpc)
R_phys = (theta.to(u.radian).value * d_A).to(u.kpc)
area_kpc2 = np.pi * (R_phys.value)**2

# Option 1: Calculate surface density from total_mass_Msun and area
df['calc_surface_density'] = df['total_mass_Msun'] / area_kpc2

# CDM strong lensing threshold
cdm_threshold = 1e8  # M_sun/kpc^2

f_star_values = [0.01, 0.03, 0.05, 0.1, 0.2]

total_lenses = len(df)
print(f"Total lenses: {total_lenses}")

for f_star in f_star_values:
    # Use your calculated surface density here:
    inferred_total = df['calc_surface_density'] / f_star
    below_threshold = inferred_total < cdm_threshold
    n_below = below_threshold.sum()
    frac_below = n_below / total_lenses * 100
    print(f"f* = {f_star:.2f} | {n_below}/{total_lenses} lenses ({frac_below:.1f}%) below CDM threshold")

import pandas as pd
import numpy as np
from astropy.constants import G, c
from astropy import units as u
from astropy.cosmology import Planck15 as cosmo

# 1. Load your DataFrame
# Replace with your actual file if needed
lens_df = pd.read_csv('lens_stellar_mass_lenscat.csv')

# 2. Fill in source redshift if missing (optional approximation)
if 'z_source' not in lens_df.columns:
    lens_df['z_source'] = lens_df['z_lens'] + 0.5

# 3. Comp

